{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1f108c2efb94d44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4781dbdb443e5573",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Always have your imports at the top\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from hashlib import sha1 # just for grading purposes\n",
    "import json # just for grading purposes\n",
    "\n",
    "from utils import get_dataset, workflow_steps, data_analysis_steps\n",
    "\n",
    "def _hash(obj, salt='none'):\n",
    "    if type(obj) is not str:\n",
    "        obj = json.dumps(obj)\n",
    "    to_encode = obj + salt\n",
    "    return sha1(to_encode.encode()).hexdigest()\n",
    "\n",
    "X, y = get_dataset()  # preloaded dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0736ca1b894afc53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: ~The Larch~ Workflow steps\n",
    "\n",
    "What are the basic workflow steps?\n",
    "\n",
    "<img src=\"media/the_larch.jpg\" width=\"300\" />\n",
    "\n",
    "<p style=\"font-size: 9px; text-align:center\">A larch</p>\n",
    "\n",
    "You probably know them already, but we want you to really internalize them. We've given you a list of steps `workflow_steps`, but it appears that, not only does it have to many steps, some are _probably_ wrong, as well.\n",
    "\n",
    "Select the correct ones and reorder them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3f3abe0075f1f26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow steps:\n",
      "1 :  Establish a Baseline\n",
      "2 :  Evaluate results\n",
      "3 :  Google Hackathon solutions\n",
      "4 :  Spam\n",
      "5 :  Increase complexity\n",
      "6 :  Get the data\n",
      "7 :  Watch Netflix\n",
      "8 :  Iterate\n",
      "9 :  Train model\n",
      "10 :  Data analysis and preparation\n"
     ]
    }
   ],
   "source": [
    "print(\"Workflow steps:\")\n",
    "for i in range(len(workflow_steps)):\n",
    "    print(i+1, ': ', workflow_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c8218e1615228f0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1.1. Filter and sort the names of the steps in the workflow_steps list\n",
    "# workflow_steps_answer = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "workflow_steps_answer = [\"Get the data\",\n",
    "                         \"Data analysis and preparation\",\n",
    "                         \"Train model\",\n",
    "                         \"Evaluate results\",                         \n",
    "                         'Iterate',\n",
    "                         \"Establish a Baseline\",\n",
    "                         \"Increase complexity\"\n",
    "                         \n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d15af97ae329a075",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-99ca7e19f837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### BEGIN TESTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0m_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkflow_steps_answer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'salt0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'701e2306da9bfde36382bdb6feb80a354916ebf4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m### END TESTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash([step.lower() for step in workflow_steps_answer], 'salt0') == '701e2306da9bfde36382bdb6feb80a354916ebf4'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8feaa0f2674908a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are way too many substeps in the Data Analysis and Preparation step to group them all under a single category. We've given you another list of steps: `data_analysis_steps`.\n",
    "\n",
    "Aside from being shuffled, it should be fine but keep an eye out. You never know what to expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee99eb109e66ea78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis and Preparation steps:\n",
      "1 :  Dealing with data problems\n",
      "2 :  Data analysis\n",
      "3 :  Feature selection\n",
      "4 :  Spanish Inquisition\n",
      "5 :  Feature engineering\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Analysis and Preparation steps:\")\n",
    "for i in range(len(data_analysis_steps)):\n",
    "    print(i+1, ': ', data_analysis_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e47f7838cbcf13d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1.2. Filter and sort the names of the steps in the data_analysis_steps list\n",
    "# data_analysis_steps_answer = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "data_analysis_steps_answer = [\"Data analysis\",\n",
    "                              \"Dealing with data problems\",\n",
    "                              \"Feature engineering\",\n",
    "                              \"Feature selection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eb6774d7694e5ada",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash([step.lower() for step in data_analysis_steps_answer], 'salt0') == '658ab90eff4a0cea2bfb51cc89c8db5b4121fa86'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27c62b7bf227baa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p style=\"text-align:center\">That's right! \n",
    "\n",
    "<p style=\"text-align:center\"><b>Nobody</b> expects the Spanish Inquisition!\n",
    "\n",
    "<img src=\"media/spanish_inquisition.gif\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb212306fe32d1fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Specific workflow questions\n",
    "\n",
    "Here are some more specific questions about individual workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53dadb4a6c1cb987",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2.1. True or False, you should split your data in a training and test set\n",
    "# split_training_test = ...\n",
    "\n",
    "# Exercise 2.2. True or False, Scikit Pipelines are only useful in production environments\n",
    "# scikit_pipelines_useful = ...\n",
    "\n",
    "# Exercise 2.3. True or False, you should try to make a complex baseline, so you just have \n",
    "#               to make simple improvements on it, later on.\n",
    "# baseline_complex = ...\n",
    "\n",
    "# Exercise 2.4. (optional) True or False, is Brian the Messiah?\n",
    "# is_brian_the_messiah = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "split_training_test = True\n",
    "scikit_pipelines_useful = False\n",
    "baseline_complex = False\n",
    "is_brian_the_messiah = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f1cfeb62360089e3",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(split_training_test, 'salt1') == '569b45c42b5c7b490c92692b911af35f575c8a06'\n",
    "assert _hash(scikit_pipelines_useful, 'salt2') == 'ef07576cc7d3bcb2cf29e1a772aec2aad7f59158'\n",
    "assert _hash(baseline_complex, 'salt3') == 'f24a294afb4a09f7f9df9ee13eb18e7d341c439d'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb240cb38ac7a823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/monty_python_messiah.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5790b6079fcdd3a1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Scikit Pipelines\n",
    "\n",
    "We've already loaded and splitted a dataset for the following exercises. They're stored in the `X_train`, `X_test`, `y_train` and `y_test` variables.\n",
    "\n",
    "In a perfect world, where you have all your data clean and ready-to-go, you can create your pipeline with just Scikit-learn's Transformers. However, in the real world, that's not the case, and you'll need to create custom Transformers to get the job done. Take a look at the data set, what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>leg_1</th>\n",
       "      <th>7</th>\n",
       "      <th>arm_2</th>\n",
       "      <th>2</th>\n",
       "      <th>11</th>\n",
       "      <th>4</th>\n",
       "      <th>16</th>\n",
       "      <th>0</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>9</th>\n",
       "      <th>19</th>\n",
       "      <th>18</th>\n",
       "      <th>8</th>\n",
       "      <th>6</th>\n",
       "      <th>arm_0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.803179</td>\n",
       "      <td>warranty</td>\n",
       "      <td>-0.271124</td>\n",
       "      <td>bay</td>\n",
       "      <td>1.492689</td>\n",
       "      <td>-0.998385</td>\n",
       "      <td>-0.226479</td>\n",
       "      <td>-0.877983</td>\n",
       "      <td>0.766080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021367</td>\n",
       "      <td>2.368674</td>\n",
       "      <td>1.170775</td>\n",
       "      <td>-0.100154</td>\n",
       "      <td>0.913585</td>\n",
       "      <td>0.367366</td>\n",
       "      <td>1.226933</td>\n",
       "      <td>-0.747212</td>\n",
       "      <td>Riviera</td>\n",
       "      <td>-2.654613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.272724</td>\n",
       "      <td>abrogate</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>beryllium</td>\n",
       "      <td>-2.696887</td>\n",
       "      <td>-1.012104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.335785</td>\n",
       "      <td>0.823171</td>\n",
       "      <td>-1.654857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>1.658822</td>\n",
       "      <td>-0.749202</td>\n",
       "      <td>-1.289961</td>\n",
       "      <td>-0.245743</td>\n",
       "      <td>-1.503143</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>0.696206</td>\n",
       "      <td>vernier</td>\n",
       "      <td>-1.207273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.062593</td>\n",
       "      <td>buckskin</td>\n",
       "      <td>-0.280675</td>\n",
       "      <td>businessman</td>\n",
       "      <td>-0.753965</td>\n",
       "      <td>-1.389572</td>\n",
       "      <td>0.758929</td>\n",
       "      <td>0.594754</td>\n",
       "      <td>1.022570</td>\n",
       "      <td>-1.645399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.692957</td>\n",
       "      <td>-0.053198</td>\n",
       "      <td>1.460980</td>\n",
       "      <td>1.384273</td>\n",
       "      <td>0.104201</td>\n",
       "      <td>0.281191</td>\n",
       "      <td>2.439752</td>\n",
       "      <td>-0.098340</td>\n",
       "      <td>crypt</td>\n",
       "      <td>-0.558181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.428115</td>\n",
       "      <td>mask</td>\n",
       "      <td>0.850222</td>\n",
       "      <td>g</td>\n",
       "      <td>1.500760</td>\n",
       "      <td>-1.294681</td>\n",
       "      <td>0.996267</td>\n",
       "      <td>0.076822</td>\n",
       "      <td>-0.467701</td>\n",
       "      <td>1.160827</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.559426</td>\n",
       "      <td>1.724002</td>\n",
       "      <td>-0.046921</td>\n",
       "      <td>-1.556582</td>\n",
       "      <td>-0.493757</td>\n",
       "      <td>0.346504</td>\n",
       "      <td>-0.349258</td>\n",
       "      <td>impeccable</td>\n",
       "      <td>-1.228234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.440117</td>\n",
       "      <td>chimeric</td>\n",
       "      <td>1.800940</td>\n",
       "      <td>clavicle</td>\n",
       "      <td>-0.676392</td>\n",
       "      <td>-0.082151</td>\n",
       "      <td>0.196521</td>\n",
       "      <td>0.642723</td>\n",
       "      <td>0.342725</td>\n",
       "      <td>1.117296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040158</td>\n",
       "      <td>1.086594</td>\n",
       "      <td>-1.054638</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>-0.089736</td>\n",
       "      <td>0.709004</td>\n",
       "      <td>0.456753</td>\n",
       "      <td>-1.430775</td>\n",
       "      <td>eyepiece</td>\n",
       "      <td>-0.556581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.001046</td>\n",
       "      <td>runabout</td>\n",
       "      <td>0.677875</td>\n",
       "      <td>certitude</td>\n",
       "      <td>-2.703232</td>\n",
       "      <td>0.950308</td>\n",
       "      <td>0.975198</td>\n",
       "      <td>-0.927353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654076</td>\n",
       "      <td>1.372711</td>\n",
       "      <td>0.500477</td>\n",
       "      <td>0.070052</td>\n",
       "      <td>0.189582</td>\n",
       "      <td>0.501094</td>\n",
       "      <td>-0.168822</td>\n",
       "      <td>-1.830633</td>\n",
       "      <td>cacao</td>\n",
       "      <td>-1.464473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.375707</td>\n",
       "      <td>Runge</td>\n",
       "      <td>0.125576</td>\n",
       "      <td>bottleneck</td>\n",
       "      <td>-0.150056</td>\n",
       "      <td>0.321357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.189470</td>\n",
       "      <td>1.613711</td>\n",
       "      <td>0.421921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173072</td>\n",
       "      <td>-1.158068</td>\n",
       "      <td>0.963360</td>\n",
       "      <td>-0.244157</td>\n",
       "      <td>-0.297564</td>\n",
       "      <td>0.701173</td>\n",
       "      <td>0.453534</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>sloven</td>\n",
       "      <td>0.659924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.211016</td>\n",
       "      <td>wattage</td>\n",
       "      <td>0.047399</td>\n",
       "      <td>horn</td>\n",
       "      <td>-0.651836</td>\n",
       "      <td>-0.662624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.259723</td>\n",
       "      <td>-0.763259</td>\n",
       "      <td>0.570599</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.559849</td>\n",
       "      <td>-1.113279</td>\n",
       "      <td>-1.627542</td>\n",
       "      <td>-0.066080</td>\n",
       "      <td>-1.661520</td>\n",
       "      <td>-1.804882</td>\n",
       "      <td>-0.384556</td>\n",
       "      <td>everything</td>\n",
       "      <td>1.890331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.187679</td>\n",
       "      <td>Exxon</td>\n",
       "      <td>0.201160</td>\n",
       "      <td>shamrock</td>\n",
       "      <td>-0.464617</td>\n",
       "      <td>1.615376</td>\n",
       "      <td>-0.903702</td>\n",
       "      <td>0.403730</td>\n",
       "      <td>1.217159</td>\n",
       "      <td>-0.322320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283288</td>\n",
       "      <td>0.964233</td>\n",
       "      <td>0.193754</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>-1.179040</td>\n",
       "      <td>0.324359</td>\n",
       "      <td>1.521316</td>\n",
       "      <td>-0.258905</td>\n",
       "      <td>commissary</td>\n",
       "      <td>-0.963142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.953329</td>\n",
       "      <td>slim</td>\n",
       "      <td>1.624678</td>\n",
       "      <td>Gullah</td>\n",
       "      <td>0.122670</td>\n",
       "      <td>-2.362932</td>\n",
       "      <td>-0.645964</td>\n",
       "      <td>-0.182896</td>\n",
       "      <td>0.619154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323079</td>\n",
       "      <td>1.178696</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>-0.482744</td>\n",
       "      <td>-0.799192</td>\n",
       "      <td>2.057495</td>\n",
       "      <td>-0.252354</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>-1.310899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.772252</td>\n",
       "      <td>Olivetti</td>\n",
       "      <td>-0.457096</td>\n",
       "      <td>musculature</td>\n",
       "      <td>1.661259</td>\n",
       "      <td>0.140886</td>\n",
       "      <td>-1.169917</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.323168</td>\n",
       "      <td>-1.768439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.602212</td>\n",
       "      <td>1.533138</td>\n",
       "      <td>0.976874</td>\n",
       "      <td>-0.466037</td>\n",
       "      <td>-0.027515</td>\n",
       "      <td>-2.872262</td>\n",
       "      <td>-0.147603</td>\n",
       "      <td>0.468774</td>\n",
       "      <td>fieldwork</td>\n",
       "      <td>-1.809219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.600217</td>\n",
       "      <td>substantive</td>\n",
       "      <td>-0.385314</td>\n",
       "      <td>carpentry</td>\n",
       "      <td>0.069802</td>\n",
       "      <td>1.632411</td>\n",
       "      <td>-0.981509</td>\n",
       "      <td>1.163164</td>\n",
       "      <td>-0.440044</td>\n",
       "      <td>-1.430141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113517</td>\n",
       "      <td>0.071236</td>\n",
       "      <td>0.367832</td>\n",
       "      <td>1.441273</td>\n",
       "      <td>0.199060</td>\n",
       "      <td>0.462103</td>\n",
       "      <td>0.130741</td>\n",
       "      <td>0.662131</td>\n",
       "      <td>stretch</td>\n",
       "      <td>-0.218004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.615936</td>\n",
       "      <td>Canaan</td>\n",
       "      <td>-0.309546</td>\n",
       "      <td>jackdaw</td>\n",
       "      <td>0.593101</td>\n",
       "      <td>-0.524520</td>\n",
       "      <td>-0.360966</td>\n",
       "      <td>0.710960</td>\n",
       "      <td>-1.222128</td>\n",
       "      <td>0.489375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326133</td>\n",
       "      <td>1.468308</td>\n",
       "      <td>-0.747180</td>\n",
       "      <td>-0.240325</td>\n",
       "      <td>-1.081063</td>\n",
       "      <td>1.159330</td>\n",
       "      <td>0.712998</td>\n",
       "      <td>-1.251114</td>\n",
       "      <td>lottery</td>\n",
       "      <td>-1.033717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.843247</td>\n",
       "      <td>pick</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>timetable</td>\n",
       "      <td>2.170943</td>\n",
       "      <td>0.511203</td>\n",
       "      <td>0.075434</td>\n",
       "      <td>1.639965</td>\n",
       "      <td>-0.137449</td>\n",
       "      <td>1.373659</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.327252</td>\n",
       "      <td>0.405709</td>\n",
       "      <td>1.612278</td>\n",
       "      <td>-0.246062</td>\n",
       "      <td>-1.601966</td>\n",
       "      <td>0.952875</td>\n",
       "      <td>0.551485</td>\n",
       "      <td>conscious</td>\n",
       "      <td>-0.468094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.575818</td>\n",
       "      <td>hairpin</td>\n",
       "      <td>-2.301921</td>\n",
       "      <td>steeple</td>\n",
       "      <td>-0.275052</td>\n",
       "      <td>-0.553649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.112328</td>\n",
       "      <td>1.964725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.515191</td>\n",
       "      <td>-0.539630</td>\n",
       "      <td>-0.430668</td>\n",
       "      <td>-0.699726</td>\n",
       "      <td>-0.530501</td>\n",
       "      <td>0.757508</td>\n",
       "      <td>0.035264</td>\n",
       "      <td>1.366874</td>\n",
       "      <td>nodule</td>\n",
       "      <td>0.672873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.490726</td>\n",
       "      <td>jewelry</td>\n",
       "      <td>-0.337086</td>\n",
       "      <td>soliton</td>\n",
       "      <td>0.148667</td>\n",
       "      <td>-0.070499</td>\n",
       "      <td>1.530751</td>\n",
       "      <td>-1.209695</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>0.486502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613403</td>\n",
       "      <td>-0.331376</td>\n",
       "      <td>1.306542</td>\n",
       "      <td>-0.939335</td>\n",
       "      <td>-0.213443</td>\n",
       "      <td>1.218762</td>\n",
       "      <td>-1.975467</td>\n",
       "      <td>-0.302470</td>\n",
       "      <td>pelagic</td>\n",
       "      <td>-0.239385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.331233</td>\n",
       "      <td>vocal</td>\n",
       "      <td>0.507991</td>\n",
       "      <td>camelback</td>\n",
       "      <td>-1.836205</td>\n",
       "      <td>1.213098</td>\n",
       "      <td>0.708109</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>2.319330</td>\n",
       "      <td>0.141717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.103367</td>\n",
       "      <td>-1.876538</td>\n",
       "      <td>0.647019</td>\n",
       "      <td>0.192049</td>\n",
       "      <td>-0.785989</td>\n",
       "      <td>0.956702</td>\n",
       "      <td>0.393318</td>\n",
       "      <td>-2.152891</td>\n",
       "      <td>maim</td>\n",
       "      <td>1.449016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.504047</td>\n",
       "      <td>sundown</td>\n",
       "      <td>-0.792873</td>\n",
       "      <td>spline</td>\n",
       "      <td>-0.530258</td>\n",
       "      <td>-2.067442</td>\n",
       "      <td>-0.062679</td>\n",
       "      <td>-0.513867</td>\n",
       "      <td>-1.304470</td>\n",
       "      <td>-0.089120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107030</td>\n",
       "      <td>-1.407357</td>\n",
       "      <td>-0.873066</td>\n",
       "      <td>0.366598</td>\n",
       "      <td>-0.985726</td>\n",
       "      <td>0.955142</td>\n",
       "      <td>0.669673</td>\n",
       "      <td>-1.035242</td>\n",
       "      <td>anybody'd</td>\n",
       "      <td>1.650958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.752270</td>\n",
       "      <td>Textron</td>\n",
       "      <td>1.271555</td>\n",
       "      <td>equidistant</td>\n",
       "      <td>0.935678</td>\n",
       "      <td>-0.254977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.342688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.503993</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.154661</td>\n",
       "      <td>-1.032605</td>\n",
       "      <td>1.246085</td>\n",
       "      <td>-1.110576</td>\n",
       "      <td>-0.777817</td>\n",
       "      <td>1.091507</td>\n",
       "      <td>-1.129052</td>\n",
       "      <td>Keenan</td>\n",
       "      <td>-0.628041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.553588</td>\n",
       "      <td>coprocessor</td>\n",
       "      <td>1.628397</td>\n",
       "      <td>deconvolve</td>\n",
       "      <td>0.568983</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>-0.803675</td>\n",
       "      <td>-0.088282</td>\n",
       "      <td>0.963879</td>\n",
       "      <td>-0.147002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379128</td>\n",
       "      <td>-1.820377</td>\n",
       "      <td>0.655741</td>\n",
       "      <td>-0.557492</td>\n",
       "      <td>1.677701</td>\n",
       "      <td>1.639117</td>\n",
       "      <td>2.210523</td>\n",
       "      <td>-0.203580</td>\n",
       "      <td>shopworn</td>\n",
       "      <td>1.393983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.183983</td>\n",
       "      <td>Rollins</td>\n",
       "      <td>0.347582</td>\n",
       "      <td>Ned</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>-0.037635</td>\n",
       "      <td>-1.348185</td>\n",
       "      <td>0.307802</td>\n",
       "      <td>0.114228</td>\n",
       "      <td>1.103302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539760</td>\n",
       "      <td>0.714794</td>\n",
       "      <td>-1.292672</td>\n",
       "      <td>-0.363612</td>\n",
       "      <td>0.170865</td>\n",
       "      <td>0.743264</td>\n",
       "      <td>0.150302</td>\n",
       "      <td>-0.778305</td>\n",
       "      <td>Wilhelm</td>\n",
       "      <td>-0.117357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.309821</td>\n",
       "      <td>lawful</td>\n",
       "      <td>0.413799</td>\n",
       "      <td>Jaime</td>\n",
       "      <td>0.633777</td>\n",
       "      <td>-1.243863</td>\n",
       "      <td>-0.835347</td>\n",
       "      <td>-0.673491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.692905</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.508331</td>\n",
       "      <td>1.281318</td>\n",
       "      <td>-0.294950</td>\n",
       "      <td>-1.187598</td>\n",
       "      <td>2.145149</td>\n",
       "      <td>0.894924</td>\n",
       "      <td>-0.129821</td>\n",
       "      <td>vocalic</td>\n",
       "      <td>-0.066922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.094192</td>\n",
       "      <td>leaky</td>\n",
       "      <td>1.529550</td>\n",
       "      <td>Titan</td>\n",
       "      <td>-1.692465</td>\n",
       "      <td>0.481009</td>\n",
       "      <td>-0.989605</td>\n",
       "      <td>1.593187</td>\n",
       "      <td>-0.790474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.627375</td>\n",
       "      <td>-1.193006</td>\n",
       "      <td>1.882024</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>-0.125787</td>\n",
       "      <td>0.471468</td>\n",
       "      <td>-0.426881</td>\n",
       "      <td>compulsion</td>\n",
       "      <td>-0.078734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.408075</td>\n",
       "      <td>Smucker</td>\n",
       "      <td>-1.008086</td>\n",
       "      <td>ember</td>\n",
       "      <td>-2.038125</td>\n",
       "      <td>0.337603</td>\n",
       "      <td>0.871125</td>\n",
       "      <td>0.289775</td>\n",
       "      <td>-0.487606</td>\n",
       "      <td>-0.411877</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.870792</td>\n",
       "      <td>-1.177626</td>\n",
       "      <td>-0.647604</td>\n",
       "      <td>0.394452</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>-0.326024</td>\n",
       "      <td>-0.432558</td>\n",
       "      <td>-0.351513</td>\n",
       "      <td>Mozart</td>\n",
       "      <td>1.347008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.477530</td>\n",
       "      <td>Pavlov</td>\n",
       "      <td>-0.193659</td>\n",
       "      <td>routine</td>\n",
       "      <td>-1.141689</td>\n",
       "      <td>1.024063</td>\n",
       "      <td>0.696387</td>\n",
       "      <td>-0.170185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.592527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.716822</td>\n",
       "      <td>1.173294</td>\n",
       "      <td>0.282479</td>\n",
       "      <td>-0.818199</td>\n",
       "      <td>0.088407</td>\n",
       "      <td>0.955305</td>\n",
       "      <td>-0.551186</td>\n",
       "      <td>-1.866537</td>\n",
       "      <td>they</td>\n",
       "      <td>-1.191372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.334501</td>\n",
       "      <td>lee</td>\n",
       "      <td>-0.653329</td>\n",
       "      <td>Betsey</td>\n",
       "      <td>-0.474945</td>\n",
       "      <td>2.143944</td>\n",
       "      <td>0.504987</td>\n",
       "      <td>-0.792521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.633919</td>\n",
       "      <td>...</td>\n",
       "      <td>1.765454</td>\n",
       "      <td>-0.071335</td>\n",
       "      <td>0.087142</td>\n",
       "      <td>-0.661786</td>\n",
       "      <td>-1.200296</td>\n",
       "      <td>0.865755</td>\n",
       "      <td>0.186454</td>\n",
       "      <td>0.404982</td>\n",
       "      <td>tribesman</td>\n",
       "      <td>0.029102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.059525</td>\n",
       "      <td>addition</td>\n",
       "      <td>-1.024388</td>\n",
       "      <td>embroider</td>\n",
       "      <td>-3.241267</td>\n",
       "      <td>-1.260884</td>\n",
       "      <td>0.443819</td>\n",
       "      <td>1.266911</td>\n",
       "      <td>2.122156</td>\n",
       "      <td>0.917862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252568</td>\n",
       "      <td>-1.207572</td>\n",
       "      <td>-0.689336</td>\n",
       "      <td>-1.519370</td>\n",
       "      <td>-0.926930</td>\n",
       "      <td>0.774634</td>\n",
       "      <td>1.032465</td>\n",
       "      <td>-1.247783</td>\n",
       "      <td>controlled</td>\n",
       "      <td>1.391755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.978764</td>\n",
       "      <td>ineluctable</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>metallurgic</td>\n",
       "      <td>-0.444293</td>\n",
       "      <td>-0.459361</td>\n",
       "      <td>1.037540</td>\n",
       "      <td>0.478980</td>\n",
       "      <td>0.830336</td>\n",
       "      <td>-0.849844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756989</td>\n",
       "      <td>-0.793714</td>\n",
       "      <td>-0.528785</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>-0.269875</td>\n",
       "      <td>-0.510016</td>\n",
       "      <td>-0.856084</td>\n",
       "      <td>-0.922165</td>\n",
       "      <td>geocentric</td>\n",
       "      <td>0.946218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.010205</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>-0.798297</td>\n",
       "      <td>nag</td>\n",
       "      <td>-0.176947</td>\n",
       "      <td>-0.552223</td>\n",
       "      <td>0.285865</td>\n",
       "      <td>-0.612789</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>0.632932</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.379319</td>\n",
       "      <td>1.312175</td>\n",
       "      <td>-0.886027</td>\n",
       "      <td>1.547505</td>\n",
       "      <td>0.658544</td>\n",
       "      <td>0.334457</td>\n",
       "      <td>-1.515744</td>\n",
       "      <td>-0.730930</td>\n",
       "      <td>recovery</td>\n",
       "      <td>-0.833116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.423599</td>\n",
       "      <td>razor</td>\n",
       "      <td>-1.067533</td>\n",
       "      <td>proboscis</td>\n",
       "      <td>2.062525</td>\n",
       "      <td>-0.447322</td>\n",
       "      <td>-0.334775</td>\n",
       "      <td>-0.643550</td>\n",
       "      <td>0.067856</td>\n",
       "      <td>1.281016</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.863789</td>\n",
       "      <td>0.780256</td>\n",
       "      <td>0.484733</td>\n",
       "      <td>-0.955123</td>\n",
       "      <td>-0.403648</td>\n",
       "      <td>0.852774</td>\n",
       "      <td>1.412221</td>\n",
       "      <td>hey</td>\n",
       "      <td>-2.030223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.651357</td>\n",
       "      <td>controversial</td>\n",
       "      <td>0.586364</td>\n",
       "      <td>sunrise</td>\n",
       "      <td>-0.528617</td>\n",
       "      <td>-0.167122</td>\n",
       "      <td>0.181022</td>\n",
       "      <td>0.662881</td>\n",
       "      <td>-0.248691</td>\n",
       "      <td>0.282580</td>\n",
       "      <td>...</td>\n",
       "      <td>1.238283</td>\n",
       "      <td>0.933440</td>\n",
       "      <td>0.304469</td>\n",
       "      <td>0.490975</td>\n",
       "      <td>0.399688</td>\n",
       "      <td>-1.296832</td>\n",
       "      <td>1.607346</td>\n",
       "      <td>0.021272</td>\n",
       "      <td>belong</td>\n",
       "      <td>-0.980944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.083106</td>\n",
       "      <td>lulu</td>\n",
       "      <td>0.760056</td>\n",
       "      <td>airborne</td>\n",
       "      <td>-1.504720</td>\n",
       "      <td>0.043602</td>\n",
       "      <td>-0.429302</td>\n",
       "      <td>-0.611769</td>\n",
       "      <td>-0.622649</td>\n",
       "      <td>1.695051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082440</td>\n",
       "      <td>1.279890</td>\n",
       "      <td>0.663523</td>\n",
       "      <td>-0.742471</td>\n",
       "      <td>-1.406317</td>\n",
       "      <td>-0.692421</td>\n",
       "      <td>0.194607</td>\n",
       "      <td>-1.457551</td>\n",
       "      <td>upkeep</td>\n",
       "      <td>-1.447232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.189017</td>\n",
       "      <td>Hackett</td>\n",
       "      <td>-0.570746</td>\n",
       "      <td>neuropsychiatric</td>\n",
       "      <td>1.420504</td>\n",
       "      <td>-1.122722</td>\n",
       "      <td>-0.623141</td>\n",
       "      <td>-0.637740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.832356</td>\n",
       "      <td>-0.270624</td>\n",
       "      <td>-1.692005</td>\n",
       "      <td>0.289169</td>\n",
       "      <td>-0.637387</td>\n",
       "      <td>-0.555477</td>\n",
       "      <td>0.492451</td>\n",
       "      <td>0.471416</td>\n",
       "      <td>Shakespearian</td>\n",
       "      <td>0.950573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          17          leg_1         7             arm_2         2        11  \\\n",
       "53 -0.803179       warranty -0.271124               bay  1.492689 -0.998385   \n",
       "62 -0.272724       abrogate -0.054295         beryllium -2.696887 -1.012104   \n",
       "52 -0.062593       buckskin -0.280675       businessman -0.753965 -1.389572   \n",
       "58 -0.428115           mask  0.850222                 g  1.500760 -1.294681   \n",
       "56  1.440117       chimeric  1.800940          clavicle -0.676392 -0.082151   \n",
       "76  1.001046       runabout  0.677875         certitude -2.703232  0.950308   \n",
       "1   1.375707          Runge  0.125576        bottleneck -0.150056  0.321357   \n",
       "47 -1.211016        wattage  0.047399              horn -0.651836 -0.662624   \n",
       "74  1.187679          Exxon  0.201160          shamrock -0.464617  1.615376   \n",
       "91 -0.953329           slim  1.624678            Gullah  0.122670 -2.362932   \n",
       "38  1.772252       Olivetti -0.457096       musculature  1.661259  0.140886   \n",
       "18 -0.600217    substantive -0.385314         carpentry  0.069802  1.632411   \n",
       "19  0.615936         Canaan -0.309546           jackdaw  0.593101 -0.524520   \n",
       "42 -0.843247           pick -0.175886         timetable  2.170943  0.511203   \n",
       "6  -0.575818        hairpin -2.301921           steeple -0.275052 -0.553649   \n",
       "12  1.490726        jewelry -0.337086           soliton  0.148667 -0.070499   \n",
       "48 -1.331233          vocal  0.507991         camelback -1.836205  1.213098   \n",
       "32  0.504047        sundown -0.792873            spline -0.530258 -2.067442   \n",
       "25  1.752270        Textron  1.271555       equidistant  0.935678 -0.254977   \n",
       "31 -0.553588    coprocessor  1.628397        deconvolve  0.568983  0.043811   \n",
       "87 -0.183983        Rollins  0.347582               Ned  0.018434 -0.037635   \n",
       "78  0.309821         lawful  0.413799             Jaime  0.633777 -1.243863   \n",
       "40  1.094192          leaky  1.529550             Titan -1.692465  0.481009   \n",
       "11 -0.408075        Smucker -1.008086             ember -2.038125  0.337603   \n",
       "49  1.477530         Pavlov -0.193659           routine -1.141689  1.024063   \n",
       "0  -0.334501            lee -0.653329            Betsey -0.474945  2.143944   \n",
       "89 -0.059525       addition -1.024388         embroider -3.241267 -1.260884   \n",
       "77 -0.978764    ineluctable  0.377300       metallurgic -0.444293 -0.459361   \n",
       "46  2.010205      Baltimore -0.798297               nag -0.176947 -0.552223   \n",
       "28  0.423599          razor -1.067533         proboscis  2.062525 -0.447322   \n",
       "16 -0.651357  controversial  0.586364           sunrise -0.528617 -0.167122   \n",
       "70 -0.083106           lulu  0.760056          airborne -1.504720  0.043602   \n",
       "2   1.189017        Hackett -0.570746  neuropsychiatric  1.420504 -1.122722   \n",
       "\n",
       "           4        16         0        14  ...        12        10        15  \\\n",
       "53 -0.226479 -0.877983  0.766080       NaN  ... -0.021367  2.368674  1.170775   \n",
       "62       NaN -0.335785  0.823171 -1.654857  ... -0.230935  1.658822 -0.749202   \n",
       "52  0.758929  0.594754  1.022570 -1.645399  ... -1.692957 -0.053198  1.460980   \n",
       "58  0.996267  0.076822 -0.467701  1.160827  ...       NaN  0.559426  1.724002   \n",
       "56  0.196521  0.642723  0.342725  1.117296  ... -0.040158  1.086594 -1.054638   \n",
       "76  0.975198 -0.927353       NaN       NaN  ... -0.654076  1.372711  0.500477   \n",
       "1        NaN  1.189470  1.613711  0.421921  ... -0.173072 -1.158068  0.963360   \n",
       "47       NaN  0.259723 -0.763259  0.570599  ...       NaN -1.559849 -1.113279   \n",
       "74 -0.903702  0.403730  1.217159 -0.322320  ...  0.283288  0.964233  0.193754   \n",
       "91 -0.645964 -0.182896  0.619154       NaN  ...  0.323079  1.178696  0.558320   \n",
       "38 -1.169917  0.513600  0.323168 -1.768439  ... -0.602212  1.533138  0.976874   \n",
       "18 -0.981509  1.163164 -0.440044 -1.430141  ...  0.113517  0.071236  0.367832   \n",
       "19 -0.360966  0.710960 -1.222128  0.489375  ...  0.326133  1.468308 -0.747180   \n",
       "42  0.075434  1.639965 -0.137449  1.373659  ...       NaN  0.327252  0.405709   \n",
       "6        NaN -0.112328  1.964725       NaN  ... -1.515191 -0.539630 -0.430668   \n",
       "12  1.530751 -1.209695  0.064474  0.486502  ... -0.613403 -0.331376  1.306542   \n",
       "48  0.708109  0.133541  2.319330  0.141717  ... -1.103367 -1.876538  0.647019   \n",
       "32 -0.062679 -0.513867 -1.304470 -0.089120  ... -0.107030 -1.407357 -0.873066   \n",
       "25       NaN -0.342688       NaN  1.503993  ...       NaN  1.154661 -1.032605   \n",
       "31 -0.803675 -0.088282  0.963879 -0.147002  ... -0.379128 -1.820377  0.655741   \n",
       "87 -1.348185  0.307802  0.114228  1.103302  ... -0.539760  0.714794 -1.292672   \n",
       "78 -0.835347 -0.673491       NaN -0.692905  ...       NaN -0.508331  1.281318   \n",
       "40 -0.989605  1.593187 -0.790474       NaN  ...       NaN  0.627375 -1.193006   \n",
       "11  0.871125  0.289775 -0.487606 -0.411877  ... -1.870792 -1.177626 -0.647604   \n",
       "49  0.696387 -0.170185       NaN  0.592527  ... -0.716822  1.173294  0.282479   \n",
       "0   0.504987 -0.792521       NaN  0.633919  ...  1.765454 -0.071335  0.087142   \n",
       "89  0.443819  1.266911  2.122156  0.917862  ... -0.252568 -1.207572 -0.689336   \n",
       "77  1.037540  0.478980  0.830336 -0.849844  ...  0.756989 -0.793714 -0.528785   \n",
       "46  0.285865 -0.612789  0.202923  0.632932  ... -1.379319  1.312175 -0.886027   \n",
       "28 -0.334775 -0.643550  0.067856  1.281016  ...       NaN  1.863789  0.780256   \n",
       "16  0.181022  0.662881 -0.248691  0.282580  ...  1.238283  0.933440  0.304469   \n",
       "70 -0.429302 -0.611769 -0.622649  1.695051  ...  0.082440  1.279890  0.663523   \n",
       "2  -0.623141 -0.637740       NaN  0.382410  ... -0.832356 -0.270624 -1.692005   \n",
       "\n",
       "           9        19        18         8         6          arm_0         1  \n",
       "53 -0.100154  0.913585  0.367366  1.226933 -0.747212        Riviera -2.654613  \n",
       "62 -1.289961 -0.245743 -1.503143  0.073318  0.696206        vernier -1.207273  \n",
       "52  1.384273  0.104201  0.281191  2.439752 -0.098340          crypt -0.558181  \n",
       "58 -0.046921 -1.556582 -0.493757  0.346504 -0.349258     impeccable -1.228234  \n",
       "56  0.569767 -0.089736  0.709004  0.456753 -1.430775       eyepiece -0.556581  \n",
       "76  0.070052  0.189582  0.501094 -0.168822 -1.830633          cacao -1.464473  \n",
       "1  -0.244157 -0.297564  0.701173  0.453534  0.015579         sloven  0.659924  \n",
       "47 -1.627542 -0.066080 -1.661520 -1.804882 -0.384556     everything  1.890331  \n",
       "74  0.998311 -1.179040  0.324359  1.521316 -0.258905     commissary -0.963142  \n",
       "91  0.020794 -0.482744 -0.799192  2.057495 -0.252354          Aruba -1.310899  \n",
       "38 -0.466037 -0.027515 -2.872262 -0.147603  0.468774      fieldwork -1.809219  \n",
       "18  1.441273  0.199060  0.462103  0.130741  0.662131        stretch -0.218004  \n",
       "19 -0.240325 -1.081063  1.159330  0.712998 -1.251114        lottery -1.033717  \n",
       "42  1.612278 -0.246062 -1.601966  0.952875  0.551485      conscious -0.468094  \n",
       "6  -0.699726 -0.530501  0.757508  0.035264  1.366874         nodule  0.672873  \n",
       "12 -0.939335 -0.213443  1.218762 -1.975467 -0.302470        pelagic -0.239385  \n",
       "48  0.192049 -0.785989  0.956702  0.393318 -2.152891           maim  1.449016  \n",
       "32  0.366598 -0.985726  0.955142  0.669673 -1.035242      anybody'd  1.650958  \n",
       "25  1.246085 -1.110576 -0.777817  1.091507 -1.129052         Keenan -0.628041  \n",
       "31 -0.557492  1.677701  1.639117  2.210523 -0.203580       shopworn  1.393983  \n",
       "87 -0.363612  0.170865  0.743264  0.150302 -0.778305        Wilhelm -0.117357  \n",
       "78 -0.294950 -1.187598  2.145149  0.894924 -0.129821        vocalic -0.066922  \n",
       "40  1.882024  0.055725 -0.125787  0.471468 -0.426881     compulsion -0.078734  \n",
       "11  0.394452  1.201214 -0.326024 -0.432558 -0.351513         Mozart  1.347008  \n",
       "49 -0.818199  0.088407  0.955305 -0.551186 -1.866537           they -1.191372  \n",
       "0  -0.661786 -1.200296  0.865755  0.186454  0.404982      tribesman  0.029102  \n",
       "89 -1.519370 -0.926930  0.774634  1.032465 -1.247783     controlled  1.391755  \n",
       "77  0.071566 -0.269875 -0.510016 -0.856084 -0.922165     geocentric  0.946218  \n",
       "46  1.547505  0.658544  0.334457 -1.515744 -0.730930       recovery -0.833116  \n",
       "28  0.484733 -0.955123 -0.403648  0.852774  1.412221            hey -2.030223  \n",
       "16  0.490975  0.399688 -1.296832  1.607346  0.021272         belong -0.980944  \n",
       "70 -0.742471 -1.406317 -0.692421  0.194607 -1.457551         upkeep -1.447232  \n",
       "2   0.289169 -0.637387 -0.555477  0.492451  0.471416  Shakespearian  0.950573  \n",
       "\n",
       "[33 rows x 24 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some data analysis here\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-083359799dd294d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "While crunching your data, you probably found two issues:\n",
    "\n",
    "1. There are 4 columns whose name starts with either `arm` or `leg` which are all filled with gibberish\n",
    "2. There are some values missing in some columns\n",
    "\n",
    "So, first things first, let's get rid of those columns through a Custom Transformer, so we can plug it in a Scikit Pipeline after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57e648bb4ed49423",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9663ebd68ad68f8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline step called RemoveLimbs that removes any\n",
    "# column whose name starts with the string 'arm' or 'leg'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def Remove_Limbs(table):\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    for colname in table.columns:\n",
    "        if colname.startswith('arm') or colname.startswith('leg'):\n",
    "            cols_to_drop.append(colname)\n",
    "\n",
    "    table.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return table\n",
    "\n",
    "Remove_Limbs(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5c8634dcc19dd7e1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RemoveLimbs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-82c9da05fdd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### BEGIN TESTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0m_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRemoveLimbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'salt5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'71443dfc3077d773d4c74e958dadf91dc2cc148a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0m_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'leg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRemoveLimbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'salt6'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ce45cf3759d2210f2d1315f1673b18f34e3ac711'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### END TESTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RemoveLimbs' is not defined"
     ]
    }
   ],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(sorted(RemoveLimbs().fit_transform(X).columns), 'salt5') == '71443dfc3077d773d4c74e958dadf91dc2cc148a'\n",
    "assert _hash(list(map(lambda col: col.startswith('arm') or col.startswith('leg'), RemoveLimbs().fit_transform(X_train).columns)), 'salt6') == 'ce45cf3759d2210f2d1315f1673b18f34e3ac711'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0ded8876cfda9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/monty_python_black_knight.gif\" width=500>\n",
    "\n",
    "Now that we have our Custom Transformer in place, we can design our pipeline. For the sake of the exercise, you'll want to create a pipeline with the following steps:\n",
    "\n",
    "1. Removes limbs columns\n",
    "2. Imputes missing values with the mean\n",
    "3. Has a Random Forest Classifier as the last step\n",
    "\n",
    "You may use `make_pipeline` to create your pipeline with as many steps as you want as long as the first two are the Custom Transformer you developed previously, a `SimpleImputer` as the second step, and a `RandomForestClassifier` as the last step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b18993e1e6a77d8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Scikit Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efabb79e960ce10b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6814224bb0f1332f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(pipeline.steps[0][0], 'salt7') == '471b02068ac2c4f479c2e9f85f4b3dc2179bb841'\n",
    "assert _hash(pipeline.steps[1][0], 'salt8') == 'ca83eaea1a7e243fa5574cfa6f52831166ee0f32'\n",
    "assert _hash(pipeline.steps[-1][0], 'salt9') == '0d66ba4309ad4939673169e74f87088dcadd510b'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53269c4aaa5f3df3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it work? Let's check it out on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd7deb2b78d36444",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4defdb6d3d670ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e887f18b73a6081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That's it for this Exercise Notebook! It doesn't get much cleaner than this, does it?\n",
    "\n",
    "You can still practice around with pipelines, maybe add a few more steps. See how you can adapt your pipeline and how it affects the predictions.\n",
    "\n",
    "Can you see how Scikit-learn's Pipelines might save time? Can you imagine how useful that would be in stressful situations (like, *for example*, an Hackathon)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
