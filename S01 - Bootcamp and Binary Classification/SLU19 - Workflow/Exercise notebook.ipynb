{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1f108c2efb94d44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4781dbdb443e5573",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Always have your imports at the top\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from hashlib import sha1 # just for grading purposes\n",
    "import json # just for grading purposes\n",
    "\n",
    "from utils import get_dataset, workflow_steps, data_analysis_steps\n",
    "\n",
    "def _hash(obj, salt='none'):\n",
    "    if type(obj) is not str:\n",
    "        obj = json.dumps(obj)\n",
    "    to_encode = obj + salt\n",
    "    return sha1(to_encode.encode()).hexdigest()\n",
    "\n",
    "X, y = get_dataset()  # preloaded dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0736ca1b894afc53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: ~The Larch~ Workflow steps\n",
    "\n",
    "What are the basic workflow steps?\n",
    "\n",
    "<img src=\"media/the_larch.jpg\" width=\"300\" />\n",
    "\n",
    "<p style=\"font-size: 9px; text-align:center\">A larch</p>\n",
    "\n",
    "You probably know them already, but we want you to really internalize them. We've given you a list of steps `workflow_steps`, but it appears that, not only does it have to many steps, some are _probably_ wrong, as well.\n",
    "\n",
    "Select the correct ones and reorder them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3f3abe0075f1f26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow steps:\n",
      "1 :  Watch Netflix\n",
      "2 :  Increase complexity\n",
      "3 :  Evaluate results\n",
      "4 :  Get the data\n",
      "5 :  Establish a Baseline\n",
      "6 :  Data analysis and preparation\n",
      "7 :  Train model\n",
      "8 :  Spam\n",
      "9 :  Google Hackathon solutions\n",
      "10 :  Iterate\n"
     ]
    }
   ],
   "source": [
    "print(\"Workflow steps:\")\n",
    "for i in range(len(workflow_steps)):\n",
    "    print(i+1, ': ', workflow_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c8218e1615228f0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1.1. Filter and sort the names of the steps in the workflow_steps list\n",
    "# workflow_steps_answer = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "workflow_steps_answer = ['Get the data', 'Data analysis and preparation', 'Train model','Evaluate results','Iterate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d15af97ae329a075",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash([step.lower() for step in workflow_steps_answer], 'salt0') == '701e2306da9bfde36382bdb6feb80a354916ebf4'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8feaa0f2674908a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are way too many substeps in the Data Analysis and Preparation step to group them all under a single category. We've given you another list of steps: `data_analysis_steps`.\n",
    "\n",
    "Aside from being shuffled, it should be fine but keep an eye out. You never know what to expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee99eb109e66ea78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis and Preparation steps:\n",
      "1 :  Feature engineering\n",
      "2 :  Dealing with data problems\n",
      "3 :  Spanish Inquisition\n",
      "4 :  Feature selection\n",
      "5 :  Data analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Analysis and Preparation steps:\")\n",
    "for i in range(len(data_analysis_steps)):\n",
    "    print(i+1, ': ', data_analysis_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e47f7838cbcf13d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1.2. Filter and sort the names of the steps in the data_analysis_steps list\n",
    "# data_analysis_steps_answer = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "data_analysis_steps_answer = [\"Data analysis\",\n",
    "                              \"Dealing with data problems\",\n",
    "                              \"Feature engineering\",\n",
    "                              \"Feature selection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eb6774d7694e5ada",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash([step.lower() for step in data_analysis_steps_answer], 'salt0') == '658ab90eff4a0cea2bfb51cc89c8db5b4121fa86'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27c62b7bf227baa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p style=\"text-align:center\">That's right! \n",
    "\n",
    "<p style=\"text-align:center\"><b>Nobody</b> expects the Spanish Inquisition!\n",
    "\n",
    "<img src=\"media/spanish_inquisition.gif\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb212306fe32d1fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Specific workflow questions\n",
    "\n",
    "Here are some more specific questions about individual workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53dadb4a6c1cb987",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2.1. True or False, you should split your data in a training and test set\n",
    "# split_training_test = ...\n",
    "\n",
    "# Exercise 2.2. True or False, Scikit Pipelines are only useful in production environments\n",
    "# scikit_pipelines_useful = ...\n",
    "\n",
    "# Exercise 2.3. True or False, you should try to make a complex baseline, so you just have \n",
    "#               to make simple improvements on it, later on.\n",
    "# baseline_complex = ...\n",
    "\n",
    "# Exercise 2.4. (optional) True or False, is Brian the Messiah?\n",
    "# is_brian_the_messiah = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "split_training_test = True\n",
    "scikit_pipelines_useful = False\n",
    "baseline_complex = False\n",
    "is_brian_the_messiah = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f1cfeb62360089e3",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(split_training_test, 'salt1') == '569b45c42b5c7b490c92692b911af35f575c8a06'\n",
    "assert _hash(scikit_pipelines_useful, 'salt2') == 'ef07576cc7d3bcb2cf29e1a772aec2aad7f59158'\n",
    "assert _hash(baseline_complex, 'salt3') == 'f24a294afb4a09f7f9df9ee13eb18e7d341c439d'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb240cb38ac7a823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/monty_python_messiah.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5790b6079fcdd3a1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Scikit Pipelines\n",
    "\n",
    "We've already loaded and splitted a dataset for the following exercises. They're stored in the `X_train`, `X_test`, `y_train` and `y_test` variables.\n",
    "\n",
    "In a perfect world, where you have all your data clean and ready-to-go, you can create your pipeline with just Scikit-learn's Transformers. However, in the real world, that's not the case, and you'll need to create custom Transformers to get the job done. Take a look at the data set, what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>leg_1</th>\n",
       "      <th>7</th>\n",
       "      <th>arm_2</th>\n",
       "      <th>2</th>\n",
       "      <th>11</th>\n",
       "      <th>4</th>\n",
       "      <th>16</th>\n",
       "      <th>0</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>9</th>\n",
       "      <th>19</th>\n",
       "      <th>18</th>\n",
       "      <th>8</th>\n",
       "      <th>6</th>\n",
       "      <th>arm_0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.086144</td>\n",
       "      <td>Locke</td>\n",
       "      <td>-2.921350</td>\n",
       "      <td>temptation</td>\n",
       "      <td>-1.072139</td>\n",
       "      <td>-0.734592</td>\n",
       "      <td>0.176442</td>\n",
       "      <td>0.428817</td>\n",
       "      <td>0.200569</td>\n",
       "      <td>-0.810252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436560</td>\n",
       "      <td>0.250588</td>\n",
       "      <td>0.046683</td>\n",
       "      <td>-1.015822</td>\n",
       "      <td>-0.827590</td>\n",
       "      <td>-0.367028</td>\n",
       "      <td>1.148637</td>\n",
       "      <td>0.903935</td>\n",
       "      <td>periscope</td>\n",
       "      <td>-0.248780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.438278</td>\n",
       "      <td>shrub</td>\n",
       "      <td>-0.668144</td>\n",
       "      <td>Erich</td>\n",
       "      <td>0.919229</td>\n",
       "      <td>-0.291811</td>\n",
       "      <td>-0.426358</td>\n",
       "      <td>0.298753</td>\n",
       "      <td>0.883110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873298</td>\n",
       "      <td>1.572337</td>\n",
       "      <td>0.778140</td>\n",
       "      <td>-0.180480</td>\n",
       "      <td>0.113270</td>\n",
       "      <td>1.148446</td>\n",
       "      <td>-0.077837</td>\n",
       "      <td>1.080048</td>\n",
       "      <td>yuh</td>\n",
       "      <td>-1.762549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.803179</td>\n",
       "      <td>warranty</td>\n",
       "      <td>-0.271124</td>\n",
       "      <td>bay</td>\n",
       "      <td>1.492689</td>\n",
       "      <td>-0.998385</td>\n",
       "      <td>-0.226479</td>\n",
       "      <td>-0.877983</td>\n",
       "      <td>0.766080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021367</td>\n",
       "      <td>2.368674</td>\n",
       "      <td>1.170775</td>\n",
       "      <td>-0.100154</td>\n",
       "      <td>0.913585</td>\n",
       "      <td>0.367366</td>\n",
       "      <td>1.226933</td>\n",
       "      <td>-0.747212</td>\n",
       "      <td>Riviera</td>\n",
       "      <td>-2.654613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.977555</td>\n",
       "      <td>SW</td>\n",
       "      <td>0.751387</td>\n",
       "      <td>value</td>\n",
       "      <td>0.099332</td>\n",
       "      <td>-0.592394</td>\n",
       "      <td>-0.576771</td>\n",
       "      <td>-0.238948</td>\n",
       "      <td>0.048522</td>\n",
       "      <td>-0.863991</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.669405</td>\n",
       "      <td>-1.693028</td>\n",
       "      <td>-0.838070</td>\n",
       "      <td>0.270457</td>\n",
       "      <td>0.500917</td>\n",
       "      <td>0.755391</td>\n",
       "      <td>-0.830950</td>\n",
       "      <td>0.543360</td>\n",
       "      <td>neuralgia</td>\n",
       "      <td>1.897924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.010205</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>-0.798297</td>\n",
       "      <td>nag</td>\n",
       "      <td>-0.176947</td>\n",
       "      <td>-0.552223</td>\n",
       "      <td>0.285865</td>\n",
       "      <td>-0.612789</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>0.632932</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.379319</td>\n",
       "      <td>1.312175</td>\n",
       "      <td>-0.886027</td>\n",
       "      <td>1.547505</td>\n",
       "      <td>0.658544</td>\n",
       "      <td>0.334457</td>\n",
       "      <td>-1.515744</td>\n",
       "      <td>-0.730930</td>\n",
       "      <td>recovery</td>\n",
       "      <td>-0.833116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.040592</td>\n",
       "      <td>Palmyra</td>\n",
       "      <td>-0.662901</td>\n",
       "      <td>nobody'd</td>\n",
       "      <td>-0.701992</td>\n",
       "      <td>-1.592994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.125225</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>0.440475</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402605</td>\n",
       "      <td>0.155132</td>\n",
       "      <td>1.519901</td>\n",
       "      <td>0.223914</td>\n",
       "      <td>0.048860</td>\n",
       "      <td>0.543298</td>\n",
       "      <td>0.552490</td>\n",
       "      <td>1.749577</td>\n",
       "      <td>liberal</td>\n",
       "      <td>-0.773361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.375707</td>\n",
       "      <td>Runge</td>\n",
       "      <td>0.125576</td>\n",
       "      <td>bottleneck</td>\n",
       "      <td>-0.150056</td>\n",
       "      <td>0.321357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.189470</td>\n",
       "      <td>1.613711</td>\n",
       "      <td>0.421921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173072</td>\n",
       "      <td>-1.158068</td>\n",
       "      <td>0.963360</td>\n",
       "      <td>-0.244157</td>\n",
       "      <td>-0.297564</td>\n",
       "      <td>0.701173</td>\n",
       "      <td>0.453534</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>sloven</td>\n",
       "      <td>0.659924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.883660</td>\n",
       "      <td>dearth</td>\n",
       "      <td>-1.576392</td>\n",
       "      <td>hint</td>\n",
       "      <td>0.652323</td>\n",
       "      <td>-0.483186</td>\n",
       "      <td>1.078681</td>\n",
       "      <td>1.683928</td>\n",
       "      <td>-1.225766</td>\n",
       "      <td>1.573987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.476540</td>\n",
       "      <td>-1.285680</td>\n",
       "      <td>0.889484</td>\n",
       "      <td>0.224452</td>\n",
       "      <td>-0.172627</td>\n",
       "      <td>-0.038508</td>\n",
       "      <td>-1.464375</td>\n",
       "      <td>1.380091</td>\n",
       "      <td>epicure</td>\n",
       "      <td>0.807427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.425458</td>\n",
       "      <td>deport</td>\n",
       "      <td>-0.047711</td>\n",
       "      <td>seem</td>\n",
       "      <td>-0.966976</td>\n",
       "      <td>0.128104</td>\n",
       "      <td>-1.583903</td>\n",
       "      <td>-0.452306</td>\n",
       "      <td>0.840644</td>\n",
       "      <td>-0.681052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>0.254157</td>\n",
       "      <td>-1.463612</td>\n",
       "      <td>-0.446183</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.760415</td>\n",
       "      <td>-0.652624</td>\n",
       "      <td>-1.158365</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>0.375316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.978764</td>\n",
       "      <td>ineluctable</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>metallurgic</td>\n",
       "      <td>-0.444293</td>\n",
       "      <td>-0.459361</td>\n",
       "      <td>1.037540</td>\n",
       "      <td>0.478980</td>\n",
       "      <td>0.830336</td>\n",
       "      <td>-0.849844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756989</td>\n",
       "      <td>-0.793714</td>\n",
       "      <td>-0.528785</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>-0.269875</td>\n",
       "      <td>-0.510016</td>\n",
       "      <td>-0.856084</td>\n",
       "      <td>-0.922165</td>\n",
       "      <td>geocentric</td>\n",
       "      <td>0.946218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.189017</td>\n",
       "      <td>Hackett</td>\n",
       "      <td>-0.570746</td>\n",
       "      <td>neuropsychiatric</td>\n",
       "      <td>1.420504</td>\n",
       "      <td>-1.122722</td>\n",
       "      <td>-0.623141</td>\n",
       "      <td>-0.637740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.832356</td>\n",
       "      <td>-0.270624</td>\n",
       "      <td>-1.692005</td>\n",
       "      <td>0.289169</td>\n",
       "      <td>-0.637387</td>\n",
       "      <td>-0.555477</td>\n",
       "      <td>0.492451</td>\n",
       "      <td>0.471416</td>\n",
       "      <td>Shakespearian</td>\n",
       "      <td>0.950573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.896839</td>\n",
       "      <td>Anabel</td>\n",
       "      <td>-0.891192</td>\n",
       "      <td>twit</td>\n",
       "      <td>-0.268531</td>\n",
       "      <td>-0.708407</td>\n",
       "      <td>-0.483061</td>\n",
       "      <td>-1.581191</td>\n",
       "      <td>-1.803140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.151815</td>\n",
       "      <td>0.341294</td>\n",
       "      <td>0.362054</td>\n",
       "      <td>0.267127</td>\n",
       "      <td>1.612221</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>-1.584136</td>\n",
       "      <td>-0.719153</td>\n",
       "      <td>Acton</td>\n",
       "      <td>-0.462814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.088749</td>\n",
       "      <td>tattle</td>\n",
       "      <td>1.543244</td>\n",
       "      <td>venturesome</td>\n",
       "      <td>1.884586</td>\n",
       "      <td>-0.082681</td>\n",
       "      <td>-0.226484</td>\n",
       "      <td>1.221034</td>\n",
       "      <td>1.513450</td>\n",
       "      <td>-0.121748</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.861991</td>\n",
       "      <td>0.846092</td>\n",
       "      <td>-1.024187</td>\n",
       "      <td>-0.372207</td>\n",
       "      <td>-0.959439</td>\n",
       "      <td>0.630812</td>\n",
       "      <td>-1.119617</td>\n",
       "      <td>hothouse</td>\n",
       "      <td>-1.140526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.755341</td>\n",
       "      <td>scarface</td>\n",
       "      <td>0.971571</td>\n",
       "      <td>ephemeris</td>\n",
       "      <td>-0.248964</td>\n",
       "      <td>0.711615</td>\n",
       "      <td>1.179297</td>\n",
       "      <td>1.551152</td>\n",
       "      <td>-1.534114</td>\n",
       "      <td>-1.124642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645376</td>\n",
       "      <td>-1.217220</td>\n",
       "      <td>-0.763516</td>\n",
       "      <td>0.332314</td>\n",
       "      <td>2.060748</td>\n",
       "      <td>0.067518</td>\n",
       "      <td>1.277677</td>\n",
       "      <td>1.368632</td>\n",
       "      <td>antebellum</td>\n",
       "      <td>1.431401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.546734</td>\n",
       "      <td>fragmentation</td>\n",
       "      <td>-0.366824</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>-0.238932</td>\n",
       "      <td>-1.006543</td>\n",
       "      <td>-0.999302</td>\n",
       "      <td>1.296995</td>\n",
       "      <td>1.317115</td>\n",
       "      <td>1.139879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391758</td>\n",
       "      <td>0.680573</td>\n",
       "      <td>0.631746</td>\n",
       "      <td>-2.121855</td>\n",
       "      <td>0.840620</td>\n",
       "      <td>-0.504775</td>\n",
       "      <td>-0.118069</td>\n",
       "      <td>-0.922410</td>\n",
       "      <td>bifurcate</td>\n",
       "      <td>-0.885418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.062593</td>\n",
       "      <td>buckskin</td>\n",
       "      <td>-0.280675</td>\n",
       "      <td>businessman</td>\n",
       "      <td>-0.753965</td>\n",
       "      <td>-1.389572</td>\n",
       "      <td>0.758929</td>\n",
       "      <td>0.594754</td>\n",
       "      <td>1.022570</td>\n",
       "      <td>-1.645399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.692957</td>\n",
       "      <td>-0.053198</td>\n",
       "      <td>1.460980</td>\n",
       "      <td>1.384273</td>\n",
       "      <td>0.104201</td>\n",
       "      <td>0.281191</td>\n",
       "      <td>2.439752</td>\n",
       "      <td>-0.098340</td>\n",
       "      <td>crypt</td>\n",
       "      <td>-0.558181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.575818</td>\n",
       "      <td>hairpin</td>\n",
       "      <td>-2.301921</td>\n",
       "      <td>steeple</td>\n",
       "      <td>-0.275052</td>\n",
       "      <td>-0.553649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.112328</td>\n",
       "      <td>1.964725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.515191</td>\n",
       "      <td>-0.539630</td>\n",
       "      <td>-0.430668</td>\n",
       "      <td>-0.699726</td>\n",
       "      <td>-0.530501</td>\n",
       "      <td>0.757508</td>\n",
       "      <td>0.035264</td>\n",
       "      <td>1.366874</td>\n",
       "      <td>nodule</td>\n",
       "      <td>0.672873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.353166</td>\n",
       "      <td>Debby</td>\n",
       "      <td>-0.295401</td>\n",
       "      <td>chimney</td>\n",
       "      <td>0.338484</td>\n",
       "      <td>-0.079641</td>\n",
       "      <td>0.579633</td>\n",
       "      <td>1.187386</td>\n",
       "      <td>-1.062394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168461</td>\n",
       "      <td>2.529834</td>\n",
       "      <td>0.884395</td>\n",
       "      <td>-0.187144</td>\n",
       "      <td>0.194384</td>\n",
       "      <td>0.325796</td>\n",
       "      <td>0.428307</td>\n",
       "      <td>1.317598</td>\n",
       "      <td>inferring</td>\n",
       "      <td>-2.683180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.190500</td>\n",
       "      <td>vexatious</td>\n",
       "      <td>-0.435486</td>\n",
       "      <td>scandalous</td>\n",
       "      <td>0.709452</td>\n",
       "      <td>0.672574</td>\n",
       "      <td>-0.535328</td>\n",
       "      <td>-2.172670</td>\n",
       "      <td>-0.132634</td>\n",
       "      <td>1.899882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513106</td>\n",
       "      <td>1.098853</td>\n",
       "      <td>0.703852</td>\n",
       "      <td>1.107081</td>\n",
       "      <td>0.331980</td>\n",
       "      <td>-0.090533</td>\n",
       "      <td>-0.974529</td>\n",
       "      <td>-0.259547</td>\n",
       "      <td>follow</td>\n",
       "      <td>-1.298263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.846707</td>\n",
       "      <td>lapelled</td>\n",
       "      <td>-0.359292</td>\n",
       "      <td>smutty</td>\n",
       "      <td>0.583928</td>\n",
       "      <td>-0.033127</td>\n",
       "      <td>-0.489439</td>\n",
       "      <td>2.526932</td>\n",
       "      <td>-0.517611</td>\n",
       "      <td>1.794558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590655</td>\n",
       "      <td>0.068456</td>\n",
       "      <td>-1.371117</td>\n",
       "      <td>-0.016423</td>\n",
       "      <td>0.681891</td>\n",
       "      <td>1.044161</td>\n",
       "      <td>0.223788</td>\n",
       "      <td>1.108704</td>\n",
       "      <td>heft</td>\n",
       "      <td>0.506885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.243339</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>0.352055</td>\n",
       "      <td>otherworld</td>\n",
       "      <td>-0.241236</td>\n",
       "      <td>0.243801</td>\n",
       "      <td>-1.525525</td>\n",
       "      <td>1.846637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.564079</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.560704</td>\n",
       "      <td>-0.722168</td>\n",
       "      <td>0.650201</td>\n",
       "      <td>-0.045586</td>\n",
       "      <td>-0.691908</td>\n",
       "      <td>0.872457</td>\n",
       "      <td>1.443765</td>\n",
       "      <td>frambesia</td>\n",
       "      <td>-1.128686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.504047</td>\n",
       "      <td>sundown</td>\n",
       "      <td>-0.792873</td>\n",
       "      <td>spline</td>\n",
       "      <td>-0.530258</td>\n",
       "      <td>-2.067442</td>\n",
       "      <td>-0.062679</td>\n",
       "      <td>-0.513867</td>\n",
       "      <td>-1.304470</td>\n",
       "      <td>-0.089120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107030</td>\n",
       "      <td>-1.407357</td>\n",
       "      <td>-0.873066</td>\n",
       "      <td>0.366598</td>\n",
       "      <td>-0.985726</td>\n",
       "      <td>0.955142</td>\n",
       "      <td>0.669673</td>\n",
       "      <td>-1.035242</td>\n",
       "      <td>anybody'd</td>\n",
       "      <td>1.650958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.622850</td>\n",
       "      <td>at</td>\n",
       "      <td>-0.142379</td>\n",
       "      <td>sheep</td>\n",
       "      <td>-1.067620</td>\n",
       "      <td>1.179440</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>-1.594428</td>\n",
       "      <td>-1.713135</td>\n",
       "      <td>-0.469176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120296</td>\n",
       "      <td>-1.544460</td>\n",
       "      <td>-0.793836</td>\n",
       "      <td>-0.114540</td>\n",
       "      <td>-0.450065</td>\n",
       "      <td>0.046981</td>\n",
       "      <td>1.353872</td>\n",
       "      <td>0.514439</td>\n",
       "      <td>damage</td>\n",
       "      <td>1.743551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.576904</td>\n",
       "      <td>mite</td>\n",
       "      <td>0.491919</td>\n",
       "      <td>rheology</td>\n",
       "      <td>-0.898415</td>\n",
       "      <td>2.189803</td>\n",
       "      <td>0.341756</td>\n",
       "      <td>-0.759133</td>\n",
       "      <td>-0.839722</td>\n",
       "      <td>-0.808298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.320233</td>\n",
       "      <td>-0.385668</td>\n",
       "      <td>0.109407</td>\n",
       "      <td>-2.123896</td>\n",
       "      <td>0.950424</td>\n",
       "      <td>1.876171</td>\n",
       "      <td>-0.599393</td>\n",
       "      <td>1.831459</td>\n",
       "      <td>apologia</td>\n",
       "      <td>0.307594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.953329</td>\n",
       "      <td>slim</td>\n",
       "      <td>1.624678</td>\n",
       "      <td>Gullah</td>\n",
       "      <td>0.122670</td>\n",
       "      <td>-2.362932</td>\n",
       "      <td>-0.645964</td>\n",
       "      <td>-0.182896</td>\n",
       "      <td>0.619154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323079</td>\n",
       "      <td>1.178696</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>-0.482744</td>\n",
       "      <td>-0.799192</td>\n",
       "      <td>2.057495</td>\n",
       "      <td>-0.252354</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>-1.310899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.600217</td>\n",
       "      <td>substantive</td>\n",
       "      <td>-0.385314</td>\n",
       "      <td>carpentry</td>\n",
       "      <td>0.069802</td>\n",
       "      <td>1.632411</td>\n",
       "      <td>-0.981509</td>\n",
       "      <td>1.163164</td>\n",
       "      <td>-0.440044</td>\n",
       "      <td>-1.430141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113517</td>\n",
       "      <td>0.071236</td>\n",
       "      <td>0.367832</td>\n",
       "      <td>1.441273</td>\n",
       "      <td>0.199060</td>\n",
       "      <td>0.462103</td>\n",
       "      <td>0.130741</td>\n",
       "      <td>0.662131</td>\n",
       "      <td>stretch</td>\n",
       "      <td>-0.218004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.914031</td>\n",
       "      <td>transfix</td>\n",
       "      <td>0.217433</td>\n",
       "      <td>studio</td>\n",
       "      <td>-0.190682</td>\n",
       "      <td>0.916328</td>\n",
       "      <td>0.612774</td>\n",
       "      <td>1.108183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870068</td>\n",
       "      <td>-2.585909</td>\n",
       "      <td>0.324237</td>\n",
       "      <td>2.088375</td>\n",
       "      <td>-0.623769</td>\n",
       "      <td>-1.053416</td>\n",
       "      <td>-2.896255</td>\n",
       "      <td>0.495682</td>\n",
       "      <td>don</td>\n",
       "      <td>2.232454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.487911</td>\n",
       "      <td>transshipping</td>\n",
       "      <td>-0.605715</td>\n",
       "      <td>Rockwell</td>\n",
       "      <td>2.157308</td>\n",
       "      <td>-0.172946</td>\n",
       "      <td>-0.605616</td>\n",
       "      <td>-0.669073</td>\n",
       "      <td>-1.371901</td>\n",
       "      <td>1.711708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742095</td>\n",
       "      <td>-0.782841</td>\n",
       "      <td>1.137462</td>\n",
       "      <td>1.471170</td>\n",
       "      <td>0.677926</td>\n",
       "      <td>1.826010</td>\n",
       "      <td>-1.613561</td>\n",
       "      <td>0.299293</td>\n",
       "      <td>Noah</td>\n",
       "      <td>0.244121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.331233</td>\n",
       "      <td>vocal</td>\n",
       "      <td>0.507991</td>\n",
       "      <td>camelback</td>\n",
       "      <td>-1.836205</td>\n",
       "      <td>1.213098</td>\n",
       "      <td>0.708109</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>2.319330</td>\n",
       "      <td>0.141717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.103367</td>\n",
       "      <td>-1.876538</td>\n",
       "      <td>0.647019</td>\n",
       "      <td>0.192049</td>\n",
       "      <td>-0.785989</td>\n",
       "      <td>0.956702</td>\n",
       "      <td>0.393318</td>\n",
       "      <td>-2.152891</td>\n",
       "      <td>maim</td>\n",
       "      <td>1.449016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.553588</td>\n",
       "      <td>coprocessor</td>\n",
       "      <td>1.628397</td>\n",
       "      <td>deconvolve</td>\n",
       "      <td>0.568983</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>-0.803675</td>\n",
       "      <td>-0.088282</td>\n",
       "      <td>0.963879</td>\n",
       "      <td>-0.147002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379128</td>\n",
       "      <td>-1.820377</td>\n",
       "      <td>0.655741</td>\n",
       "      <td>-0.557492</td>\n",
       "      <td>1.677701</td>\n",
       "      <td>1.639117</td>\n",
       "      <td>2.210523</td>\n",
       "      <td>-0.203580</td>\n",
       "      <td>shopworn</td>\n",
       "      <td>1.393983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.187679</td>\n",
       "      <td>Exxon</td>\n",
       "      <td>0.201160</td>\n",
       "      <td>shamrock</td>\n",
       "      <td>-0.464617</td>\n",
       "      <td>1.615376</td>\n",
       "      <td>-0.903702</td>\n",
       "      <td>0.403730</td>\n",
       "      <td>1.217159</td>\n",
       "      <td>-0.322320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283288</td>\n",
       "      <td>0.964233</td>\n",
       "      <td>0.193754</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>-1.179040</td>\n",
       "      <td>0.324359</td>\n",
       "      <td>1.521316</td>\n",
       "      <td>-0.258905</td>\n",
       "      <td>commissary</td>\n",
       "      <td>-0.963142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.125489</td>\n",
       "      <td>jackdaw</td>\n",
       "      <td>0.129221</td>\n",
       "      <td>interruptible</td>\n",
       "      <td>2.445752</td>\n",
       "      <td>0.869606</td>\n",
       "      <td>0.654366</td>\n",
       "      <td>-1.778720</td>\n",
       "      <td>0.413435</td>\n",
       "      <td>1.355638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109395</td>\n",
       "      <td>-1.548510</td>\n",
       "      <td>-0.734265</td>\n",
       "      <td>-0.773789</td>\n",
       "      <td>0.279969</td>\n",
       "      <td>-0.055585</td>\n",
       "      <td>1.876796</td>\n",
       "      <td>0.725767</td>\n",
       "      <td>chapati</td>\n",
       "      <td>1.722513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.224765</td>\n",
       "      <td>Berniece</td>\n",
       "      <td>-1.801980</td>\n",
       "      <td>banish</td>\n",
       "      <td>-1.170113</td>\n",
       "      <td>-0.388177</td>\n",
       "      <td>-2.553921</td>\n",
       "      <td>0.949554</td>\n",
       "      <td>0.160574</td>\n",
       "      <td>0.170416</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.808045</td>\n",
       "      <td>1.134462</td>\n",
       "      <td>0.436938</td>\n",
       "      <td>-1.366879</td>\n",
       "      <td>0.934320</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.759155</td>\n",
       "      <td>emptyhanded</td>\n",
       "      <td>0.268438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          17           leg_1         7             arm_2         2        11  \\\n",
       "93  0.086144           Locke -2.921350        temptation -1.072139 -0.734592   \n",
       "39 -1.438278           shrub -0.668144             Erich  0.919229 -0.291811   \n",
       "53 -0.803179        warranty -0.271124               bay  1.492689 -0.998385   \n",
       "71 -0.977555              SW  0.751387             value  0.099332 -0.592394   \n",
       "46  2.010205       Baltimore -0.798297               nag -0.176947 -0.552223   \n",
       "36  0.040592         Palmyra -0.662901          nobody'd -0.701992 -1.592994   \n",
       "1   1.375707           Runge  0.125576        bottleneck -0.150056  0.321357   \n",
       "5   0.883660          dearth -1.576392              hint  0.652323 -0.483186   \n",
       "14  0.425458          deport -0.047711              seem -0.966976  0.128104   \n",
       "77 -0.978764     ineluctable  0.377300       metallurgic -0.444293 -0.459361   \n",
       "2   1.189017         Hackett -0.570746  neuropsychiatric  1.420504 -1.122722   \n",
       "80  0.896839          Anabel -0.891192              twit -0.268531 -0.708407   \n",
       "75  1.088749          tattle  1.543244       venturesome  1.884586 -0.082681   \n",
       "95  1.755341        scarface  0.971571         ephemeris -0.248964  0.711615   \n",
       "86  0.546734   fragmentation -0.366824            Reagan -0.238932 -1.006543   \n",
       "52 -0.062593        buckskin -0.280675       businessman -0.753965 -1.389572   \n",
       "6  -0.575818         hairpin -2.301921           steeple -0.275052 -0.553649   \n",
       "15 -0.353166           Debby -0.295401           chimney  0.338484 -0.079641   \n",
       "45  0.190500       vexatious -0.435486        scandalous  0.709452  0.672574   \n",
       "88  1.846707        lapelled -0.359292            smutty  0.583928 -0.033127   \n",
       "84  0.243339  multiplicative  0.352055        otherworld -0.241236  0.243801   \n",
       "32  0.504047         sundown -0.792873            spline -0.530258 -2.067442   \n",
       "55  0.622850              at -0.142379             sheep -1.067620  1.179440   \n",
       "81 -0.576904            mite  0.491919          rheology -0.898415  2.189803   \n",
       "91 -0.953329            slim  1.624678            Gullah  0.122670 -2.362932   \n",
       "18 -0.600217     substantive -0.385314         carpentry  0.069802  1.632411   \n",
       "44  1.914031        transfix  0.217433            studio -0.190682  0.916328   \n",
       "34 -0.487911   transshipping -0.605715          Rockwell  2.157308 -0.172946   \n",
       "48 -1.331233           vocal  0.507991         camelback -1.836205  1.213098   \n",
       "31 -0.553588     coprocessor  1.628397        deconvolve  0.568983  0.043811   \n",
       "74  1.187679           Exxon  0.201160          shamrock -0.464617  1.615376   \n",
       "29 -1.125489         jackdaw  0.129221     interruptible  2.445752  0.869606   \n",
       "30 -0.224765        Berniece -1.801980            banish -1.170113 -0.388177   \n",
       "\n",
       "           4        16         0        14  ...        12        10        15  \\\n",
       "93  0.176442  0.428817  0.200569 -0.810252  ...  0.436560  0.250588  0.046683   \n",
       "39 -0.426358  0.298753  0.883110       NaN  ...  1.873298  1.572337  0.778140   \n",
       "53 -0.226479 -0.877983  0.766080       NaN  ... -0.021367  2.368674  1.170775   \n",
       "71 -0.576771 -0.238948  0.048522 -0.863991  ... -1.669405 -1.693028 -0.838070   \n",
       "46  0.285865 -0.612789  0.202923  0.632932  ... -1.379319  1.312175 -0.886027   \n",
       "36       NaN  0.125225 -0.019638  0.440475  ... -1.402605  0.155132  1.519901   \n",
       "1        NaN  1.189470  1.613711  0.421921  ... -0.173072 -1.158068  0.963360   \n",
       "5   1.078681  1.683928 -1.225766  1.573987  ...  1.476540 -1.285680  0.889484   \n",
       "14 -1.583903 -0.452306  0.840644 -0.681052  ... -0.003603  0.254157 -1.463612   \n",
       "77  1.037540  0.478980  0.830336 -0.849844  ...  0.756989 -0.793714 -0.528785   \n",
       "2  -0.623141 -0.637740       NaN  0.382410  ... -0.832356 -0.270624 -1.692005   \n",
       "80 -0.483061 -1.581191 -1.803140       NaN  ... -2.151815  0.341294  0.362054   \n",
       "75 -0.226484  1.221034  1.513450 -0.121748  ...       NaN  0.861991  0.846092   \n",
       "95  1.179297  1.551152 -1.534114 -1.124642  ...  0.645376 -1.217220 -0.763516   \n",
       "86 -0.999302  1.296995  1.317115  1.139879  ... -0.391758  0.680573  0.631746   \n",
       "52  0.758929  0.594754  1.022570 -1.645399  ... -1.692957 -0.053198  1.460980   \n",
       "6        NaN -0.112328  1.964725       NaN  ... -1.515191 -0.539630 -0.430668   \n",
       "15  0.579633  1.187386 -1.062394       NaN  ...  0.168461  2.529834  0.884395   \n",
       "45 -0.535328 -2.172670 -0.132634  1.899882  ...  0.513106  1.098853  0.703852   \n",
       "88 -0.489439  2.526932 -0.517611  1.794558  ...  0.590655  0.068456 -1.371117   \n",
       "84 -1.525525  1.846637       NaN -0.564079  ...       NaN  1.560704 -0.722168   \n",
       "32 -0.062679 -0.513867 -1.304470 -0.089120  ... -0.107030 -1.407357 -0.873066   \n",
       "55  0.005244 -1.594428 -1.713135 -0.469176  ...  0.120296 -1.544460 -0.793836   \n",
       "81  0.341756 -0.759133 -0.839722 -0.808298  ... -1.320233 -0.385668  0.109407   \n",
       "91 -0.645964 -0.182896  0.619154       NaN  ...  0.323079  1.178696  0.558320   \n",
       "18 -0.981509  1.163164 -0.440044 -1.430141  ...  0.113517  0.071236  0.367832   \n",
       "44  0.612774  1.108183       NaN  0.346488  ...  0.870068 -2.585909  0.324237   \n",
       "34 -0.605616 -0.669073 -1.371901  1.711708  ...  0.742095 -0.782841  1.137462   \n",
       "48  0.708109  0.133541  2.319330  0.141717  ... -1.103367 -1.876538  0.647019   \n",
       "31 -0.803675 -0.088282  0.963879 -0.147002  ... -0.379128 -1.820377  0.655741   \n",
       "74 -0.903702  0.403730  1.217159 -0.322320  ...  0.283288  0.964233  0.193754   \n",
       "29  0.654366 -1.778720  0.413435  1.355638  ...  0.109395 -1.548510 -0.734265   \n",
       "30 -2.553921  0.949554  0.160574  0.170416  ...       NaN -0.808045  1.134462   \n",
       "\n",
       "           9        19        18         8         6          arm_0         1  \n",
       "93 -1.015822 -0.827590 -0.367028  1.148637  0.903935      periscope -0.248780  \n",
       "39 -0.180480  0.113270  1.148446 -0.077837  1.080048            yuh -1.762549  \n",
       "53 -0.100154  0.913585  0.367366  1.226933 -0.747212        Riviera -2.654613  \n",
       "71  0.270457  0.500917  0.755391 -0.830950  0.543360      neuralgia  1.897924  \n",
       "46  1.547505  0.658544  0.334457 -1.515744 -0.730930       recovery -0.833116  \n",
       "36  0.223914  0.048860  0.543298  0.552490  1.749577        liberal -0.773361  \n",
       "1  -0.244157 -0.297564  0.701173  0.453534  0.015579         sloven  0.659924  \n",
       "5   0.224452 -0.172627 -0.038508 -1.464375  1.380091        epicure  0.807427  \n",
       "14 -0.446183  0.785800  0.760415 -0.652624 -1.158365        Clayton  0.375316  \n",
       "77  0.071566 -0.269875 -0.510016 -0.856084 -0.922165     geocentric  0.946218  \n",
       "2   0.289169 -0.637387 -0.555477  0.492451  0.471416  Shakespearian  0.950573  \n",
       "80  0.267127  1.612221  0.146793 -1.584136 -0.719153          Acton -0.462814  \n",
       "75 -1.024187 -0.372207 -0.959439  0.630812 -1.119617       hothouse -1.140526  \n",
       "95  0.332314  2.060748  0.067518  1.277677  1.368632     antebellum  1.431401  \n",
       "86 -2.121855  0.840620 -0.504775 -0.118069 -0.922410      bifurcate -0.885418  \n",
       "52  1.384273  0.104201  0.281191  2.439752 -0.098340          crypt -0.558181  \n",
       "6  -0.699726 -0.530501  0.757508  0.035264  1.366874         nodule  0.672873  \n",
       "15 -0.187144  0.194384  0.325796  0.428307  1.317598      inferring -2.683180  \n",
       "45  1.107081  0.331980 -0.090533 -0.974529 -0.259547         follow -1.298263  \n",
       "88 -0.016423  0.681891  1.044161  0.223788  1.108704           heft  0.506885  \n",
       "84  0.650201 -0.045586 -0.691908  0.872457  1.443765      frambesia -1.128686  \n",
       "32  0.366598 -0.985726  0.955142  0.669673 -1.035242      anybody'd  1.650958  \n",
       "55 -0.114540 -0.450065  0.046981  1.353872  0.514439         damage  1.743551  \n",
       "81 -2.123896  0.950424  1.876171 -0.599393  1.831459       apologia  0.307594  \n",
       "91  0.020794 -0.482744 -0.799192  2.057495 -0.252354          Aruba -1.310899  \n",
       "18  1.441273  0.199060  0.462103  0.130741  0.662131        stretch -0.218004  \n",
       "44  2.088375 -0.623769 -1.053416 -2.896255  0.495682            don  2.232454  \n",
       "34  1.471170  0.677926  1.826010 -1.613561  0.299293           Noah  0.244121  \n",
       "48  0.192049 -0.785989  0.956702  0.393318 -2.152891           maim  1.449016  \n",
       "31 -0.557492  1.677701  1.639117  2.210523 -0.203580       shopworn  1.393983  \n",
       "74  0.998311 -1.179040  0.324359  1.521316 -0.258905     commissary -0.963142  \n",
       "29 -0.773789  0.279969 -0.055585  1.876796  0.725767        chapati  1.722513  \n",
       "30  0.436938 -1.366879  0.934320  0.003046  0.759155    emptyhanded  0.268438  \n",
       "\n",
       "[33 rows x 24 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some data analysis here\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-083359799dd294d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "While crunching your data, you probably found two issues:\n",
    "\n",
    "1. There are 4 columns whose name starts with either `arm` or `leg` which are all filled with gibberish\n",
    "2. There are some values missing in some columns\n",
    "\n",
    "So, first things first, let's get rid of those columns through a Custom Transformer, so we can plug it in a Scikit Pipeline after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57e648bb4ed49423",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9663ebd68ad68f8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline step called RemoveLimbs that removes any\n",
    "# column whose name starts with the string 'arm' or 'leg'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "class RemoveLimbs(TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        return X.select_dtypes(exclude='object').copy()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5c8634dcc19dd7e1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(sorted(RemoveLimbs().fit_transform(X).columns), 'salt5') == '71443dfc3077d773d4c74e958dadf91dc2cc148a'\n",
    "assert _hash(list(map(lambda col: col.startswith('arm') or col.startswith('leg'), RemoveLimbs().fit_transform(X_train).columns)), 'salt6') == 'ce45cf3759d2210f2d1315f1673b18f34e3ac711'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0ded8876cfda9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/monty_python_black_knight.gif\" width=500>\n",
    "\n",
    "Now that we have our Custom Transformer in place, we can design our pipeline. For the sake of the exercise, you'll want to create a pipeline with the following steps:\n",
    "\n",
    "1. Removes limbs columns\n",
    "2. Imputes missing values with the mean\n",
    "3. Has a Random Forest Classifier as the last step\n",
    "\n",
    "You may use `make_pipeline` to create your pipeline with as many steps as you want as long as the first two are the Custom Transformer you developed previously, a `SimpleImputer` as the second step, and a `RandomForestClassifier` as the last step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b18993e1e6a77d8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Scikit Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efabb79e960ce10b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    RemoveLimbs(),\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestClassifier(n_estimators=10)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6814224bb0f1332f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(pipeline.steps[0][0], 'salt7') == '471b02068ac2c4f479c2e9f85f4b3dc2179bb841'\n",
    "assert _hash(pipeline.steps[1][0], 'salt8') == 'ca83eaea1a7e243fa5574cfa6f52831166ee0f32'\n",
    "assert _hash(pipeline.steps[-1][0], 'salt9') == '0d66ba4309ad4939673169e74f87088dcadd510b'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53269c4aaa5f3df3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it work? Let's check it out on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd7deb2b78d36444",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('removelimbs',\n",
       "                 <__main__.RemoveLimbs object at 0x7f9399e102e8>),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(n_estimators=10))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4defdb6d3d670ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e887f18b73a6081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That's it for this Exercise Notebook! It doesn't get much cleaner than this, does it?\n",
    "\n",
    "You can still practice around with pipelines, maybe add a few more steps. See how you can adapt your pipeline and how it affects the predictions.\n",
    "\n",
    "Can you see how Scikit-learn's Pipelines might save time? Can you imagine how useful that would be in stressful situations (like, *for example*, an Hackathon)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
