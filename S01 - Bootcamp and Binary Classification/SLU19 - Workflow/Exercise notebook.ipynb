{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1f108c2efb94d44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4781dbdb443e5573",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Always have your imports at the top\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from hashlib import sha1 # just for grading purposes\n",
    "import json # just for grading purposes\n",
    "\n",
    "from utils import get_dataset, workflow_steps, data_analysis_steps\n",
    "\n",
    "def _hash(obj, salt='none'):\n",
    "    if type(obj) is not str:\n",
    "        obj = json.dumps(obj)\n",
    "    to_encode = obj + salt\n",
    "    return sha1(to_encode.encode()).hexdigest()\n",
    "\n",
    "X, y = get_dataset()  # preloaded dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0736ca1b894afc53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: ~The Larch~ Workflow steps\n",
    "\n",
    "What are the basic workflow steps?\n",
    "\n",
    "<img src=\"media/the_larch.jpg\" width=\"300\" />\n",
    "\n",
    "<p style=\"font-size: 9px; text-align:center\">A larch</p>\n",
    "\n",
    "You probably know them already, but we want you to really internalize them. We've given you a list of steps `workflow_steps`, but it appears that, not only does it have to many steps, some are _probably_ wrong, as well.\n",
    "\n",
    "Select the correct ones and reorder them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3f3abe0075f1f26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow steps:\n",
      "1 :  Watch Netflix\n",
      "2 :  Increase complexity\n",
      "3 :  Evaluate results\n",
      "4 :  Get the data\n",
      "5 :  Establish a Baseline\n",
      "6 :  Data analysis and preparation\n",
      "7 :  Train model\n",
      "8 :  Spam\n",
      "9 :  Google Hackathon solutions\n",
      "10 :  Iterate\n"
     ]
    }
   ],
   "source": [
    "print(\"Workflow steps:\")\n",
    "for i in range(len(workflow_steps)):\n",
    "    print(i+1, ': ', workflow_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c8218e1615228f0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1.1. Filter and sort the names of the steps in the workflow_steps list\n",
    "# workflow_steps_answer = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "workflow_steps_answer = ['Get the data', 'Data analysis and preparation', 'Train model','Evaluate results','Iterate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d15af97ae329a075",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash([step.lower() for step in workflow_steps_answer], 'salt0') == '701e2306da9bfde36382bdb6feb80a354916ebf4'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8feaa0f2674908a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are way too many substeps in the Data Analysis and Preparation step to group them all under a single category. We've given you another list of steps: `data_analysis_steps`.\n",
    "\n",
    "Aside from being shuffled, it should be fine but keep an eye out. You never know what to expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee99eb109e66ea78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis and Preparation steps:\n",
      "1 :  Feature engineering\n",
      "2 :  Dealing with data problems\n",
      "3 :  Spanish Inquisition\n",
      "4 :  Feature selection\n",
      "5 :  Data analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Analysis and Preparation steps:\")\n",
    "for i in range(len(data_analysis_steps)):\n",
    "    print(i+1, ': ', data_analysis_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e47f7838cbcf13d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1.2. Filter and sort the names of the steps in the data_analysis_steps list\n",
    "# data_analysis_steps_answer = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "data_analysis_steps_answer = [\"Data analysis\",\n",
    "                              \"Dealing with data problems\",\n",
    "                              \"Feature engineering\",\n",
    "                              \"Feature selection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eb6774d7694e5ada",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash([step.lower() for step in data_analysis_steps_answer], 'salt0') == '658ab90eff4a0cea2bfb51cc89c8db5b4121fa86'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27c62b7bf227baa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p style=\"text-align:center\">That's right! \n",
    "\n",
    "<p style=\"text-align:center\"><b>Nobody</b> expects the Spanish Inquisition!\n",
    "\n",
    "<img src=\"media/spanish_inquisition.gif\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb212306fe32d1fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Specific workflow questions\n",
    "\n",
    "Here are some more specific questions about individual workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53dadb4a6c1cb987",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2.1. True or False, you should split your data in a training and test set\n",
    "# split_training_test = ...\n",
    "\n",
    "# Exercise 2.2. True or False, Scikit Pipelines are only useful in production environments\n",
    "# scikit_pipelines_useful = ...\n",
    "\n",
    "# Exercise 2.3. True or False, you should try to make a complex baseline, so you just have \n",
    "#               to make simple improvements on it, later on.\n",
    "# baseline_complex = ...\n",
    "\n",
    "# Exercise 2.4. (optional) True or False, is Brian the Messiah?\n",
    "# is_brian_the_messiah = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "split_training_test = True\n",
    "scikit_pipelines_useful = False\n",
    "baseline_complex = False\n",
    "is_brian_the_messiah = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f1cfeb62360089e3",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(split_training_test, 'salt1') == '569b45c42b5c7b490c92692b911af35f575c8a06'\n",
    "assert _hash(scikit_pipelines_useful, 'salt2') == 'ef07576cc7d3bcb2cf29e1a772aec2aad7f59158'\n",
    "assert _hash(baseline_complex, 'salt3') == 'f24a294afb4a09f7f9df9ee13eb18e7d341c439d'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb240cb38ac7a823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/monty_python_messiah.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5790b6079fcdd3a1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Scikit Pipelines\n",
    "\n",
    "We've already loaded and splitted a dataset for the following exercises. They're stored in the `X_train`, `X_test`, `y_train` and `y_test` variables.\n",
    "\n",
    "In a perfect world, where you have all your data clean and ready-to-go, you can create your pipeline with just Scikit-learn's Transformers. However, in the real world, that's not the case, and you'll need to create custom Transformers to get the job done. Take a look at the data set, what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>leg_1</th>\n",
       "      <th>7</th>\n",
       "      <th>arm_2</th>\n",
       "      <th>2</th>\n",
       "      <th>11</th>\n",
       "      <th>4</th>\n",
       "      <th>16</th>\n",
       "      <th>0</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>9</th>\n",
       "      <th>19</th>\n",
       "      <th>18</th>\n",
       "      <th>8</th>\n",
       "      <th>6</th>\n",
       "      <th>arm_0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.010205</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>-0.798297</td>\n",
       "      <td>nag</td>\n",
       "      <td>-0.176947</td>\n",
       "      <td>-0.552223</td>\n",
       "      <td>0.285865</td>\n",
       "      <td>-0.612789</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>0.632932</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.379319</td>\n",
       "      <td>1.312175</td>\n",
       "      <td>-0.886027</td>\n",
       "      <td>1.547505</td>\n",
       "      <td>0.658544</td>\n",
       "      <td>0.334457</td>\n",
       "      <td>-1.515744</td>\n",
       "      <td>-0.730930</td>\n",
       "      <td>recovery</td>\n",
       "      <td>-0.833116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1.279577</td>\n",
       "      <td>faucet</td>\n",
       "      <td>0.026091</td>\n",
       "      <td>withe</td>\n",
       "      <td>-0.624819</td>\n",
       "      <td>0.195845</td>\n",
       "      <td>1.665474</td>\n",
       "      <td>0.256030</td>\n",
       "      <td>0.408253</td>\n",
       "      <td>-0.978373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>1.505752</td>\n",
       "      <td>-0.678261</td>\n",
       "      <td>1.029156</td>\n",
       "      <td>-1.840874</td>\n",
       "      <td>1.014370</td>\n",
       "      <td>-1.702584</td>\n",
       "      <td>-0.725744</td>\n",
       "      <td>doze</td>\n",
       "      <td>-1.096621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.331233</td>\n",
       "      <td>vocal</td>\n",
       "      <td>0.507991</td>\n",
       "      <td>camelback</td>\n",
       "      <td>-1.836205</td>\n",
       "      <td>1.213098</td>\n",
       "      <td>0.708109</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>2.319330</td>\n",
       "      <td>0.141717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.103367</td>\n",
       "      <td>-1.876538</td>\n",
       "      <td>0.647019</td>\n",
       "      <td>0.192049</td>\n",
       "      <td>-0.785989</td>\n",
       "      <td>0.956702</td>\n",
       "      <td>0.393318</td>\n",
       "      <td>-2.152891</td>\n",
       "      <td>maim</td>\n",
       "      <td>1.449016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.423599</td>\n",
       "      <td>razor</td>\n",
       "      <td>-1.067533</td>\n",
       "      <td>proboscis</td>\n",
       "      <td>2.062525</td>\n",
       "      <td>-0.447322</td>\n",
       "      <td>-0.334775</td>\n",
       "      <td>-0.643550</td>\n",
       "      <td>0.067856</td>\n",
       "      <td>1.281016</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.863789</td>\n",
       "      <td>0.780256</td>\n",
       "      <td>0.484733</td>\n",
       "      <td>-0.955123</td>\n",
       "      <td>-0.403648</td>\n",
       "      <td>0.852774</td>\n",
       "      <td>1.412221</td>\n",
       "      <td>hey</td>\n",
       "      <td>-2.030223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.796026</td>\n",
       "      <td>Apocrypha</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>foyer</td>\n",
       "      <td>1.076007</td>\n",
       "      <td>-0.309209</td>\n",
       "      <td>0.213197</td>\n",
       "      <td>-0.160133</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>-0.752156</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825848</td>\n",
       "      <td>0.211646</td>\n",
       "      <td>-1.875172</td>\n",
       "      <td>-0.319054</td>\n",
       "      <td>-0.751969</td>\n",
       "      <td>1.340450</td>\n",
       "      <td>-0.060661</td>\n",
       "      <td>bistate</td>\n",
       "      <td>-0.843897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.190500</td>\n",
       "      <td>vexatious</td>\n",
       "      <td>-0.435486</td>\n",
       "      <td>scandalous</td>\n",
       "      <td>0.709452</td>\n",
       "      <td>0.672574</td>\n",
       "      <td>-0.535328</td>\n",
       "      <td>-2.172670</td>\n",
       "      <td>-0.132634</td>\n",
       "      <td>1.899882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513106</td>\n",
       "      <td>1.098853</td>\n",
       "      <td>0.703852</td>\n",
       "      <td>1.107081</td>\n",
       "      <td>0.331980</td>\n",
       "      <td>-0.090533</td>\n",
       "      <td>-0.974529</td>\n",
       "      <td>-0.259547</td>\n",
       "      <td>follow</td>\n",
       "      <td>-1.298263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.309821</td>\n",
       "      <td>lawful</td>\n",
       "      <td>0.413799</td>\n",
       "      <td>Jaime</td>\n",
       "      <td>0.633777</td>\n",
       "      <td>-1.243863</td>\n",
       "      <td>-0.835347</td>\n",
       "      <td>-0.673491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.692905</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.508331</td>\n",
       "      <td>1.281318</td>\n",
       "      <td>-0.294950</td>\n",
       "      <td>-1.187598</td>\n",
       "      <td>2.145149</td>\n",
       "      <td>0.894924</td>\n",
       "      <td>-0.129821</td>\n",
       "      <td>vocalic</td>\n",
       "      <td>-0.066922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.953329</td>\n",
       "      <td>slim</td>\n",
       "      <td>1.624678</td>\n",
       "      <td>Gullah</td>\n",
       "      <td>0.122670</td>\n",
       "      <td>-2.362932</td>\n",
       "      <td>-0.645964</td>\n",
       "      <td>-0.182896</td>\n",
       "      <td>0.619154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323079</td>\n",
       "      <td>1.178696</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>-0.482744</td>\n",
       "      <td>-0.799192</td>\n",
       "      <td>2.057495</td>\n",
       "      <td>-0.252354</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>-1.310899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.334501</td>\n",
       "      <td>lee</td>\n",
       "      <td>-0.653329</td>\n",
       "      <td>Betsey</td>\n",
       "      <td>-0.474945</td>\n",
       "      <td>2.143944</td>\n",
       "      <td>0.504987</td>\n",
       "      <td>-0.792521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.633919</td>\n",
       "      <td>...</td>\n",
       "      <td>1.765454</td>\n",
       "      <td>-0.071335</td>\n",
       "      <td>0.087142</td>\n",
       "      <td>-0.661786</td>\n",
       "      <td>-1.200296</td>\n",
       "      <td>0.865755</td>\n",
       "      <td>0.186454</td>\n",
       "      <td>0.404982</td>\n",
       "      <td>tribesman</td>\n",
       "      <td>0.029102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.380101</td>\n",
       "      <td>wrap</td>\n",
       "      <td>-0.055548</td>\n",
       "      <td>precocious</td>\n",
       "      <td>-1.703382</td>\n",
       "      <td>-0.964923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.269407</td>\n",
       "      <td>1.058424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384065</td>\n",
       "      <td>-1.402101</td>\n",
       "      <td>-0.993251</td>\n",
       "      <td>-1.183259</td>\n",
       "      <td>1.628616</td>\n",
       "      <td>0.074095</td>\n",
       "      <td>-1.758739</td>\n",
       "      <td>-0.032695</td>\n",
       "      <td>swore</td>\n",
       "      <td>1.696070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.184551</td>\n",
       "      <td>tangential</td>\n",
       "      <td>2.006093</td>\n",
       "      <td>con</td>\n",
       "      <td>1.542110</td>\n",
       "      <td>1.726964</td>\n",
       "      <td>-0.551858</td>\n",
       "      <td>-1.525656</td>\n",
       "      <td>0.224685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.061504</td>\n",
       "      <td>1.031102</td>\n",
       "      <td>0.175930</td>\n",
       "      <td>-1.418366</td>\n",
       "      <td>-0.564248</td>\n",
       "      <td>2.558199</td>\n",
       "      <td>0.932591</td>\n",
       "      <td>1.208366</td>\n",
       "      <td>befriend</td>\n",
       "      <td>-1.016951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.146345</td>\n",
       "      <td>trickle</td>\n",
       "      <td>-0.754276</td>\n",
       "      <td>livery</td>\n",
       "      <td>0.302635</td>\n",
       "      <td>0.150419</td>\n",
       "      <td>0.638730</td>\n",
       "      <td>1.105526</td>\n",
       "      <td>2.403416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064138</td>\n",
       "      <td>-0.942751</td>\n",
       "      <td>1.092675</td>\n",
       "      <td>0.201099</td>\n",
       "      <td>1.633432</td>\n",
       "      <td>-1.143005</td>\n",
       "      <td>-0.057619</td>\n",
       "      <td>0.328762</td>\n",
       "      <td>thunderclap</td>\n",
       "      <td>0.409106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1.422254</td>\n",
       "      <td>pegboard</td>\n",
       "      <td>-1.081548</td>\n",
       "      <td>sensitive</td>\n",
       "      <td>-0.646573</td>\n",
       "      <td>1.644968</td>\n",
       "      <td>-1.606446</td>\n",
       "      <td>-0.127918</td>\n",
       "      <td>0.576557</td>\n",
       "      <td>-0.249036</td>\n",
       "      <td>...</td>\n",
       "      <td>1.687142</td>\n",
       "      <td>-1.434839</td>\n",
       "      <td>-0.756006</td>\n",
       "      <td>3.078881</td>\n",
       "      <td>-0.756351</td>\n",
       "      <td>0.203464</td>\n",
       "      <td>0.311250</td>\n",
       "      <td>0.881640</td>\n",
       "      <td>Edith</td>\n",
       "      <td>1.627489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.576904</td>\n",
       "      <td>mite</td>\n",
       "      <td>0.491919</td>\n",
       "      <td>rheology</td>\n",
       "      <td>-0.898415</td>\n",
       "      <td>2.189803</td>\n",
       "      <td>0.341756</td>\n",
       "      <td>-0.759133</td>\n",
       "      <td>-0.839722</td>\n",
       "      <td>-0.808298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.320233</td>\n",
       "      <td>-0.385668</td>\n",
       "      <td>0.109407</td>\n",
       "      <td>-2.123896</td>\n",
       "      <td>0.950424</td>\n",
       "      <td>1.876171</td>\n",
       "      <td>-0.599393</td>\n",
       "      <td>1.831459</td>\n",
       "      <td>apologia</td>\n",
       "      <td>0.307594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.849602</td>\n",
       "      <td>shepherdess</td>\n",
       "      <td>-0.692910</td>\n",
       "      <td>experimentation</td>\n",
       "      <td>0.357015</td>\n",
       "      <td>1.586017</td>\n",
       "      <td>-0.208122</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>2.133033</td>\n",
       "      <td>-1.237815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>-1.006350</td>\n",
       "      <td>-0.639226</td>\n",
       "      <td>-0.151785</td>\n",
       "      <td>-0.589365</td>\n",
       "      <td>-0.493001</td>\n",
       "      <td>-1.952088</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>Waldorf</td>\n",
       "      <td>1.186741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.125489</td>\n",
       "      <td>jackdaw</td>\n",
       "      <td>0.129221</td>\n",
       "      <td>interruptible</td>\n",
       "      <td>2.445752</td>\n",
       "      <td>0.869606</td>\n",
       "      <td>0.654366</td>\n",
       "      <td>-1.778720</td>\n",
       "      <td>0.413435</td>\n",
       "      <td>1.355638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109395</td>\n",
       "      <td>-1.548510</td>\n",
       "      <td>-0.734265</td>\n",
       "      <td>-0.773789</td>\n",
       "      <td>0.279969</td>\n",
       "      <td>-0.055585</td>\n",
       "      <td>1.876796</td>\n",
       "      <td>0.725767</td>\n",
       "      <td>chapati</td>\n",
       "      <td>1.722513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.187679</td>\n",
       "      <td>Exxon</td>\n",
       "      <td>0.201160</td>\n",
       "      <td>shamrock</td>\n",
       "      <td>-0.464617</td>\n",
       "      <td>1.615376</td>\n",
       "      <td>-0.903702</td>\n",
       "      <td>0.403730</td>\n",
       "      <td>1.217159</td>\n",
       "      <td>-0.322320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283288</td>\n",
       "      <td>0.964233</td>\n",
       "      <td>0.193754</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>-1.179040</td>\n",
       "      <td>0.324359</td>\n",
       "      <td>1.521316</td>\n",
       "      <td>-0.258905</td>\n",
       "      <td>commissary</td>\n",
       "      <td>-0.963142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.490726</td>\n",
       "      <td>jewelry</td>\n",
       "      <td>-0.337086</td>\n",
       "      <td>soliton</td>\n",
       "      <td>0.148667</td>\n",
       "      <td>-0.070499</td>\n",
       "      <td>1.530751</td>\n",
       "      <td>-1.209695</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>0.486502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613403</td>\n",
       "      <td>-0.331376</td>\n",
       "      <td>1.306542</td>\n",
       "      <td>-0.939335</td>\n",
       "      <td>-0.213443</td>\n",
       "      <td>1.218762</td>\n",
       "      <td>-1.975467</td>\n",
       "      <td>-0.302470</td>\n",
       "      <td>pelagic</td>\n",
       "      <td>-0.239385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.408075</td>\n",
       "      <td>Smucker</td>\n",
       "      <td>-1.008086</td>\n",
       "      <td>ember</td>\n",
       "      <td>-2.038125</td>\n",
       "      <td>0.337603</td>\n",
       "      <td>0.871125</td>\n",
       "      <td>0.289775</td>\n",
       "      <td>-0.487606</td>\n",
       "      <td>-0.411877</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.870792</td>\n",
       "      <td>-1.177626</td>\n",
       "      <td>-0.647604</td>\n",
       "      <td>0.394452</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>-0.326024</td>\n",
       "      <td>-0.432558</td>\n",
       "      <td>-0.351513</td>\n",
       "      <td>Mozart</td>\n",
       "      <td>1.347008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.477530</td>\n",
       "      <td>Pavlov</td>\n",
       "      <td>-0.193659</td>\n",
       "      <td>routine</td>\n",
       "      <td>-1.141689</td>\n",
       "      <td>1.024063</td>\n",
       "      <td>0.696387</td>\n",
       "      <td>-0.170185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.592527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.716822</td>\n",
       "      <td>1.173294</td>\n",
       "      <td>0.282479</td>\n",
       "      <td>-0.818199</td>\n",
       "      <td>0.088407</td>\n",
       "      <td>0.955305</td>\n",
       "      <td>-0.551186</td>\n",
       "      <td>-1.866537</td>\n",
       "      <td>they</td>\n",
       "      <td>-1.191372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.425458</td>\n",
       "      <td>deport</td>\n",
       "      <td>-0.047711</td>\n",
       "      <td>seem</td>\n",
       "      <td>-0.966976</td>\n",
       "      <td>0.128104</td>\n",
       "      <td>-1.583903</td>\n",
       "      <td>-0.452306</td>\n",
       "      <td>0.840644</td>\n",
       "      <td>-0.681052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>0.254157</td>\n",
       "      <td>-1.463612</td>\n",
       "      <td>-0.446183</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.760415</td>\n",
       "      <td>-0.652624</td>\n",
       "      <td>-1.158365</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>0.375316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.977555</td>\n",
       "      <td>SW</td>\n",
       "      <td>0.751387</td>\n",
       "      <td>value</td>\n",
       "      <td>0.099332</td>\n",
       "      <td>-0.592394</td>\n",
       "      <td>-0.576771</td>\n",
       "      <td>-0.238948</td>\n",
       "      <td>0.048522</td>\n",
       "      <td>-0.863991</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.669405</td>\n",
       "      <td>-1.693028</td>\n",
       "      <td>-0.838070</td>\n",
       "      <td>0.270457</td>\n",
       "      <td>0.500917</td>\n",
       "      <td>0.755391</td>\n",
       "      <td>-0.830950</td>\n",
       "      <td>0.543360</td>\n",
       "      <td>neuralgia</td>\n",
       "      <td>1.897924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.243339</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>0.352055</td>\n",
       "      <td>otherworld</td>\n",
       "      <td>-0.241236</td>\n",
       "      <td>0.243801</td>\n",
       "      <td>-1.525525</td>\n",
       "      <td>1.846637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.564079</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.560704</td>\n",
       "      <td>-0.722168</td>\n",
       "      <td>0.650201</td>\n",
       "      <td>-0.045586</td>\n",
       "      <td>-0.691908</td>\n",
       "      <td>0.872457</td>\n",
       "      <td>1.443765</td>\n",
       "      <td>frambesia</td>\n",
       "      <td>-1.128686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.909417</td>\n",
       "      <td>scion</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>invasion</td>\n",
       "      <td>-1.398568</td>\n",
       "      <td>-0.007973</td>\n",
       "      <td>-0.875618</td>\n",
       "      <td>-1.037246</td>\n",
       "      <td>0.077368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650643</td>\n",
       "      <td>-1.098317</td>\n",
       "      <td>-0.397696</td>\n",
       "      <td>1.523124</td>\n",
       "      <td>0.926178</td>\n",
       "      <td>-1.382800</td>\n",
       "      <td>-0.861284</td>\n",
       "      <td>-0.487125</td>\n",
       "      <td>moneymake</td>\n",
       "      <td>1.170598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.272724</td>\n",
       "      <td>abrogate</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>beryllium</td>\n",
       "      <td>-2.696887</td>\n",
       "      <td>-1.012104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.335785</td>\n",
       "      <td>0.823171</td>\n",
       "      <td>-1.654857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>1.658822</td>\n",
       "      <td>-0.749202</td>\n",
       "      <td>-1.289961</td>\n",
       "      <td>-0.245743</td>\n",
       "      <td>-1.503143</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>0.696206</td>\n",
       "      <td>vernier</td>\n",
       "      <td>-1.207273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.059525</td>\n",
       "      <td>addition</td>\n",
       "      <td>-1.024388</td>\n",
       "      <td>embroider</td>\n",
       "      <td>-3.241267</td>\n",
       "      <td>-1.260884</td>\n",
       "      <td>0.443819</td>\n",
       "      <td>1.266911</td>\n",
       "      <td>2.122156</td>\n",
       "      <td>0.917862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252568</td>\n",
       "      <td>-1.207572</td>\n",
       "      <td>-0.689336</td>\n",
       "      <td>-1.519370</td>\n",
       "      <td>-0.926930</td>\n",
       "      <td>0.774634</td>\n",
       "      <td>1.032465</td>\n",
       "      <td>-1.247783</td>\n",
       "      <td>controlled</td>\n",
       "      <td>1.391755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-0.603985</td>\n",
       "      <td>stratus</td>\n",
       "      <td>-0.155677</td>\n",
       "      <td>bract</td>\n",
       "      <td>0.086590</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.471645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.576892</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167782</td>\n",
       "      <td>-0.575002</td>\n",
       "      <td>-0.149518</td>\n",
       "      <td>0.529804</td>\n",
       "      <td>0.371146</td>\n",
       "      <td>-0.203045</td>\n",
       "      <td>-1.129707</td>\n",
       "      <td>0.254421</td>\n",
       "      <td>sat</td>\n",
       "      <td>0.588465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.843247</td>\n",
       "      <td>pick</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>timetable</td>\n",
       "      <td>2.170943</td>\n",
       "      <td>0.511203</td>\n",
       "      <td>0.075434</td>\n",
       "      <td>1.639965</td>\n",
       "      <td>-0.137449</td>\n",
       "      <td>1.373659</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.327252</td>\n",
       "      <td>0.405709</td>\n",
       "      <td>1.612278</td>\n",
       "      <td>-0.246062</td>\n",
       "      <td>-1.601966</td>\n",
       "      <td>0.952875</td>\n",
       "      <td>0.551485</td>\n",
       "      <td>conscious</td>\n",
       "      <td>-0.468094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.998354</td>\n",
       "      <td>terpsichorean</td>\n",
       "      <td>-0.213989</td>\n",
       "      <td>parish</td>\n",
       "      <td>0.984322</td>\n",
       "      <td>1.579572</td>\n",
       "      <td>-0.034685</td>\n",
       "      <td>-1.004141</td>\n",
       "      <td>-0.420187</td>\n",
       "      <td>-0.522860</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.778613</td>\n",
       "      <td>-1.137141</td>\n",
       "      <td>-1.344451</td>\n",
       "      <td>1.550500</td>\n",
       "      <td>0.234215</td>\n",
       "      <td>-0.281785</td>\n",
       "      <td>0.674819</td>\n",
       "      <td>atonic</td>\n",
       "      <td>-0.240384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.051946</td>\n",
       "      <td>ground</td>\n",
       "      <td>-0.080717</td>\n",
       "      <td>accession</td>\n",
       "      <td>0.732640</td>\n",
       "      <td>-0.321635</td>\n",
       "      <td>-0.071601</td>\n",
       "      <td>-0.259042</td>\n",
       "      <td>0.381935</td>\n",
       "      <td>2.076748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078635</td>\n",
       "      <td>-0.787365</td>\n",
       "      <td>1.122101</td>\n",
       "      <td>1.030283</td>\n",
       "      <td>0.727630</td>\n",
       "      <td>-0.037222</td>\n",
       "      <td>0.430042</td>\n",
       "      <td>-1.998201</td>\n",
       "      <td>spoken</td>\n",
       "      <td>0.254643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.914031</td>\n",
       "      <td>transfix</td>\n",
       "      <td>0.217433</td>\n",
       "      <td>studio</td>\n",
       "      <td>-0.190682</td>\n",
       "      <td>0.916328</td>\n",
       "      <td>0.612774</td>\n",
       "      <td>1.108183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870068</td>\n",
       "      <td>-2.585909</td>\n",
       "      <td>0.324237</td>\n",
       "      <td>2.088375</td>\n",
       "      <td>-0.623769</td>\n",
       "      <td>-1.053416</td>\n",
       "      <td>-2.896255</td>\n",
       "      <td>0.495682</td>\n",
       "      <td>don</td>\n",
       "      <td>2.232454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.696456</td>\n",
       "      <td>deign</td>\n",
       "      <td>-0.096713</td>\n",
       "      <td>swingable</td>\n",
       "      <td>0.211017</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>-1.280429</td>\n",
       "      <td>-1.556629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.184902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544919</td>\n",
       "      <td>1.082660</td>\n",
       "      <td>-1.033981</td>\n",
       "      <td>-0.704344</td>\n",
       "      <td>-2.081929</td>\n",
       "      <td>1.754794</td>\n",
       "      <td>1.049009</td>\n",
       "      <td>0.399136</td>\n",
       "      <td>sweatpants</td>\n",
       "      <td>-0.561560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.353166</td>\n",
       "      <td>Debby</td>\n",
       "      <td>-0.295401</td>\n",
       "      <td>chimney</td>\n",
       "      <td>0.338484</td>\n",
       "      <td>-0.079641</td>\n",
       "      <td>0.579633</td>\n",
       "      <td>1.187386</td>\n",
       "      <td>-1.062394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168461</td>\n",
       "      <td>2.529834</td>\n",
       "      <td>0.884395</td>\n",
       "      <td>-0.187144</td>\n",
       "      <td>0.194384</td>\n",
       "      <td>0.325796</td>\n",
       "      <td>0.428307</td>\n",
       "      <td>1.317598</td>\n",
       "      <td>inferring</td>\n",
       "      <td>-2.683180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          17           leg_1         7            arm_2         2        11  \\\n",
       "46  2.010205       Baltimore -0.798297              nag -0.176947 -0.552223   \n",
       "35 -1.279577          faucet  0.026091            withe -0.624819  0.195845   \n",
       "48 -1.331233           vocal  0.507991        camelback -1.836205  1.213098   \n",
       "28  0.423599           razor -1.067533        proboscis  2.062525 -0.447322   \n",
       "50 -0.796026       Apocrypha  0.021312            foyer  1.076007 -0.309209   \n",
       "45  0.190500       vexatious -0.435486       scandalous  0.709452  0.672574   \n",
       "78  0.309821          lawful  0.413799            Jaime  0.633777 -1.243863   \n",
       "91 -0.953329            slim  1.624678           Gullah  0.122670 -2.362932   \n",
       "0  -0.334501             lee -0.653329           Betsey -0.474945  2.143944   \n",
       "23 -1.380101            wrap -0.055548       precocious -1.703382 -0.964923   \n",
       "10  0.184551      tangential  2.006093              con  1.542110  1.726964   \n",
       "21 -1.146345         trickle -0.754276           livery  0.302635  0.150419   \n",
       "73 -1.422254        pegboard -1.081548        sensitive -0.646573  1.644968   \n",
       "81 -0.576904            mite  0.491919         rheology -0.898415  2.189803   \n",
       "67  0.849602     shepherdess -0.692910  experimentation  0.357015  1.586017   \n",
       "29 -1.125489         jackdaw  0.129221    interruptible  2.445752  0.869606   \n",
       "74  1.187679           Exxon  0.201160         shamrock -0.464617  1.615376   \n",
       "12  1.490726         jewelry -0.337086          soliton  0.148667 -0.070499   \n",
       "11 -0.408075         Smucker -1.008086            ember -2.038125  0.337603   \n",
       "49  1.477530          Pavlov -0.193659          routine -1.141689  1.024063   \n",
       "14  0.425458          deport -0.047711             seem -0.966976  0.128104   \n",
       "71 -0.977555              SW  0.751387            value  0.099332 -0.592394   \n",
       "84  0.243339  multiplicative  0.352055       otherworld -0.241236  0.243801   \n",
       "27  1.909417           scion  0.562969         invasion -1.398568 -0.007973   \n",
       "62 -0.272724        abrogate -0.054295        beryllium -2.696887 -1.012104   \n",
       "89 -0.059525        addition -1.024388        embroider -3.241267 -1.260884   \n",
       "83 -0.603985         stratus -0.155677            bract  0.086590  1.006293   \n",
       "42 -0.843247            pick -0.175886        timetable  2.170943  0.511203   \n",
       "51 -0.998354   terpsichorean -0.213989           parish  0.984322  1.579572   \n",
       "24  0.051946          ground -0.080717        accession  0.732640 -0.321635   \n",
       "44  1.914031        transfix  0.217433           studio -0.190682  0.916328   \n",
       "9   1.696456           deign -0.096713        swingable  0.211017  0.924027   \n",
       "15 -0.353166           Debby -0.295401          chimney  0.338484 -0.079641   \n",
       "\n",
       "           4        16         0        14  ...        12        10        15  \\\n",
       "46  0.285865 -0.612789  0.202923  0.632932  ... -1.379319  1.312175 -0.886027   \n",
       "35  1.665474  0.256030  0.408253 -0.978373  ...  0.517659  1.505752 -0.678261   \n",
       "48  0.708109  0.133541  2.319330  0.141717  ... -1.103367 -1.876538  0.647019   \n",
       "28 -0.334775 -0.643550  0.067856  1.281016  ...       NaN  1.863789  0.780256   \n",
       "50  0.213197 -0.160133  0.319175 -0.752156  ...       NaN  0.825848  0.211646   \n",
       "45 -0.535328 -2.172670 -0.132634  1.899882  ...  0.513106  1.098853  0.703852   \n",
       "78 -0.835347 -0.673491       NaN -0.692905  ...       NaN -0.508331  1.281318   \n",
       "91 -0.645964 -0.182896  0.619154       NaN  ...  0.323079  1.178696  0.558320   \n",
       "0   0.504987 -0.792521       NaN  0.633919  ...  1.765454 -0.071335  0.087142   \n",
       "23       NaN -0.269407  1.058424       NaN  ...  0.384065 -1.402101 -0.993251   \n",
       "10 -0.551858 -1.525656  0.224685       NaN  ...  2.061504  1.031102  0.175930   \n",
       "21  0.638730  1.105526  2.403416       NaN  ... -0.064138 -0.942751  1.092675   \n",
       "73 -1.606446 -0.127918  0.576557 -0.249036  ...  1.687142 -1.434839 -0.756006   \n",
       "81  0.341756 -0.759133 -0.839722 -0.808298  ... -1.320233 -0.385668  0.109407   \n",
       "67 -0.208122  0.280992  2.133033 -1.237815  ...  0.899600 -1.006350 -0.639226   \n",
       "29  0.654366 -1.778720  0.413435  1.355638  ...  0.109395 -1.548510 -0.734265   \n",
       "74 -0.903702  0.403730  1.217159 -0.322320  ...  0.283288  0.964233  0.193754   \n",
       "12  1.530751 -1.209695  0.064474  0.486502  ... -0.613403 -0.331376  1.306542   \n",
       "11  0.871125  0.289775 -0.487606 -0.411877  ... -1.870792 -1.177626 -0.647604   \n",
       "49  0.696387 -0.170185       NaN  0.592527  ... -0.716822  1.173294  0.282479   \n",
       "14 -1.583903 -0.452306  0.840644 -0.681052  ... -0.003603  0.254157 -1.463612   \n",
       "71 -0.576771 -0.238948  0.048522 -0.863991  ... -1.669405 -1.693028 -0.838070   \n",
       "84 -1.525525  1.846637       NaN -0.564079  ...       NaN  1.560704 -0.722168   \n",
       "27 -0.875618 -1.037246  0.077368       NaN  ... -0.650643 -1.098317 -0.397696   \n",
       "62       NaN -0.335785  0.823171 -1.654857  ... -0.230935  1.658822 -0.749202   \n",
       "89  0.443819  1.266911  2.122156  0.917862  ... -0.252568 -1.207572 -0.689336   \n",
       "83       NaN -2.471645       NaN -0.576892  ...  1.167782 -0.575002 -0.149518   \n",
       "42  0.075434  1.639965 -0.137449  1.373659  ...       NaN  0.327252  0.405709   \n",
       "51 -0.034685 -1.004141 -0.420187 -0.522860  ...       NaN  0.778613 -1.137141   \n",
       "24 -0.071601 -0.259042  0.381935  2.076748  ...  0.078635 -0.787365  1.122101   \n",
       "44  0.612774  1.108183       NaN  0.346488  ...  0.870068 -2.585909  0.324237   \n",
       "9  -1.280429 -1.556629       NaN -0.184902  ... -0.544919  1.082660 -1.033981   \n",
       "15  0.579633  1.187386 -1.062394       NaN  ...  0.168461  2.529834  0.884395   \n",
       "\n",
       "           9        19        18         8         6        arm_0         1  \n",
       "46  1.547505  0.658544  0.334457 -1.515744 -0.730930     recovery -0.833116  \n",
       "35  1.029156 -1.840874  1.014370 -1.702584 -0.725744         doze -1.096621  \n",
       "48  0.192049 -0.785989  0.956702  0.393318 -2.152891         maim  1.449016  \n",
       "28  0.484733 -0.955123 -0.403648  0.852774  1.412221          hey -2.030223  \n",
       "50 -1.875172 -0.319054 -0.751969  1.340450 -0.060661      bistate -0.843897  \n",
       "45  1.107081  0.331980 -0.090533 -0.974529 -0.259547       follow -1.298263  \n",
       "78 -0.294950 -1.187598  2.145149  0.894924 -0.129821      vocalic -0.066922  \n",
       "91  0.020794 -0.482744 -0.799192  2.057495 -0.252354        Aruba -1.310899  \n",
       "0  -0.661786 -1.200296  0.865755  0.186454  0.404982    tribesman  0.029102  \n",
       "23 -1.183259  1.628616  0.074095 -1.758739 -0.032695        swore  1.696070  \n",
       "10 -1.418366 -0.564248  2.558199  0.932591  1.208366     befriend -1.016951  \n",
       "21  0.201099  1.633432 -1.143005 -0.057619  0.328762  thunderclap  0.409106  \n",
       "73  3.078881 -0.756351  0.203464  0.311250  0.881640        Edith  1.627489  \n",
       "81 -2.123896  0.950424  1.876171 -0.599393  1.831459     apologia  0.307594  \n",
       "67 -0.151785 -0.589365 -0.493001 -1.952088  0.307300      Waldorf  1.186741  \n",
       "29 -0.773789  0.279969 -0.055585  1.876796  0.725767      chapati  1.722513  \n",
       "74  0.998311 -1.179040  0.324359  1.521316 -0.258905   commissary -0.963142  \n",
       "12 -0.939335 -0.213443  1.218762 -1.975467 -0.302470      pelagic -0.239385  \n",
       "11  0.394452  1.201214 -0.326024 -0.432558 -0.351513       Mozart  1.347008  \n",
       "49 -0.818199  0.088407  0.955305 -0.551186 -1.866537         they -1.191372  \n",
       "14 -0.446183  0.785800  0.760415 -0.652624 -1.158365      Clayton  0.375316  \n",
       "71  0.270457  0.500917  0.755391 -0.830950  0.543360    neuralgia  1.897924  \n",
       "84  0.650201 -0.045586 -0.691908  0.872457  1.443765    frambesia -1.128686  \n",
       "27  1.523124  0.926178 -1.382800 -0.861284 -0.487125    moneymake  1.170598  \n",
       "62 -1.289961 -0.245743 -1.503143  0.073318  0.696206      vernier -1.207273  \n",
       "89 -1.519370 -0.926930  0.774634  1.032465 -1.247783   controlled  1.391755  \n",
       "83  0.529804  0.371146 -0.203045 -1.129707  0.254421          sat  0.588465  \n",
       "42  1.612278 -0.246062 -1.601966  0.952875  0.551485    conscious -0.468094  \n",
       "51 -1.344451  1.550500  0.234215 -0.281785  0.674819       atonic -0.240384  \n",
       "24  1.030283  0.727630 -0.037222  0.430042 -1.998201       spoken  0.254643  \n",
       "44  2.088375 -0.623769 -1.053416 -2.896255  0.495682          don  2.232454  \n",
       "9  -0.704344 -2.081929  1.754794  1.049009  0.399136   sweatpants -0.561560  \n",
       "15 -0.187144  0.194384  0.325796  0.428307  1.317598    inferring -2.683180  \n",
       "\n",
       "[33 rows x 24 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some data analysis here\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-083359799dd294d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "While crunching your data, you probably found two issues:\n",
    "\n",
    "1. There are 4 columns whose name starts with either `arm` or `leg` which are all filled with gibberish\n",
    "2. There are some values missing in some columns\n",
    "\n",
    "So, first things first, let's get rid of those columns through a Custom Transformer, so we can plug it in a Scikit Pipeline after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57e648bb4ed49423",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9663ebd68ad68f8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline step called RemoveLimbs that removes any\n",
    "# column whose name starts with the string 'arm' or 'leg'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "class RemoveLimbs(TransformerMixin):\n",
    "    \n",
    "    def fit_transform(self, X, *_):\n",
    "        return X.select_dtypes(exclude='object').copy()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5c8634dcc19dd7e1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(sorted(RemoveLimbs().fit_transform(X).columns), 'salt5') == '71443dfc3077d773d4c74e958dadf91dc2cc148a'\n",
    "assert _hash(list(map(lambda col: col.startswith('arm') or col.startswith('leg'), RemoveLimbs().fit_transform(X_train).columns)), 'salt6') == 'ce45cf3759d2210f2d1315f1673b18f34e3ac711'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0ded8876cfda9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/monty_python_black_knight.gif\" width=500>\n",
    "\n",
    "Now that we have our Custom Transformer in place, we can design our pipeline. For the sake of the exercise, you'll want to create a pipeline with the following steps:\n",
    "\n",
    "1. Removes limbs columns\n",
    "2. Imputes missing values with the mean\n",
    "3. Has a Random Forest Classifier as the last step\n",
    "\n",
    "You may use `make_pipeline` to create your pipeline with as many steps as you want as long as the first two are the Custom Transformer you developed previously, a `SimpleImputer` as the second step, and a `RandomForestClassifier` as the last step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b18993e1e6a77d8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Scikit Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efabb79e960ce10b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6814224bb0f1332f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert _hash(pipeline.steps[0][0], 'salt7') == '471b02068ac2c4f479c2e9f85f4b3dc2179bb841'\n",
    "assert _hash(pipeline.steps[1][0], 'salt8') == 'ca83eaea1a7e243fa5574cfa6f52831166ee0f32'\n",
    "assert _hash(pipeline.steps[-1][0], 'salt9') == '0d66ba4309ad4939673169e74f87088dcadd510b'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53269c4aaa5f3df3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it work? Let's check it out on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd7deb2b78d36444",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4defdb6d3d670ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e887f18b73a6081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That's it for this Exercise Notebook! It doesn't get much cleaner than this, does it?\n",
    "\n",
    "You can still practice around with pipelines, maybe add a few more steps. See how you can adapt your pipeline and how it affects the predictions.\n",
    "\n",
    "Can you see how Scikit-learn's Pipelines might save time? Can you imagine how useful that would be in stressful situations (like, *for example*, an Hackathon)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
