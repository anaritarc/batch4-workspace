{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12bea12324c032d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ritacarvalho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f07b8631beb0508c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q1. S&P 500 Companies\n",
    "\n",
    "For the first question you will be making use of regex. In particular you have a list of companies currently in the S&P 500, their [stock tickers](https://en.wikipedia.org/wiki/Ticker_symbol) (an abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market) , and their industries, and you'll have to answer some very specific questions about that list.\n",
    "\n",
    "Start by loading the data into a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe61fa6cbbdef77d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/SP500.txt\"\n",
    "companies = []\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    companies = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3M Company (MMM) -- Industrials',\n",
       " 'Abbott Laboratories (ABT) -- Health Care',\n",
       " 'AbbVie Inc. (ABBV) -- Health Care',\n",
       " 'ABIOMED Inc (ABMD) -- Health Care',\n",
       " 'Accenture plc (ACN) -- Information Technology',\n",
       " 'Activision Blizzard (ATVI) -- Communication Services',\n",
       " 'Adobe Inc. (ADBE) -- Information Technology',\n",
       " 'Advanced Micro Devices Inc (AMD) -- Information Technology',\n",
       " 'Advance Auto Parts (AAP) -- Consumer Discretionary',\n",
       " 'AES Corp (AES) -- Utilities',\n",
       " 'AFLAC Inc (AFL) -- Financials',\n",
       " 'Agilent Technologies Inc (A) -- Health Care',\n",
       " 'Air Products & Chemicals Inc (APD) -- Materials',\n",
       " 'Akamai Technologies Inc (AKAM) -- Information Technology',\n",
       " 'Alaska Air Group Inc (ALK) -- Industrials',\n",
       " 'Albemarle Corp (ALB) -- Materials',\n",
       " 'Alexandria Real Estate Equities (ARE) -- Real Estate',\n",
       " 'Alexion Pharmaceuticals (ALXN) -- Health Care',\n",
       " 'Align Technology (ALGN) -- Health Care',\n",
       " 'Allegion (ALLE) -- Industrials',\n",
       " 'Alliant Energy Corp (LNT) -- Utilities',\n",
       " 'Allstate Corp (ALL) -- Financials',\n",
       " 'Alphabet Inc. (Class A) (GOOGL) -- Communication Services',\n",
       " 'Alphabet Inc. (Class C) (GOOG) -- Communication Services',\n",
       " 'Altria Group Inc (MO) -- Consumer Staples',\n",
       " 'Amazon.com Inc. (AMZN) -- Consumer Discretionary',\n",
       " 'Amcor plc (AMCR) -- Materials',\n",
       " 'Ameren Corp (AEE) -- Utilities',\n",
       " 'American Airlines Group (AAL) -- Industrials',\n",
       " 'American Electric Power (AEP) -- Utilities',\n",
       " 'American Express Co (AXP) -- Financials',\n",
       " 'American International Group (AIG) -- Financials',\n",
       " 'American Tower Corp. (AMT) -- Real Estate',\n",
       " 'American Water Works Company Inc (AWK) -- Utilities',\n",
       " 'Ameriprise Financial (AMP) -- Financials',\n",
       " 'AmerisourceBergen Corp (ABC) -- Health Care',\n",
       " 'AMETEK Inc. (AME) -- Industrials',\n",
       " 'Amgen Inc. (AMGN) -- Health Care',\n",
       " 'Amphenol Corp (APH) -- Information Technology',\n",
       " 'Analog Devices, Inc. (ADI) -- Information Technology',\n",
       " 'ANSYS (ANSS) -- Information Technology',\n",
       " 'Anthem (ANTM) -- Health Care',\n",
       " 'Aon plc (AON) -- Financials',\n",
       " 'A.O. Smith Corp (AOS) -- Industrials',\n",
       " 'Apache Corporation (APA) -- Energy',\n",
       " 'Apartment Investment & Management (AIV) -- Real Estate',\n",
       " 'Apple Inc. (AAPL) -- Information Technology',\n",
       " 'Applied Materials Inc. (AMAT) -- Information Technology',\n",
       " 'Aptiv PLC (APTV) -- Consumer Discretionary',\n",
       " 'Archer-Daniels-Midland Co (ADM) -- Consumer Staples',\n",
       " 'Arista Networks (ANET) -- Information Technology',\n",
       " 'Arthur J. Gallagher & Co. (AJG) -- Financials',\n",
       " 'Assurant (AIZ) -- Financials',\n",
       " 'AT&T Inc. (T) -- Communication Services',\n",
       " 'Atmos Energy (ATO) -- Utilities',\n",
       " 'Autodesk Inc. (ADSK) -- Information Technology',\n",
       " 'Automatic Data Processing (ADP) -- Information Technology',\n",
       " 'AutoZone Inc (AZO) -- Consumer Discretionary',\n",
       " 'AvalonBay Communities (AVB) -- Real Estate',\n",
       " 'Avery Dennison Corp (AVY) -- Materials',\n",
       " 'Baker Hughes Co (BKR) -- Energy',\n",
       " 'Ball Corp (BLL) -- Materials',\n",
       " 'Bank of America Corp (BAC) -- Financials',\n",
       " 'The Bank of New York Mellon (BK) -- Financials',\n",
       " 'Baxter International Inc. (BAX) -- Health Care',\n",
       " 'Becton Dickinson (BDX) -- Health Care',\n",
       " 'Berkshire Hathaway (BRK.B) -- Financials',\n",
       " 'Best Buy Co. Inc. (BBY) -- Consumer Discretionary',\n",
       " 'Bio-Rad Laboratories (BIO) -- Health Care',\n",
       " 'Biogen Inc. (BIIB) -- Health Care',\n",
       " 'BlackRock (BLK) -- Financials',\n",
       " 'Boeing Company (BA) -- Industrials',\n",
       " 'Booking Holdings Inc (BKNG) -- Consumer Discretionary',\n",
       " 'BorgWarner (BWA) -- Consumer Discretionary',\n",
       " 'Boston Properties (BXP) -- Real Estate',\n",
       " 'Boston Scientific (BSX) -- Health Care',\n",
       " 'Bristol-Myers Squibb (BMY) -- Health Care',\n",
       " 'Broadcom Inc. (AVGO) -- Information Technology',\n",
       " 'Broadridge Financial Solutions (BR) -- Information Technology',\n",
       " 'Brown-Forman Corp. (BF.B) -- Consumer Staples',\n",
       " 'C. H. Robinson Worldwide (CHRW) -- Industrials',\n",
       " 'Cabot Oil & Gas (COG) -- Energy',\n",
       " 'Cadence Design Systems (CDNS) -- Information Technology',\n",
       " 'Campbell Soup (CPB) -- Consumer Staples',\n",
       " 'Capital One Financial (COF) -- Financials',\n",
       " 'Cardinal Health Inc. (CAH) -- Health Care',\n",
       " 'Carmax Inc (KMX) -- Consumer Discretionary',\n",
       " 'Carnival Corp. (CCL) -- Consumer Discretionary',\n",
       " 'Carrier Global (CARR) -- Industrials',\n",
       " 'Caterpillar Inc. (CAT) -- Industrials',\n",
       " 'Cboe Global Markets (CBOE) -- Financials',\n",
       " 'CBRE Group (CBRE) -- Real Estate',\n",
       " 'CDW (CDW) -- Information Technology',\n",
       " 'Celanese (CE) -- Materials',\n",
       " 'Centene Corporation (CNC) -- Health Care',\n",
       " 'CenterPoint Energy (CNP) -- Utilities',\n",
       " 'CenturyLink Inc (CTL) -- Communication Services',\n",
       " 'Cerner (CERN) -- Health Care',\n",
       " 'CF Industries Holdings Inc (CF) -- Materials',\n",
       " 'Charles Schwab Corporation (SCHW) -- Financials',\n",
       " 'Charter Communications (CHTR) -- Communication Services',\n",
       " 'Chevron Corp. (CVX) -- Energy',\n",
       " 'Chipotle Mexican Grill (CMG) -- Consumer Discretionary',\n",
       " 'Chubb Limited (CB) -- Financials',\n",
       " 'Church & Dwight (CHD) -- Consumer Staples',\n",
       " 'CIGNA Corp. (CI) -- Health Care',\n",
       " 'Cincinnati Financial (CINF) -- Financials',\n",
       " 'Cintas Corporation (CTAS) -- Industrials',\n",
       " 'Cisco Systems (CSCO) -- Information Technology',\n",
       " 'Citigroup Inc. (C) -- Financials',\n",
       " 'Citizens Financial Group (CFG) -- Financials',\n",
       " 'Citrix Systems (CTXS) -- Information Technology',\n",
       " 'The Clorox Company (CLX) -- Consumer Staples',\n",
       " 'CME Group Inc. (CME) -- Financials',\n",
       " 'CMS Energy (CMS) -- Utilities',\n",
       " 'Coca-Cola Company (KO) -- Consumer Staples',\n",
       " 'Cognizant Technology Solutions (CTSH) -- Information Technology',\n",
       " 'Colgate-Palmolive (CL) -- Consumer Staples',\n",
       " 'Comcast Corp. (CMCSA) -- Communication Services',\n",
       " 'Comerica Inc. (CMA) -- Financials',\n",
       " 'Conagra Brands (CAG) -- Consumer Staples',\n",
       " 'Concho Resources (CXO) -- Energy',\n",
       " 'ConocoPhillips (COP) -- Energy',\n",
       " 'Consolidated Edison (ED) -- Utilities',\n",
       " 'Constellation Brands (STZ) -- Consumer Staples',\n",
       " 'The Cooper Companies (COO) -- Health Care',\n",
       " 'Copart Inc (CPRT) -- Industrials',\n",
       " 'Corning Inc. (GLW) -- Information Technology',\n",
       " 'Corteva (CTVA) -- Materials',\n",
       " 'Costco Wholesale Corp. (COST) -- Consumer Staples',\n",
       " 'Coty, Inc (COTY) -- Consumer Staples',\n",
       " 'Crown Castle International Corp. (CCI) -- Real Estate',\n",
       " 'CSX Corp. (CSX) -- Industrials',\n",
       " 'Cummins Inc. (CMI) -- Industrials',\n",
       " 'CVS Health (CVS) -- Health Care',\n",
       " 'D. R. Horton (DHI) -- Consumer Discretionary',\n",
       " 'Danaher Corp. (DHR) -- Health Care',\n",
       " 'Darden Restaurants (DRI) -- Consumer Discretionary',\n",
       " 'DaVita Inc. (DVA) -- Health Care',\n",
       " 'Deere & Co. (DE) -- Industrials',\n",
       " 'Delta Air Lines Inc. (DAL) -- Industrials',\n",
       " 'Dentsply Sirona (XRAY) -- Health Care',\n",
       " 'Devon Energy (DVN) -- Energy',\n",
       " 'DexCom (DXCM) -- Health Care',\n",
       " 'Diamondback Energy (FANG) -- Energy',\n",
       " 'Digital Realty Trust Inc (DLR) -- Real Estate',\n",
       " 'Discover Financial Services (DFS) -- Financials',\n",
       " 'Discovery, Inc. (Class A) (DISCA) -- Communication Services',\n",
       " 'Discovery, Inc. (Class C) (DISCK) -- Communication Services',\n",
       " 'Dish Network (DISH) -- Communication Services',\n",
       " 'Dollar General (DG) -- Consumer Discretionary',\n",
       " 'Dollar Tree (DLTR) -- Consumer Discretionary',\n",
       " 'Dominion Energy (D) -- Utilities',\n",
       " \"Domino's Pizza (DPZ) -- Consumer Discretionary\",\n",
       " 'Dover Corporation (DOV) -- Industrials',\n",
       " 'Dow Inc. (DOW) -- Materials',\n",
       " 'DTE Energy Co. (DTE) -- Utilities',\n",
       " 'Duke Energy (DUK) -- Utilities',\n",
       " 'Duke Realty Corp (DRE) -- Real Estate',\n",
       " 'DuPont de Nemours Inc (DD) -- Materials',\n",
       " 'DXC Technology (DXC) -- Information Technology',\n",
       " 'E*Trade (ETFC) -- Financials',\n",
       " 'Eastman Chemical (EMN) -- Materials',\n",
       " 'Eaton Corporation (ETN) -- Industrials',\n",
       " 'eBay Inc. (EBAY) -- Consumer Discretionary',\n",
       " 'Ecolab Inc. (ECL) -- Materials',\n",
       " \"Edison Int'l (EIX) -- Utilities\",\n",
       " 'Edwards Lifesciences (EW) -- Health Care',\n",
       " 'Electronic Arts (EA) -- Communication Services',\n",
       " 'Emerson Electric Company (EMR) -- Industrials',\n",
       " 'Entergy Corp. (ETR) -- Utilities',\n",
       " 'EOG Resources (EOG) -- Energy',\n",
       " 'Equifax Inc. (EFX) -- Industrials',\n",
       " 'Equinix (EQIX) -- Real Estate',\n",
       " 'Equity Residential (EQR) -- Real Estate',\n",
       " 'Essex Property Trust, Inc. (ESS) -- Real Estate',\n",
       " 'Estée Lauder Companies (EL) -- Consumer Staples',\n",
       " 'Evergy (EVRG) -- Utilities',\n",
       " 'Eversource Energy (ES) -- Utilities',\n",
       " 'Everest Re Group Ltd. (RE) -- Financials',\n",
       " 'Exelon Corp. (EXC) -- Utilities',\n",
       " 'Expedia Group (EXPE) -- Consumer Discretionary',\n",
       " 'Expeditors (EXPD) -- Industrials',\n",
       " 'Extra Space Storage (EXR) -- Real Estate',\n",
       " 'Exxon Mobil Corp. (XOM) -- Energy',\n",
       " 'F5 Networks (FFIV) -- Information Technology',\n",
       " 'Facebook, Inc. (FB) -- Communication Services',\n",
       " 'Fastenal Co (FAST) -- Industrials',\n",
       " 'Federal Realty Investment Trust (FRT) -- Real Estate',\n",
       " 'FedEx Corporation (FDX) -- Industrials',\n",
       " 'Fidelity National Information Services (FIS) -- Information Technology',\n",
       " 'Fifth Third Bancorp (FITB) -- Financials',\n",
       " 'FirstEnergy Corp (FE) -- Utilities',\n",
       " 'First Republic Bank (FRC) -- Financials',\n",
       " 'Fiserv Inc (FISV) -- Information Technology',\n",
       " 'FleetCor Technologies Inc (FLT) -- Information Technology',\n",
       " 'FLIR Systems (FLIR) -- Information Technology',\n",
       " 'Flowserve Corporation (FLS) -- Industrials',\n",
       " 'FMC Corporation (FMC) -- Materials',\n",
       " 'Ford Motor Company (F) -- Consumer Discretionary',\n",
       " 'Fortinet (FTNT) -- Information Technology',\n",
       " 'Fortive Corp (FTV) -- Industrials',\n",
       " 'Fortune Brands Home & Security (FBHS) -- Industrials',\n",
       " 'Fox Corporation (Class A) (FOXA) -- Communication Services',\n",
       " 'Fox Corporation (Class B) (FOX) -- Communication Services',\n",
       " 'Franklin Resources (BEN) -- Financials',\n",
       " 'Freeport-McMoRan Inc. (FCX) -- Materials',\n",
       " 'Gap Inc. (GPS) -- Consumer Discretionary',\n",
       " 'Garmin Ltd. (GRMN) -- Consumer Discretionary',\n",
       " 'Gartner Inc (IT) -- Information Technology',\n",
       " 'General Dynamics (GD) -- Industrials',\n",
       " 'General Electric (GE) -- Industrials',\n",
       " 'General Mills (GIS) -- Consumer Staples',\n",
       " 'General Motors (GM) -- Consumer Discretionary',\n",
       " 'Genuine Parts (GPC) -- Consumer Discretionary',\n",
       " 'Gilead Sciences (GILD) -- Health Care',\n",
       " 'Globe Life Inc. (GL) -- Financials',\n",
       " 'Global Payments Inc. (GPN) -- Information Technology',\n",
       " 'Goldman Sachs Group (GS) -- Financials',\n",
       " 'Grainger (W.W.) Inc. (GWW) -- Industrials',\n",
       " 'H&R Block (HRB) -- Consumer Discretionary',\n",
       " 'Halliburton Co. (HAL) -- Energy',\n",
       " 'Hanesbrands Inc (HBI) -- Consumer Discretionary',\n",
       " 'Hartford Financial Svc.Gp. (HIG) -- Financials',\n",
       " 'Hasbro Inc. (HAS) -- Consumer Discretionary',\n",
       " 'HCA Healthcare (HCA) -- Health Care',\n",
       " 'Healthpeak Properties (PEAK) -- Real Estate',\n",
       " 'Henry Schein (HSIC) -- Health Care',\n",
       " 'The Hershey Company (HSY) -- Consumer Staples',\n",
       " 'Hess Corporation (HES) -- Energy',\n",
       " 'Hewlett Packard Enterprise (HPE) -- Information Technology',\n",
       " 'Hilton Worldwide Holdings Inc (HLT) -- Consumer Discretionary',\n",
       " 'HollyFrontier Corp (HFC) -- Energy',\n",
       " 'Hologic (HOLX) -- Health Care',\n",
       " 'Home Depot (HD) -- Consumer Discretionary',\n",
       " \"Honeywell Int'l Inc. (HON) -- Industrials\",\n",
       " 'Hormel Foods Corp. (HRL) -- Consumer Staples',\n",
       " 'Host Hotels & Resorts (HST) -- Real Estate',\n",
       " 'Howmet Aerospace (HWM) -- Industrials',\n",
       " 'HP Inc. (HPQ) -- Information Technology',\n",
       " 'Humana Inc. (HUM) -- Health Care',\n",
       " 'Huntington Bancshares (HBAN) -- Financials',\n",
       " 'Huntington Ingalls Industries (HII) -- Industrials',\n",
       " 'IDEX Corporation (IEX) -- Industrials',\n",
       " 'IDEXX Laboratories (IDXX) -- Health Care',\n",
       " 'IHS Markit Ltd. (INFO) -- Industrials',\n",
       " 'Illinois Tool Works (ITW) -- Industrials',\n",
       " 'Illumina Inc (ILMN) -- Health Care',\n",
       " 'Incyte (INCY) -- Health Care',\n",
       " 'Ingersoll Rand (IR) -- Industrials',\n",
       " 'Intel Corp. (INTC) -- Information Technology',\n",
       " 'Intercontinental Exchange (ICE) -- Financials',\n",
       " 'International Business Machines (IBM) -- Information Technology',\n",
       " 'International Paper (IP) -- Materials',\n",
       " 'Interpublic Group (IPG) -- Communication Services',\n",
       " 'International Flavors & Fragrances (IFF) -- Materials',\n",
       " 'Intuit Inc. (INTU) -- Information Technology',\n",
       " 'Intuitive Surgical Inc. (ISRG) -- Health Care',\n",
       " 'Invesco Ltd. (IVZ) -- Financials',\n",
       " 'IPG Photonics Corp. (IPGP) -- Information Technology',\n",
       " 'IQVIA Holdings Inc. (IQV) -- Health Care',\n",
       " 'Iron Mountain Incorporated (IRM) -- Real Estate',\n",
       " 'Jack Henry & Associates (JKHY) -- Information Technology',\n",
       " 'Jacobs Engineering Group (J) -- Industrials',\n",
       " 'J. B. Hunt Transport Services (JBHT) -- Industrials',\n",
       " 'JM Smucker (SJM) -- Consumer Staples',\n",
       " 'Johnson & Johnson (JNJ) -- Health Care',\n",
       " 'Johnson Controls International (JCI) -- Industrials',\n",
       " 'JPMorgan Chase & Co. (JPM) -- Financials',\n",
       " 'Juniper Networks (JNPR) -- Information Technology',\n",
       " 'Kansas City Southern (KSU) -- Industrials',\n",
       " 'Kellogg Co. (K) -- Consumer Staples',\n",
       " 'KeyCorp (KEY) -- Financials',\n",
       " 'Keysight Technologies (KEYS) -- Information Technology',\n",
       " 'Kimberly-Clark (KMB) -- Consumer Staples',\n",
       " 'Kimco Realty (KIM) -- Real Estate',\n",
       " 'Kinder Morgan (KMI) -- Energy',\n",
       " 'KLA Corporation (KLAC) -- Information Technology',\n",
       " \"Kohl's Corp. (KSS) -- Consumer Discretionary\",\n",
       " 'Kraft Heinz Co (KHC) -- Consumer Staples',\n",
       " 'Kroger Co. (KR) -- Consumer Staples',\n",
       " 'L Brands Inc. (LB) -- Consumer Discretionary',\n",
       " 'L3Harris Technologies (LHX) -- Industrials',\n",
       " 'Laboratory Corp. of America Holding (LH) -- Health Care',\n",
       " 'Lam Research (LRCX) -- Information Technology',\n",
       " 'Lamb Weston Holdings Inc (LW) -- Consumer Staples',\n",
       " 'Las Vegas Sands (LVS) -- Consumer Discretionary',\n",
       " 'Leggett & Platt (LEG) -- Consumer Discretionary',\n",
       " 'Leidos Holdings (LDOS) -- Information Technology',\n",
       " 'Lennar Corp. (LEN) -- Consumer Discretionary',\n",
       " 'Lilly (Eli) & Co. (LLY) -- Health Care',\n",
       " 'Lincoln National (LNC) -- Financials',\n",
       " 'Linde plc (LIN) -- Materials',\n",
       " 'Live Nation Entertainment (LYV) -- Communication Services',\n",
       " 'LKQ Corporation (LKQ) -- Consumer Discretionary',\n",
       " 'Lockheed Martin Corp. (LMT) -- Industrials',\n",
       " 'Loews Corp. (L) -- Financials',\n",
       " \"Lowe's Cos. (LOW) -- Consumer Discretionary\",\n",
       " 'LyondellBasell (LYB) -- Materials',\n",
       " 'M&T Bank Corp. (MTB) -- Financials',\n",
       " 'Marathon Oil Corp. (MRO) -- Energy',\n",
       " 'Marathon Petroleum (MPC) -- Energy',\n",
       " 'MarketAxess (MKTX) -- Financials',\n",
       " \"Marriott Int'l. (MAR) -- Consumer Discretionary\",\n",
       " 'Marsh & McLennan (MMC) -- Financials',\n",
       " 'Martin Marietta Materials (MLM) -- Materials',\n",
       " 'Masco Corp. (MAS) -- Industrials',\n",
       " 'Mastercard Inc. (MA) -- Information Technology',\n",
       " 'McCormick & Co. (MKC) -- Consumer Staples',\n",
       " 'Maxim Integrated Products Inc (MXIM) -- Information Technology',\n",
       " \"McDonald's Corp. (MCD) -- Consumer Discretionary\",\n",
       " 'McKesson Corp. (MCK) -- Health Care',\n",
       " 'Medtronic plc (MDT) -- Health Care',\n",
       " 'Merck & Co. (MRK) -- Health Care',\n",
       " 'MetLife Inc. (MET) -- Financials',\n",
       " 'Mettler Toledo (MTD) -- Health Care',\n",
       " 'MGM Resorts International (MGM) -- Consumer Discretionary',\n",
       " 'Microchip Technology (MCHP) -- Information Technology',\n",
       " 'Micron Technology (MU) -- Information Technology',\n",
       " 'Microsoft Corp. (MSFT) -- Information Technology',\n",
       " 'Mid-America Apartments (MAA) -- Real Estate',\n",
       " 'Mohawk Industries (MHK) -- Consumer Discretionary',\n",
       " 'Molson Coors Beverage Company (TAP) -- Consumer Staples',\n",
       " 'Mondelez International (MDLZ) -- Consumer Staples',\n",
       " 'Monster Beverage (MNST) -- Consumer Staples',\n",
       " \"Moody's Corp (MCO) -- Financials\",\n",
       " 'Morgan Stanley (MS) -- Financials',\n",
       " 'The Mosaic Company (MOS) -- Materials',\n",
       " 'Motorola Solutions Inc. (MSI) -- Information Technology',\n",
       " 'MSCI Inc (MSCI) -- Financials',\n",
       " 'Mylan N.V. (MYL) -- Health Care',\n",
       " 'Nasdaq, Inc. (NDAQ) -- Financials',\n",
       " 'National Oilwell Varco Inc. (NOV) -- Energy',\n",
       " 'NetApp (NTAP) -- Information Technology',\n",
       " 'Netflix Inc. (NFLX) -- Communication Services',\n",
       " 'Newell Brands (NWL) -- Consumer Discretionary',\n",
       " 'Newmont Corporation (NEM) -- Materials',\n",
       " 'News Corp. Class A (NWSA) -- Communication Services',\n",
       " 'News Corp. Class B (NWS) -- Communication Services',\n",
       " 'NextEra Energy (NEE) -- Utilities',\n",
       " 'Nielsen Holdings (NLSN) -- Industrials',\n",
       " 'Nike, Inc. (NKE) -- Consumer Discretionary',\n",
       " 'NiSource Inc. (NI) -- Utilities',\n",
       " 'Noble Energy (NBL) -- Energy',\n",
       " 'Norfolk Southern Corp. (NSC) -- Industrials',\n",
       " 'Northern Trust Corp. (NTRS) -- Financials',\n",
       " 'Northrop Grumman (NOC) -- Industrials',\n",
       " 'NortonLifeLock (NLOK) -- Information Technology',\n",
       " 'Norwegian Cruise Line Holdings (NCLH) -- Consumer Discretionary',\n",
       " 'NRG Energy (NRG) -- Utilities',\n",
       " 'Nucor Corp. (NUE) -- Materials',\n",
       " 'Nvidia Corporation (NVDA) -- Information Technology',\n",
       " 'NVR, Inc. (NVR) -- Consumer Discretionary',\n",
       " \"O'Reilly Automotive (ORLY) -- Consumer Discretionary\",\n",
       " 'Occidental Petroleum (OXY) -- Energy',\n",
       " 'Old Dominion Freight Line (ODFL) -- Industrials',\n",
       " 'Omnicom Group (OMC) -- Communication Services',\n",
       " 'ONEOK (OKE) -- Energy',\n",
       " 'Oracle Corp. (ORCL) -- Information Technology',\n",
       " 'Otis Worldwide (OTIS) -- Industrials',\n",
       " 'PACCAR Inc. (PCAR) -- Industrials',\n",
       " 'Packaging Corporation of America (PKG) -- Materials',\n",
       " 'Parker-Hannifin (PH) -- Industrials',\n",
       " 'Paychex Inc. (PAYX) -- Information Technology',\n",
       " 'Paycom (PAYC) -- Information Technology',\n",
       " 'PayPal (PYPL) -- Information Technology',\n",
       " 'Pentair plc (PNR) -- Industrials',\n",
       " \"People's United Financial (PBCT) -- Financials\",\n",
       " 'PepsiCo Inc. (PEP) -- Consumer Staples',\n",
       " 'PerkinElmer (PKI) -- Health Care',\n",
       " 'Perrigo (PRGO) -- Health Care',\n",
       " 'Pfizer Inc. (PFE) -- Health Care',\n",
       " 'Philip Morris International (PM) -- Consumer Staples',\n",
       " 'Phillips 66 (PSX) -- Energy',\n",
       " 'Pinnacle West Capital (PNW) -- Utilities',\n",
       " 'Pioneer Natural Resources (PXD) -- Energy',\n",
       " 'PNC Financial Services (PNC) -- Financials',\n",
       " 'PPG Industries (PPG) -- Materials',\n",
       " 'PPL Corp. (PPL) -- Utilities',\n",
       " 'Principal Financial Group (PFG) -- Financials',\n",
       " 'Procter & Gamble (PG) -- Consumer Staples',\n",
       " 'Progressive Corp. (PGR) -- Financials',\n",
       " 'Prologis (PLD) -- Real Estate',\n",
       " 'Prudential Financial (PRU) -- Financials',\n",
       " 'Public Service Enterprise Group (PSEG) (PEG) -- Utilities',\n",
       " 'Public Storage (PSA) -- Real Estate',\n",
       " 'PulteGroup (PHM) -- Consumer Discretionary',\n",
       " 'PVH Corp. (PVH) -- Consumer Discretionary',\n",
       " 'Qorvo (QRVO) -- Information Technology',\n",
       " 'Quanta Services Inc. (PWR) -- Industrials',\n",
       " 'QUALCOMM Inc. (QCOM) -- Information Technology',\n",
       " 'Quest Diagnostics (DGX) -- Health Care',\n",
       " 'Ralph Lauren Corporation (RL) -- Consumer Discretionary',\n",
       " 'Raymond James Financial Inc. (RJF) -- Financials',\n",
       " 'Raytheon Technologies (RTX) -- Industrials',\n",
       " 'Realty Income Corporation (O) -- Real Estate',\n",
       " 'Regency Centers Corporation (REG) -- Real Estate',\n",
       " 'Regeneron Pharmaceuticals (REGN) -- Health Care',\n",
       " 'Regions Financial Corp. (RF) -- Financials',\n",
       " 'Republic Services Inc (RSG) -- Industrials',\n",
       " 'ResMed (RMD) -- Health Care',\n",
       " 'Robert Half International (RHI) -- Industrials',\n",
       " 'Rockwell Automation Inc. (ROK) -- Industrials',\n",
       " 'Rollins Inc. (ROL) -- Industrials',\n",
       " 'Roper Technologies (ROP) -- Industrials',\n",
       " 'Ross Stores (ROST) -- Consumer Discretionary',\n",
       " 'Royal Caribbean Group (RCL) -- Consumer Discretionary',\n",
       " 'S&P Global, Inc. (SPGI) -- Financials',\n",
       " 'Salesforce.com (CRM) -- Information Technology',\n",
       " 'SBA Communications (SBAC) -- Real Estate',\n",
       " 'Schlumberger Ltd. (SLB) -- Energy',\n",
       " 'Seagate Technology (STX) -- Information Technology',\n",
       " 'Sealed Air (SEE) -- Materials',\n",
       " 'Sempra Energy (SRE) -- Utilities',\n",
       " 'ServiceNow (NOW) -- Information Technology',\n",
       " 'Sherwin-Williams (SHW) -- Materials',\n",
       " 'Simon Property Group Inc (SPG) -- Real Estate',\n",
       " 'Skyworks Solutions (SWKS) -- Information Technology',\n",
       " 'SL Green Realty (SLG) -- Real Estate',\n",
       " 'Snap-on (SNA) -- Industrials',\n",
       " 'Southern Company (SO) -- Utilities',\n",
       " 'Southwest Airlines (LUV) -- Industrials',\n",
       " 'Stanley Black & Decker (SWK) -- Industrials',\n",
       " 'Starbucks Corp. (SBUX) -- Consumer Discretionary',\n",
       " 'State Street Corp. (STT) -- Financials',\n",
       " 'STERIS plc (STE) -- Health Care',\n",
       " 'Stryker Corp. (SYK) -- Health Care',\n",
       " 'SVB Financial (SIVB) -- Financials',\n",
       " 'Synchrony Financial (SYF) -- Financials',\n",
       " 'Synopsys Inc. (SNPS) -- Information Technology',\n",
       " 'Sysco Corp. (SYY) -- Consumer Staples',\n",
       " 'T-Mobile US (TMUS) -- Communication Services',\n",
       " 'T. Rowe Price Group (TROW) -- Financials',\n",
       " 'Take-Two Interactive (TTWO) -- Communication Services',\n",
       " 'Tapestry, Inc. (TPR) -- Consumer Discretionary',\n",
       " 'Target Corp. (TGT) -- Consumer Discretionary',\n",
       " 'TE Connectivity Ltd. (TEL) -- Information Technology',\n",
       " 'TechnipFMC (FTI) -- Energy',\n",
       " 'Teledyne Technologies (TDY) -- Industrials',\n",
       " 'Teleflex (TFX) -- Health Care',\n",
       " 'Texas Instruments (TXN) -- Information Technology',\n",
       " 'Textron Inc. (TXT) -- Industrials',\n",
       " 'Thermo Fisher Scientific (TMO) -- Health Care',\n",
       " 'Tiffany & Co. (TIF) -- Consumer Discretionary',\n",
       " 'TJX Companies Inc. (TJX) -- Consumer Discretionary',\n",
       " 'Tractor Supply Company (TSCO) -- Consumer Discretionary',\n",
       " 'Trane Technologies plc (TT) -- Industrials',\n",
       " 'TransDigm Group (TDG) -- Industrials',\n",
       " 'The Travelers Companies Inc. (TRV) -- Financials',\n",
       " 'Truist Financial (TFC) -- Financials',\n",
       " 'Twitter, Inc. (TWTR) -- Communication Services',\n",
       " 'Tyler Technologies (TYL) -- Information Technology',\n",
       " 'Tyson Foods (TSN) -- Consumer Staples',\n",
       " 'UDR, Inc. (UDR) -- Real Estate',\n",
       " 'Ulta Beauty (ULTA) -- Consumer Discretionary',\n",
       " 'U.S. Bancorp (USB) -- Financials',\n",
       " 'Under Armour (Class A) (UAA) -- Consumer Discretionary',\n",
       " 'Under Armour (Class C) (UA) -- Consumer Discretionary',\n",
       " 'Union Pacific Corp (UNP) -- Industrials',\n",
       " 'United Airlines Holdings (UAL) -- Industrials',\n",
       " 'United Health Group Inc. (UNH) -- Health Care',\n",
       " 'United Parcel Service (UPS) -- Industrials',\n",
       " 'United Rentals, Inc. (URI) -- Industrials',\n",
       " 'Universal Health Services (UHS) -- Health Care',\n",
       " 'Unum Group (UNM) -- Financials',\n",
       " 'VF Corporation (VFC) -- Consumer Discretionary',\n",
       " 'Valero Energy (VLO) -- Energy',\n",
       " 'Varian Medical Systems (VAR) -- Health Care',\n",
       " 'Ventas Inc (VTR) -- Real Estate',\n",
       " 'Verisign Inc. (VRSN) -- Information Technology',\n",
       " 'Verisk Analytics (VRSK) -- Industrials',\n",
       " 'Verizon Communications (VZ) -- Communication Services',\n",
       " 'Vertex Pharmaceuticals Inc (VRTX) -- Health Care',\n",
       " 'ViacomCBS (VIAC) -- Communication Services',\n",
       " 'Visa Inc. (V) -- Information Technology',\n",
       " 'Vornado Realty Trust (VNO) -- Real Estate',\n",
       " 'Vulcan Materials (VMC) -- Materials',\n",
       " 'W. R. Berkley Corporation (WRB) -- Financials',\n",
       " 'Wabtec Corporation (WAB) -- Industrials',\n",
       " 'Walmart (WMT) -- Consumer Staples',\n",
       " 'Walgreens Boots Alliance (WBA) -- Consumer Staples',\n",
       " 'The Walt Disney Company (DIS) -- Communication Services',\n",
       " 'Waste Management Inc. (WM) -- Industrials',\n",
       " 'Waters Corporation (WAT) -- Health Care',\n",
       " 'WEC Energy Group (WEC) -- Utilities',\n",
       " 'Wells Fargo (WFC) -- Financials',\n",
       " 'Welltower Inc. (WELL) -- Real Estate',\n",
       " 'West Pharmaceutical Services (WST) -- Health Care',\n",
       " 'Western Digital (WDC) -- Information Technology',\n",
       " 'Western Union Co (WU) -- Information Technology',\n",
       " 'WestRock (WRK) -- Materials',\n",
       " 'Weyerhaeuser (WY) -- Real Estate',\n",
       " 'Whirlpool Corp. (WHR) -- Consumer Discretionary',\n",
       " 'Williams Companies (WMB) -- Energy',\n",
       " 'Willis Towers Watson (WLTW) -- Financials',\n",
       " 'Wynn Resorts Ltd (WYNN) -- Consumer Discretionary',\n",
       " 'Xcel Energy Inc (XEL) -- Utilities',\n",
       " 'Xerox (XRX) -- Information Technology',\n",
       " 'Xilinx (XLNX) -- Information Technology',\n",
       " 'Xylem Inc. (XYL) -- Industrials',\n",
       " 'Yum! Brands Inc (YUM) -- Consumer Discretionary',\n",
       " 'Zebra Technologies (ZBRA) -- Information Technology',\n",
       " 'Zimmer Biomet Holdings (ZBH) -- Health Care',\n",
       " 'Zions Bancorp (ZION) -- Financials',\n",
       " 'Zoetis (ZTS) -- Health Care']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the format\n",
    "companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first item, for example, `3M Company` is the company name, `MMM` is the ticker symbol, and `Industrials` is the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df341d1f1838c29e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q1.a)\n",
    "\n",
    "First, we want to know which companies have at least one digit in their name. Return the full strings that include these companies in a list assigned to a variable `ans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cddd8e2e48afb31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ans = [ ... ]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ans = re.findall(r\"[a-zA-Z]*\\s*[\\d]+[a-zA-Z]*[^',]*\", str(companies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5a358ed83c473214",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(' '.join(ans).encode()).hexdigest() == '68e02db4d479495ee3af9038ee5686ff3b098e9a6268ae4137a32020834878e7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e3cc736b933d71e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "Next, find the companies that start with \"C\" or \"L\" and whose names end in \"Corp.\" or \"Inc.\" (including the punctuation). Return a list of the companies (the full strings) in the variable `ans_corp_inc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bf14a2df3dd3e4e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Care',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp (AES) -- Utilities',\n",
       " 'LAC Inc (AFL) -- Financials',\n",
       " 'Care',\n",
       " 'Corp (ALB) -- Materials',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Corp (LNT) -- Utilities',\n",
       " 'Corp (ALL) -- Financials',\n",
       " 'Consumer Staples',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp (AEE) -- Utilities',\n",
       " 'Corp. (AMT) -- Real Estate',\n",
       " 'Corp (ABC) -- Health Care',\n",
       " 'Care',\n",
       " 'Corp (APH) -- Information Technology',\n",
       " 'Care',\n",
       " 'Corp (AOS) -- Industrials',\n",
       " 'Corporation (APA) -- Energy',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Staples',\n",
       " 'Co. (AJG) -- Financials',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp (AVY) -- Materials',\n",
       " 'Corp (BLL) -- Materials',\n",
       " 'Corp (BAC) -- Financials',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Co. Inc. (BBY) -- Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Corp. (BF.B) -- Consumer Staples',\n",
       " 'Consumer Staples',\n",
       " 'Capital One Financial (COF) -- Financials',\n",
       " 'Cardinal Health Inc. (CAH) -- Health Care',\n",
       " 'Carmax Inc (KMX) -- Consumer Discretionary',\n",
       " 'Carnival Corp. (CCL) -- Consumer Discretionary',\n",
       " 'Carrier Global (CARR) -- Industrials',\n",
       " 'Cboe Global Markets (CBOE) -- Financials',\n",
       " 'Centene Corporation (CNC) -- Health Care',\n",
       " 'CenterPoint Energy (CNP) -- Utilities',\n",
       " 'CenturyLink Inc (CTL) -- Communication Services',\n",
       " 'Cerner (CERN) -- Health Care',\n",
       " 'Corporation (SCHW) -- Financials',\n",
       " 'Corp. (CVX) -- Energy',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Staples',\n",
       " 'Corp. (CI) -- Health Care',\n",
       " 'Cincinnati Financial (CINF) -- Financials',\n",
       " 'Cintas Corporation (CTAS) -- Industrials',\n",
       " 'CSCO) -- Information Technology',\n",
       " 'Clorox Company (CLX) -- Consumer Staples',\n",
       " 'Coca-Cola Company (KO) -- Consumer Staples',\n",
       " 'Consumer Staples',\n",
       " 'Corp. (CMCSA) -- Communication Services',\n",
       " 'Conagra Brands (CAG) -- Consumer Staples',\n",
       " 'Concho Resources (CXO) -- Energy',\n",
       " 'ConocoPhillips (COP) -- Energy',\n",
       " 'Consolidated Edison (ED) -- Utilities',\n",
       " 'Constellation Brands (STZ) -- Consumer Staples',\n",
       " 'Cooper Companies (COO) -- Health Care',\n",
       " 'Copart Inc (CPRT) -- Industrials',\n",
       " 'Corning Inc. (GLW) -- Information Technology',\n",
       " 'Corteva (CTVA) -- Materials',\n",
       " 'Corp. (COST) -- Consumer Staples',\n",
       " 'Consumer Staples',\n",
       " 'Crown Castle International Corp. (CCI) -- Real Estate',\n",
       " 'Corp. (CSX) -- Industrials',\n",
       " 'CMI) -- Industrials',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (DHR) -- Health Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Co. (DE) -- Industrials',\n",
       " 'Lines Inc. (DAL) -- Industrials',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary\"',\n",
       " 'Corporation (DOV) -- Industrials',\n",
       " 'Co. (DTE) -- Utilities',\n",
       " 'Corp (DRE) -- Real Estate',\n",
       " 'Corporation (ETN) -- Industrials',\n",
       " 'Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Corp. (ETR) -- Utilities',\n",
       " 'Consumer Staples',\n",
       " 'Corp. (EXC) -- Utilities',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (XOM) -- Energy',\n",
       " 'Corporation (FDX) -- Industrials',\n",
       " 'Corp (FE) -- Utilities',\n",
       " 'Cor Technologies Inc (FLT) -- Information Technology',\n",
       " 'Corporation (FLS) -- Industrials',\n",
       " 'Corporation (FMC) -- Materials',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp (FTV) -- Industrials',\n",
       " 'Corporation (Class A) (FOXA) -- Communication Services',\n",
       " 'Corporation (Class B) (FOX) -- Communication Services',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Staples',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Co. (HAL) -- Energy',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Consumer Staples',\n",
       " 'Corporation (HES) -- Energy',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp (HFC) -- Energy',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (HRL) -- Consumer Staples',\n",
       " 'Care',\n",
       " 'Corporation (IEX) -- Industrials',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Corp. (INTC) -- Information Technology',\n",
       " 'Care',\n",
       " 'Corp. (IPGP) -- Information Technology',\n",
       " 'Care',\n",
       " 'Consumer Staples',\n",
       " 'Care',\n",
       " 'Controls International (JCI) -- Industrials',\n",
       " 'Co. (JPM) -- Financials',\n",
       " 'Co. (K) -- Consumer Staples',\n",
       " 'Corp (KEY) -- Financials',\n",
       " 'Consumer Staples',\n",
       " 'Corporation (KLAC) -- Information Technology',\n",
       " 'Corp. (KSS) -- Consumer Discretionary\"',\n",
       " 'Consumer Staples',\n",
       " 'Co. (KR) -- Consumer Staples',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. of America Holding (LH) -- Health Care',\n",
       " 'LRCX) -- Information Technology',\n",
       " 'Consumer Staples',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Lennar Corp. (LEN) -- Consumer Discretionary',\n",
       " 'Co. (LLY) -- Health Care',\n",
       " 'Lincoln National (LNC) -- Financials',\n",
       " 'Linde plc (LIN) -- Materials',\n",
       " 'Corporation (LKQ) -- Consumer Discretionary',\n",
       " 'Lockheed Martin Corp. (LMT) -- Industrials',\n",
       " 'Corp. (L) -- Financials',\n",
       " 'Consumer Discretionary\"',\n",
       " 'LyondellBasell (LYB) -- Materials',\n",
       " 'Corp. (MTB) -- Financials',\n",
       " 'Corp. (MRO) -- Energy',\n",
       " 'Consumer Discretionary\"',\n",
       " 'Lennan (MMC) -- Financials',\n",
       " 'Corp. (MAS) -- Industrials',\n",
       " 'Cormick & Co. (MKC) -- Consumer Staples',\n",
       " 'Corp. (MCD) -- Consumer Discretionary\"',\n",
       " 'Corp. (MCK) -- Health Care',\n",
       " 'Care',\n",
       " 'Co. (MRK) -- Health Care',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (MSFT) -- Information Technology',\n",
       " 'Consumer Discretionary',\n",
       " 'Coors Beverage Company (TAP) -- Consumer Staples',\n",
       " 'Consumer Staples',\n",
       " 'Consumer Staples',\n",
       " 'Corp (MCO) -- Financials\"',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Corporation (NEM) -- Materials',\n",
       " 'Corp. Class A (NWSA) -- Communication Services',\n",
       " 'Corp. Class B (NWS) -- Communication Services',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (NSC) -- Industrials',\n",
       " 'Corp. (NTRS) -- Financials',\n",
       " 'Lock (NLOK) -- Information Technology',\n",
       " 'Line Holdings (NCLH) -- Consumer Discretionary',\n",
       " 'Corp. (NUE) -- Materials',\n",
       " 'Corporation (NVDA) -- Information Technology',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary\"',\n",
       " 'Line (ODFL) -- Industrials',\n",
       " 'Corp. (ORCL) -- Information Technology',\n",
       " 'Corporation of America (PKG) -- Materials',\n",
       " 'Consumer Staples',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Consumer Staples',\n",
       " 'Capital (PNW) -- Utilities',\n",
       " 'Corp. (PPL) -- Utilities',\n",
       " 'Consumer Staples',\n",
       " 'Corp. (PGR) -- Financials',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (PVH) -- Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Corporation (RL) -- Consumer Discretionary',\n",
       " 'Corporation (O) -- Real Estate',\n",
       " 'Centers Corporation (REG) -- Real Estate',\n",
       " 'Care',\n",
       " 'Corp. (RF) -- Financials',\n",
       " 'Care',\n",
       " 'Consumer Discretionary',\n",
       " 'Caribbean Group (RCL) -- Consumer Discretionary',\n",
       " 'Corp. (SBUX) -- Consumer Discretionary',\n",
       " 'Corp. (STT) -- Financials',\n",
       " 'Care',\n",
       " 'Corp. (SYK) -- Health Care',\n",
       " 'Corp. (SYY) -- Consumer Staples',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp. (TGT) -- Consumer Discretionary',\n",
       " 'Connectivity Ltd. (TEL) -- Information Technology',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Co. (TIF) -- Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Staples',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Corp (UNP) -- Industrials',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Corporation (VFC) -- Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Care',\n",
       " 'Corporation (WRB) -- Financials',\n",
       " 'Corporation (WAB) -- Industrials',\n",
       " 'Consumer Staples',\n",
       " 'Consumer Staples',\n",
       " 'Corporation (WAT) -- Health Care',\n",
       " 'Care',\n",
       " 'Corp. (WHR) -- Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Consumer Discretionary',\n",
       " 'Care',\n",
       " 'Care']"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ans_corp_inc = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ans_corp_inc = re.findall(r\"[CL][a-zA-Z][Corp.|Inc.]+[^',]*\", str(companies)) \n",
    "ans_corp_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-19b61469674be299",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies starting with C or L that end in Inc. or Corp.:  251\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-e31cb6af1b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of companies starting with C or L that end in Inc. or Corp.: \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_corp_inc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'Crown Castle International Corp. (CCI) -- Real Estate'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans_corp_inc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'Citigroup Inc. (C) -- Financials'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans_corp_inc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'L Brands Inc. (LB) -- Consumer Discretionary'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans_corp_inc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'Lennar Corp. (LEN) -- Consumer Discretionary'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans_corp_inc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Number of companies starting with C or L that end in Inc. or Corp.: \" , len(ans_corp_inc))\n",
    "assert 'Crown Castle International Corp. (CCI) -- Real Estate' in ans_corp_inc\n",
    "assert 'Citigroup Inc. (C) -- Financials' in ans_corp_inc\n",
    "assert 'L Brands Inc. (LB) -- Consumer Discretionary' in ans_corp_inc\n",
    "assert 'Lennar Corp. (LEN) -- Consumer Discretionary' in ans_corp_inc\n",
    "assert 'Charles Schwab Corporation (SCHW) -- Financials' not in ans_corp_inc\n",
    "assert 'Laboratory Corp. of America Holding (LH) -- Health Care' not in ans_corp_inc\n",
    "assert hashlib.sha256(' '.join(ans_corp_inc).encode()).hexdigest() == '6096674be26a40eb53363e63a9ac32fc5d157f1fd1737d55af9b584752708023'\n",
    "assert len(ans_corp_inc) == 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6727f8213243afc0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q1.c)\n",
    "\n",
    "Now, extract the stock tickers from the strings using `re.search()`. You should be able to do this using just 1 regex pattern. Hint: you may want to read about [capturing groups](https://docs.python.org/3/howto/regex.html#grouping), and don't forget you can use tools like https://regex101.com/ to test your regexes. Store the tickers as a list called `tickers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-447eb675481e47fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# tickers = [ ... ]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "tickers = []\n",
    "\n",
    "pattern = re.compile(r'^.*\\((.*?)\\)[^\\(]*$')\n",
    "for value in companies:\n",
    "    tickers.append(pattern.search(value).group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-535072609d84d577",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(tickers) == 505\n",
    "assert hashlib.sha256(tickers[10].encode()).hexdigest() == '104671425b6d8ba2bbf18db03a7144427eff2afce7f5a180b67687ea7160ed2c'\n",
    "assert hashlib.sha256(tickers[79].encode()).hexdigest() == '811ebf6f0e86baf332242b37c11f3bcb8a06ad9b128f137b7ae72d707b43bc2e'\n",
    "assert hashlib.sha256(tickers[263].encode()).hexdigest() == '6da43b944e494e885e69af021f93c6d9331c78aa228084711429160a5bbd15b5'\n",
    "assert hashlib.sha256(' '.join(tickers).encode()).hexdigest() == '604bd5836b606048446b5632c3146d9d8f4d74d514fde285d4ef22599aeca126'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0409818769bbf3d0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q2. Airline tweets\n",
    "\n",
    "Since probably most of us have been missing traveling during this pandemic, we'll be working with some [tweets](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) describing how people felt about certain airlines to remind us of some of the horrors -- and joys -- of air travel.\n",
    "\n",
    "First, we will be performing common preprocessing operations on this text. Start by downloading the data and loading it into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abce95c72c255d16",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/Tweets.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df[['airline_sentiment', 'text']]\n",
    "# ignore the neutral class for this exercise\n",
    "df = df[df['airline_sentiment'] != 'neutral']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11541, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# train dev test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8078,), (8078,), (1731,), (1731,), (1732,), (1732,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-addc0c904c359402",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.a)\n",
    "\n",
    "First tokenize the data. Implement the function to receive an NLTK-style tokenizer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b7930a61806e32",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_tokenizer(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with the tokens of given text. I.e\n",
    "    for an input ['Abc def', 'Ghi jkl mn'] it returns [['Abc', 'def'], ['Ghi', 'jkl', 'mn']]\n",
    "    \n",
    "    Args:\n",
    "    data - list with the data\n",
    "    tokenizer - nltk tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    tokens = []\n",
    "    for line in data:\n",
    "        tokens.append(tokenizer.tokenize(line))\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f829c5a222c54690",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "data_tok = apply_tokenizer(X_train, tokenizer)\n",
    "\n",
    "assert len(data_tok) == 8078\n",
    "assert len([w for s in data_tok for w in s]) == 191097\n",
    "assert hashlib.sha256(' '.join(data_tok[1234]).encode()).hexdigest() == 'cd12c361e49ccd59ba526ee54b8a6093787977c0ed8bfc43358e5fcf0b0f44c3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b76d100b971a777f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.b)\n",
    "\n",
    "The second step you will implement is lowercasing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee47fb5a45fbd622",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_lowercase(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with all the tokens lowecased.\n",
    "    \n",
    "    Args:\n",
    "    data - list with tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    output = []\n",
    "    for line in data: \n",
    "        output_word=[]\n",
    "        for word in line:\n",
    "            output_word.append(str(word).lower())\n",
    "            \n",
    "        output.append(output_word)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7979e12840663ea2",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_tok_lc = apply_lowercase(data_tok)\n",
    "\n",
    "assert len(data_tok_lc) == 8078\n",
    "assert len([w for s in data_tok_lc for w in s]) == 191097\n",
    "assert hashlib.sha256(' '.join(data_tok_lc[1234]).encode()).hexdigest() == 'bc48ec5ed569e79050a6c09c3a9e1e9f7e73d29aaf0c9ef948867d2e31ba25ff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8044c14c20583cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.c)\n",
    "\n",
    "Now implement a function that filters the stopwords. We will use NLTK's built-in English stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ecb29c8fe117f1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_stopwords(data, stopword_list):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no stopwords.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stopword_list - list of stopwords to filter out\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the stopwords from the text\n",
    "    # data_filt = ...\n",
    "    # YOUR CODE HERE\n",
    "    data_filt = []\n",
    "\n",
    "    for line in data: \n",
    "        output_word=[]\n",
    "        for word in line:\n",
    "            if word not in stopword_list:\n",
    "                output_word.append(word)\n",
    "        data_filt.append(output_word)\n",
    "    \n",
    "    return data_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f70da0255ea6e291",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_tok_lc_sw = apply_filter_stopwords(data_tok_lc, stopword_list)\n",
    "assert len(data_tok_lc_sw) == 8078\n",
    "assert len([w for s in data_tok_lc_sw for w in s]) == 124220\n",
    "assert hashlib.sha256(' '.join(data_tok_lc_sw[1234]).encode()).hexdigest() == 'a281eeb3450057df271950c6c76d287eb6ba05f7a6e9b5c5949e0e6b73e847c7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d0cb317596faa2f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.d)\n",
    "\n",
    "After filtering stopwords, we want to remove punctuation from the text as well. Make use of `string.punctuation` to do so. Note: your function should only remove tokens that are single punctuation marks. Tokens such as `'!!'` or `'@JetBlue'` should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0cd3cf5cc97a8f2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_punct(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no punctuation.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    data_punct = []\n",
    "\n",
    "    for line in data: \n",
    "        output_word=[]\n",
    "        for word in line:\n",
    "            if word not in string.punctuation:\n",
    "                output_word.append(word)\n",
    "        data_punct.append(output_word)\n",
    "    \n",
    "    return data_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-650a677a3a01d1bf",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_tok_lc_sw_punct = apply_filter_punct(data_tok_lc_sw)\n",
    "\n",
    "assert len(data_tok_lc_sw_punct) == 8078\n",
    "assert len([w for s in data_tok_lc_sw_punct for w in s]) == 91017\n",
    "assert hashlib.sha256(' '.join(data_tok_lc_sw_punct[1234]).encode()).hexdigest() == 'eb94735dd3b8067e44f7529ffd5e358afed116a6acd2fd349e1f67bc770d6ef4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38d66dd89731e114",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.e)\n",
    "\n",
    "The last preprocessing step you are going to implement is stemming. Implement the function to receive an NLTK-style stemmer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a831ba989f3e50e6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_stemmer(data, stemmer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with stemmed data.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stemmer - instance of stemmer to use\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    data_stem = []\n",
    "\n",
    "    for line in data: \n",
    "        output_word=[]\n",
    "        for word in line:\n",
    "            output_word.append(stemmer.stem(word))\n",
    "        data_stem.append(output_word)\n",
    "    \n",
    "    return data_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3596a6510ebbda3d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "data_tok_lc_sw_punct_stem = apply_stemmer(data_tok_lc_sw_punct, stemmer)\n",
    "\n",
    "assert len(data_tok_lc_sw_punct_stem) == 8078\n",
    "assert len([w for s in data_tok_lc_sw_punct_stem for w in s]) == 91017\n",
    "assert hashlib.sha256(' '.join(data_tok_lc_sw_punct_stem[1234]).encode()).hexdigest() == '248360a1225a1f168e92ca94c5563fb19aec943325dda3ccc6111c1d763f09be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bfe5aed6ebdbb26",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.f)\n",
    "\n",
    "Finally, join everything in a function, that applies the steps in the following order, in :\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Filtering stopwords\n",
    "* Filtering punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea5b2305431c20dd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, lower=True, remove_punct=True, stopwords=[]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, sentences):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        # sentences_tokens = ...\n",
    "        # YOUR CODE HERE\n",
    "        sentences_tokens = apply_tokenizer(sentences, tokenizer)\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_lowercase(sentences_tokens)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_filter_punct(sentences_tokens)\n",
    "\n",
    "        if self.stopwords:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_filter_stopwords(sentences_tokens, stopword_list)\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = apply_stemmer(sentences_tokens, stemmer)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentences_prep = [\" \".join(tokens).strip() for tokens in sentences_tokens]\n",
    "        return sentences_prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a4c1aa16cea2ffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopword_list\n",
    ")\n",
    "\n",
    "X_train_pre = text_cleaner.clean_sentences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@USAirways It was US 893. The gate was open after about 50 mins waiting. What a great way to finish an 18 hour delayed arrival!!',\n",
       "  'usairway us 893 gate open 50 min wait great way finish 18 hour delay arriv !!'),\n",
       " ('@JetBlue is the greatest airline ever 💕✈️💺 #TrueBluePoints #jetbluemember',\n",
       "  'jetblu greatest airlin ever 💕✈️💺 truebluepoint jetbluememb'),\n",
       " (\"@SouthwestAir is having a sale! I'm delighted!\",\n",
       "  'southwestair sale delight')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X_train, X_train_pre))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4a87d0c9b1f20f7e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(X_train_pre) == 8078\n",
    "assert len([w for s in X_train_pre for w in s.split()]) == 91017\n",
    "assert X_train_pre[8] == 'unit travel megzezzo injur gate agent chicago awesom help ty roadwarrior'\n",
    "assert X_train_pre[7999] == 'jetblu case alert arriv late flight four hour delay buy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f48abbdab8acc3d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q3. Text classification\n",
    "\n",
    "We will now use what we've learned to try to classify the sentiment of these airline tweets as positive or negative. Let's first load the preprocessed data (it's slightly different from the answer to Q2) and double check the balance of the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_list(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = file_to_list('data/tweets_train_preprocessed.txt')\n",
    "X_dev_pre = file_to_list('data/tweets_dev_preprocessed.txt')\n",
    "X_test_pre = file_to_list('data/tweets_test_preprocessed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    6407\n",
       "positive    1671\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    1380\n",
       "positive     351\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e5147de37fa0351",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So, we should be aiming for better than 80% accuracy which is what we would get if we naively predicted negative for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35b87545f98deeea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.a) \n",
    "The first thing we'll try is the simple baseline of a Bag of Words model. Use sklearn's CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-046593b38d1d0a86",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fit and transform the preprocessed train and dev data with CountVectorizer\n",
    "# vec = ...\n",
    "# X_train_vec = ...\n",
    "# X_dev_vec = ...\n",
    "# YOUR CODE HERE\n",
    "vec = CountVectorizer()\n",
    "X_train_vec = vec.fit_transform(X_train_pre)\n",
    "X_dev_vec = vec.transform(X_dev_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a4941d45e32e6ca7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(vec.vocabulary_) == 8211\n",
    "assert 'awwweesssooome' in vec.vocabulary_\n",
    "assert vec.vocabulary_['awwweesssooome'] == 1544\n",
    "assert hashlib.sha256(' '.join(vec.get_feature_names()[-20:]).encode()).hexdigest() == '585a48103c34d5be5b489f3aef9534bf02dde2e56949e5a48b70e95c5e834304'\n",
    "assert ' '.join([str(i) for i in X_train_vec[11].indices]) == '3601 1313 3798 5322 2651 1991 3355 7285 7042 3991 7050 6701'\n",
    "assert ' '.join([str(i) for i in X_train_vec[11].data]) == '1 1 1 1 1 1 1 1 1 1 1 1'\n",
    "assert ' '.join([str(i) for i in X_dev_vec[1111].indices]) == '1287 1313 1591 1977 2583 3339 3342 4029 4506 4525 4886 5344 8046'\n",
    "assert ' '.join([str(i) for i in X_dev_vec[1111].data]) == '1 1 1 1 1 1 1 1 1 1 1 1 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b676075741ecae30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's take a look at some of the words in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8652c29d3d784f8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000ft', '000lbs', '0016', '00am', '00p', '00pm', '01', '0162389030167']\n",
      "['8wbzorrn3c', '8x7xvm', '90', '900', '900s', '904am', '90min', '910', '912', '9148445695']\n",
      "['mk', 'mke', 'mkpognntyc', 'mkt', 'mktg', 'mkwlkr', 'ml', 'ml1jacpmch', 'mli', 'mmm']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names()[:10])\n",
    "print(vec.get_feature_names()[1000:1010])\n",
    "print(vec.get_feature_names()[5000:5010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the words seem pretty random, we might not need them at all. But let's come back to this later and get our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c28fc309dcd2e90",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# finally, train a Multinomial Naive Bayes classifier on train and predict on dev\n",
    "# store the classifier in a variable clf\n",
    "# return the dev set predictions in a variable y_dev_pred\n",
    "# YOUR CODE HERE\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-41ffc1ce5d8435d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_allclose(clf.intercept_, np.array([-1.57572207]), rtol=1e-3)\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'negative positive negative negative negative negative positive negative negative negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94      1380\n",
      "    positive       0.85      0.63      0.72       351\n",
      "\n",
      "    accuracy                           0.90      1731\n",
      "   macro avg       0.88      0.80      0.83      1731\n",
      "weighted avg       0.90      0.90      0.90      1731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the results\n",
    "print(classification_report(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: americanair exact ill fli aa dalla airlin trust\n",
      "Predicted: negative, Actual: positive\n",
      "\n",
      "Sentence: delet account jetblu\n",
      "Predicted: positive, Actual: negative\n",
      "\n",
      "Sentence: unit flight rsw tonight amp twin 3 year old pilot row stay help get boy amp bag lifesav\n",
      "Predicted: negative, Actual: positive\n",
      "\n",
      "Sentence: southwestair holi fuckinf shit\n",
      "Predicted: negative, Actual: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we should also look at some misclassified examples\n",
    "for text, pred, true in zip(X_dev_pre[:50], y_dev_pred[:50], y_dev[:50]):\n",
    "    if pred != true:\n",
    "        print(f\"Sentence: {text}\")\n",
    "        print(f\"Predicted: {pred}, Actual: {true}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We beat the naivest baseline of always guessing negative, but let's see if we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.c)\n",
    "Now let's try ngrams instead of plain unigrams to see if we get a boost in performance. But first, let's streamline the process in a nice function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af90809e6c74e780",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(X_train, X_dev, y_train, y_dev, ngram_range=(1,1), max_features=None):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with the predictions and the\n",
    "    current accuracy in the validation set. Print the classification report as well.\n",
    "    Assume the documents are already preprocessed\n",
    "    \n",
    "    Args:\n",
    "    X_train - preprocessed tweets in training data\n",
    "    X_dev - preprocessed tweets in dev data\n",
    "    y_train - labels of training data\n",
    "    y_dev - labels of dev data\n",
    "    ngram_range - ngram range to use in CountVectorizer (tuple)\n",
    "    max_features - max number of features to use in CountVectorizer (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline containing the countvectorizer and the multinomial NB classifier\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # y_dev_pred = (...)\n",
    "    # print the classification report\n",
    "    # acc = (...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range, max_features=max_features)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_dev_pred = text_clf.predict(X_dev)\n",
    "    \n",
    "    acc = np.mean(y_dev_pred == y_dev)\n",
    "\n",
    "    # return text_clf, y_dev_pred, acc\n",
    "    # YOUR CODE HERE\n",
    "    return text_clf, y_dev_pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-97ba1cceed5a53f9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev)\n",
    "\n",
    "# check same as before\n",
    "assert_allclose(clf['clf'].intercept_, np.array([-1.57572207]), rtol=1e-3)\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'negative positive negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.9024, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.d)\n",
    "Now try with both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e471f692627c2756",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run train_and_validate() but with the correct ngram range to have both unigrams and bigrams in the vocabulary\n",
    "# clf,y_dev_pred, acc = ...\n",
    "# YOUR CODE HERE\n",
    "#clf, y_dev_pred, acc = \n",
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-865becc9662b6c06",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_allclose(clf['clf'].intercept_, np.array([-1.57572207]), rtol=1e-3)\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'positive negative negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.8908, rtol=1e-2)\n",
    "assert len(clf['vect'].vocabulary_) == 60813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.e)\n",
    "Find the top 20 most common vocabulary items in the training data. Hint: don't forget how the countvectorizer BoW matrix is actually structured, and how you may have to combine the rows to get the information you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-088364758c0cf246",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "split not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-30dfa6ee0498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mngrams_dic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/blu07/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: split not found"
     ]
    }
   ],
   "source": [
    "# transform the preprocessed training data again\n",
    "# return the a list of tuples containing the top 20 most common ngrams and their counts (ngram, count)\n",
    "# top_20_ngrams = ...\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ngrams_dic = Counter()\n",
    "\n",
    "for doc in X_train_:\n",
    "    words = doc.split()\n",
    "    ngrams_dic.update(words)\n",
    "    \n",
    "ngrams = OrderedDict(ngrams_dic.most_common())\n",
    "\n",
    "top_20_ngrams = list(ngrams_dic.items())[:20]\n",
    "top_20_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1e00b9db09c36cc1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ffe012132ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                          \u001b[0;34m(\u001b[0m\u001b[0;34m'co'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m494\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                          \u001b[0;34m(\u001b[0m\u001b[0;34m'bag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m485\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                          ('hold', 475)]\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert top_20_ngrams == [('flight', 2844),\n",
    "                         ('unit', 2373),\n",
    "                         ('usairway', 1875),\n",
    "                         ('americanair', 1725),\n",
    "                         ('southwestair', 1236),\n",
    "                         ('jetblu', 1180),\n",
    "                         ('thank', 1060),\n",
    "                         ('get', 975),\n",
    "                         ('hour', 814),\n",
    "                         ('cancel', 678),\n",
    "                         ('delay', 666),\n",
    "                         ('servic', 647),\n",
    "                         ('time', 633),\n",
    "                         ('help', 604),\n",
    "                         ('custom', 600),\n",
    "                         ('call', 518),\n",
    "                         ('wait', 515),\n",
    "                         ('co', 494),\n",
    "                         ('bag', 485),\n",
    "                         ('hold', 475)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9cbc43f83473ed9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.f)\n",
    "We saw with just unigrams that there were already a lot of unhelpful words in the vocabulary, and now with the addition of bigrams the vocabulary is much bigger. Let's get rid of some infrequent ngrams by limiting the max number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c3e6d0f22dc12fb5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run train_and_validate() with uni- and bigrams and max features of 20000\n",
    "# clf, y_dev_pred, acc = ...\n",
    "# YOUR CODE HERE\n",
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev, ngram_range=(1,2), max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25d7ead74fe6ec50",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ' '.join(y_dev_pred[10:20]) == 'negative negative negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.9076, rtol=1e-2)\n",
    "assert len(clf['vect'].vocabulary_) == 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics improved a little! Let's see if we can get even better performance now by using the relative importance of ngrams with TfIdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28921443e50007c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4. TfIdf\n",
    "#### Q4.a)\n",
    "First, rewrite the train and validate function from before to include the TfIdf step in the Pipeline. Use sklearn's `TfIdfTransformer`. Also, add kwargs for CountVectorizer's `max_df` and `min_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-02453bb681cd33b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate_with_tfidf(X_train, X_dev, y_train, y_dev, ngram_range=(1,1), max_features=None, max_df=1.0, min_df=1):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with the predictions and the\n",
    "    current accuracy in the validation set. Print the classification report as well.\n",
    "    Assume the documents are already preprocessed\n",
    "    \n",
    "    Args:\n",
    "    X_train - preprocessed tweets in training data\n",
    "    X_dev - preprocessed tweets in dev data\n",
    "    y_train - labels of training data\n",
    "    y_dev - labels of dev data\n",
    "    ngram_range - ngram range to use in CountVectorizer (tuple)\n",
    "    max_features - max number of features to use in CountVectorizer (int)\n",
    "    max_df = max_df for CountVectorizer (int or float)\n",
    "    min_df = min_df for CountVectorizer (int or float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline containing the countvectorizer and the multinomial NB classifier\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # y_dev_pred = (...)\n",
    "    # print the classification report\n",
    "    # acc = (...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_dev_pred = text_clf.predict(X_dev)\n",
    "    \n",
    "    acc = np.mean(y_dev_pred == y_dev)\n",
    "    # return text_clf, y_dev_pred, acc\n",
    "    # YOUR CODE HERE\n",
    "    return text_clf, y_dev_pred, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ee1c189fe642928a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_dev_pred, acc = train_and_validate_with_tfidf(X_train_pre, X_dev_pre, y_train, y_dev, max_df=0.95, min_df=0.05)\n",
    "\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'negative negative negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.8492, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.b)\n",
    "Next, find the top 20 most important words in the training tweets according to TfIdf. The solution should be similar to how you found the most common words from CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f1b44cc6ab17796",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-92-511c7d3b97f5>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-92-511c7d3b97f5>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    top_20_most_important_words =\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# don't forget to re-transform the preprocessed training data\n",
    "# top_20_most_important_words = ...\n",
    "# do not return the scores, just the words\n",
    "# YOUR CODE HERE\n",
    "top_20_most_important_words ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8d60f5f2f2137831",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_20_most_important_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-14ddbd5fd328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m assert top_20_most_important_words == ['unit',\n\u001b[0m\u001b[1;32m      2\u001b[0m                                          \u001b[0;34m'flight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                          \u001b[0;34m'usairway'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          \u001b[0;34m'americanair'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0;34m'southwestair'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_20_most_important_words' is not defined"
     ]
    }
   ],
   "source": [
    "assert top_20_most_important_words == ['unit',\n",
    "                                         'flight',\n",
    "                                         'usairway',\n",
    "                                         'americanair',\n",
    "                                         'southwestair',\n",
    "                                         'jetblu',\n",
    "                                         'thank',\n",
    "                                         'get',\n",
    "                                         'hour',\n",
    "                                         'delay',\n",
    "                                         'time',\n",
    "                                         'servic',\n",
    "                                         'help',\n",
    "                                         'cancel',\n",
    "                                         'custom',\n",
    "                                         'bag',\n",
    "                                         'call',\n",
    "                                         'wait',\n",
    "                                         'plane',\n",
    "                                         'co']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the most important words from TfIdf are slightly different from the most common words from the BoW. The metrics we got using TfIdf may have gotten worse, but with a bit more tuning maybe we can get get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab54cc6297c36ee2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q4.c)\n",
    "\n",
    "Use the `train_and_validate_with_tfidf` function you created before to train with different hyperparameters and get an accuracy score above 88.5% on the validation dataset. (This threshold is below what we got for plain CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07b6f3694941ac8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867706528018486"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf, _, acc = train_and_validate_with_tfidf(...)\n",
    "# YOUR CODE HERE\n",
    "clf, _, acc = train_and_validate_with_tfidf(X_train_pre, X_dev_pre, y_train, y_dev, max_df=0.97, min_df=0.0009, max_features=100000)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07b6f3694941ac81",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(acc >= 0.885)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.99      0.94      1391\n",
      "    positive       0.96      0.51      0.66       341\n",
      "\n",
      "    accuracy                           0.90      1732\n",
      "   macro avg       0.93      0.75      0.80      1732\n",
      "weighted avg       0.91      0.90      0.89      1732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = clf['tfidf'].transform(clf['vect'].transform(X_test_pre))\n",
    "y_test_pred = clf['clf'].predict(X_test_vec)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up not being able to beat our baseline of BoW with Tfidf, maybe because the dataset is small and simple, and a simple algorithm was enough. Still, in general it's good to try TfIdf for text classification tasks and have an understanding of how your results change with different hyperparameters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
