{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-229243dce415caea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU09 - Exercises\n",
    "\n",
    "Welcome to the exercises of the BLU09! Should you get stuck on an exercise take a look at the hints or at the learning notebook in order to get some clues. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import hashlib\n",
    "import inspect\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from hashlib import sha256\n",
    "from collections import Counter\n",
    "import string\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c9c6fdf3e87a0f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8204f83b1061d2df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## The Goal\n",
    "In this learning unit you are going to create a binary classifier to determine if a movie review is 'positive' or 'negative'. You will start by building some basic features, then go on to build more complex ones, and finally putting it all together. You should be able to have a working classifier by the end of the notebook. \n",
    "\n",
    "## The Dataset\n",
    "For this Exercise Notebook, you are going to use the IMDB Large movie dataset - [Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). Each movie review has either a Positive, or a Negative label. A negative review has a score equal or less than 4 (out of 10), and a positive review has a score equal or more than 7 (out of 10). Hence, reviews with more neutral ratings are not included in the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa384cff83153717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72fee239d1777a4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First of all, let's load both the train and test set into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7fa5bb85f101a77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=42):\n",
    "\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "                \n",
    "    print(\"\\nFinished loading Train set\\n\")\n",
    "    \n",
    "    # Load the test data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "                \n",
    "    print(\"\\nFinished loading Test set\\n\")\n",
    "    \n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "    \n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings: The two cells below might take a few minutes depending on your machine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d2fb6dc78356253",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished loading Train set\n",
      "\n",
      "\n",
      "Finished loading Test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_imdb_sentiment_analysis_dataset(\"datasets/\")\n",
    "df = pd.DataFrame(data=[x_train, y_train], index=[\"text\", \"label\"]).T\n",
    "df = df.append(pd.DataFrame(data=[x_test, y_test], index=[\"text\", \"label\"]).T)[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbc68a16f01c06dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load the small-sized SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "en_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "# Create a list of SpaCy \"Docs\" by leveraging the SpaCy pipeline\n",
    "docs = list(nlp.pipe(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58afbb4ebfcde68b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's have a look at the first 2 reviews to understand the text we are dealing with ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ce84919fe5c0179",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The exploding zeppelins crashing down upon 'Sky Captain' Jude Law's base present an adequate metaphor to describe how truly terrible this movie is. First off, let me state right off the bat that I sincerely doubt that Paramount will ever recover any money from this film. A cult hit it might become, but only because it is so remarkable for what it failed to achieve. I can see the studio pitch now. \"Let's combine 1920's German Expressionism and a 1940's globetrotting adventure with a modern action flick and use computer animation to dominate every scene! Wow, won't that be a success! \" Skycaptain bludgeons the viewer with its sheer excess. There are too many fake explosions, too many unconvincing dogfight scenes, and too few real moments where the characters are anything but painfully two-dimensional. After all, why shock and awe with one floating airship when you can have three, or five, or one hundred?! Moreover, what could have been a groundbreaking film, seamlessly combining computer generated imagery and human actors in a stylized and intriguing setting, will instead become a flop in no small part because it fails to meet the most important requirement of any flick using CGI. Quite simply, the graphics are amazingly poor. From the movement of the cars to the physics of the aircraft in the dogfights, everything seems to be just a little off. I'm not being nit-picky here in any way. An infant could notice that a car doesn't glide along the road like a maglev train (unless its a Mercedes S500). And for those of you raising your voices in protest, crying out 'This is a stylized film, it's not supposed to be like reality', let me just say this. Lord of the Rings has set the standard for integrating real-life actors with CGI, Starship Troopers has set the standard for ironic science fiction films, the Rocketeer did a solid job reintroducing the decade of the 1920's back into the Hollywood film portfolio, and Tim Burton's Batman created a unique picture of New York City/Gotham that has yet to be repeated. Sky Captain falls so short of all these films, it is hard for me to mention them in the same sentence. Plus, the acting is so poor, it makes me positively ill. So there you have it. I spent $9 to see this film and you get my review. I hope it might dissuade you all from making the same mistake that I did.,\n",
       " Classic, highly influential low budget thriller that gave birth to a horror icon and launched the careers of both director Carpenter and star Curtis.<br /><br />Seemingly unstoppable murderer escapes from mental institution and returns to his hometown where he begins to stalk a local babysitter on Halloween.<br /><br />Halloween is a film that never fails to live up to its reputation as a horror masterpiece! Carpenter's frightening story and clever direction give this film such chillingly good life that it must be seen to really be felt! The direction often consists of such simple elements, shadows, dark streets, creaking doors, that it makes even the everyday setting of a small town neighborhood truly creepy. Carpenter well-times his suspense and his jolting shocks to make them the most effectively startling, that in itself is a feat few horror filmmakers ever manage! Plus, he is wise enough to give us some truly likable young characters and a very scary villain to keep the tension all the more strong. Highest kudos also go to Carpenter's simple, yet frighteningly unnerving music score. In a sense, Halloween is a fine example of a perfect horror film!<br /><br />The cast is excellent. Young Jamie Lee Curtis does a very nice turn as lovable babysitter Laurie Strode, she's so good that she would go on to be in a number of other horror films before breaking into bigger films. The great Donald Pleasants does a perfect performance as a Myer's doctor, who's desperate to capture him again. Supporting cast Loomis, Soles, Castle, and others are good too.<br /><br />So like its own villain, Halloween is an unstoppable force that never fails to thrill and chill. It is a MUST for all genre fans!<br /><br />**** out of ****]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76826c54dbe7c129",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 - Text cleaning\n",
    "\n",
    "Looking at the text above, you see that there are several HTML tags. First, let's clean 'em up! BeautifulSoup has a cool `get_text()` method that strips all the leftover html tags. Then let's use Regex, something that you have learned previously, to remove all the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6a921c3380d7349",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_punct(text):\n",
    "    #remove everything except words, digits and space\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "        \n",
    "    #regex often miss the underscore so let's remove that as well\n",
    "    text = re.sub(r'\\_','',text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    if stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "\n",
    "    text_processed = ' '.join(tokens)\n",
    "    return text_processed\n",
    "\n",
    "def preprocessing(df):\n",
    "    \"\"\"\n",
    "    Implement the three above functions in the respective order to remove html tags, punctuations and stopwords\n",
    "    Hint: Use the apply function.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "    \n",
    "    #df_['text'] = df_['text'].apply(...).apply(...).apply(...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    df_['text'] = df_['text'].apply(remove_html_tags).apply(remove_punct).apply(remove_stopwords,args=[en_stopwords])\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4e472878224d73d2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's clean, and process the df\n",
    "df_raw = df.copy()\n",
    "df = preprocessing(df)\n",
    "value_hash = '81596d9ecc63f0a3d1b634903b64affc939a27cb09ebe297d4c0d9697ca2bb11'\n",
    "assert sha256(str(df['text']).encode()).hexdigest() == value_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3964f5278d3e2e45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2 - Text exploration with SpaCy \n",
    "\n",
    "Now that we have cleaned the data, let's start extracting some useful features. We will first start simple and perform some exploration using `SpaCy`.\n",
    "\n",
    "### Q2.a) Create a simple matcher\n",
    "You suspect that some positive words such as \"excellent\", \"classic\", and \"great\" often occur in Positive reviews. Let's quickly test that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a56287fb46358b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classic\n",
      "1    240\n",
      "0    133\n",
      "Name: label, dtype: int64\n",
      "-------\n",
      "excellent\n",
      "1    332\n",
      "0     68\n",
      "Name: label, dtype: int64\n",
      "-------\n",
      "great\n",
      "1    934\n",
      "0    469\n",
      "Name: label, dtype: int64\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for word in ['classic', 'excellent', 'great']:\n",
    "    print(word)\n",
    "    print(df[df['text'].str.contains(word)].label.value_counts())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afd00f2b839c84f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Indeed, your intuition is right. It's clear that those positive words are more likely to occur in Positive reviews. \n",
    "\n",
    "Now, take advantage of SpaCy's `Matcher` to count the total *exact* number of matches of these words. Looking at the below figure should help you choose the pattern to use for this purpose.\n",
    "\n",
    "![](media/token_attributes.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1c582f5472593cf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of total exact matches of the words \"excellent\", \"great\", and \"classic\" using the SpaCy Matcher and assign it to \"count\"\n",
    "#matcher = Matcher(...)\n",
    "#\n",
    "#for ... :\n",
    "#    pattern = [...]\n",
    "#    matcher.add(...)\n",
    "#\n",
    "#count =0\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    count += ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "List_m = ['excellent', 'great', 'classic']\n",
    "    \n",
    "for word in List_m:\n",
    "    pattern = [{'ORTH': word} for c in word.split()]\n",
    "    matcher.add(word, None, pattern)\n",
    "    \n",
    "count = 0\n",
    "\n",
    "for doc in docs:\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    count += len(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c4c03bc54acfdc0",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "count_hash = '33e14c27247dae6ca2ac565cf7d5fa4200defa487918c52a2dfcccb6d09b4329'\n",
    "assert sha256(str(count).encode()).hexdigest() == count_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90610064b252d533",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q2.b) Extract Emojis\n",
    "\n",
    "Looking at a few review examples, you realized that people tend to use emojis in their reviews. Perhaps we could extract some signals out of these? \n",
    "\n",
    "Let's build a matcher to extract positive emojis & negative emojis from the text and store their counts in `positive_emojis_count` and `negative_emojis_count`. \n",
    "\n",
    "You can easily do this with Regex - Spacy allows us to add the `REGEX` operator to our Matcher object. Hint: Check out [Spacy's documentation](https://spacy.io/usage/rule-based-matching#regex) to learn how to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdebc756314d71d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-8b750cbd267a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_emojis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpositive_emojis_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_emoji_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mnegative_emojis_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_emoji_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-242-8b750cbd267a>\u001b[0m in \u001b[0;36mcount_emoji_matches\u001b[0;34m(pattern, docs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mn_emojis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0memojis_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#pos_patterns = ['TEXT': ....] - For Positive emoji let's use \":)\"\n",
    "#neg_patterns = ['TEXT': ....] - For Negative emoji let's use \":(\"\n",
    "\n",
    "# Hint - Don't forget to escape the special character \"(\" and \")\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "pos_patterns = [{'TEXT':{\"REGEX\": '((?::)(?:)?(?:\\)))'}}] \n",
    "neg_patterns = [{'TEXT':{\"REGEX\": '((?::)(?:)?(?:\\())'}}]\n",
    "\n",
    "def count_emoji_matches(pattern, docs = docs):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"EMOJIS\", None, pattern)\n",
    "    \n",
    "    n_emojis = []\n",
    "    for doc in docs:\n",
    "        matches = matcher(d)\n",
    "        emojis_count = len(matches)\n",
    "        for match in matches:\n",
    "            emojis_count += 1\n",
    "        n_emojis.append(emojis_count)\n",
    "            \n",
    "    return n_emojis\n",
    "\n",
    "positive_emojis_count = sum(count_emoji_matches(pos_patterns))\n",
    "negative_emojis_count = sum(count_emoji_matches(neg_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-790c430e04cb0f1e",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "positive_hash = '7688b6ef52555962d008fff894223582c484517cea7da49ee67800adc7fc8866'\n",
    "negative_hash = 'd4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35'\n",
    "assert sha256(str(positive_emojis_count).encode()).hexdigest() == positive_hash\n",
    "assert sha256(str(negative_emojis_count).encode()).hexdigest() == negative_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-880531b38c9b67b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q2.c) Extract Part of Speech features\n",
    "\n",
    "You also think that negative reviews may have several adverbs followed by an adjective to express the extent to which how bad a movie is (e.g. ridiculously bad, unbelievable awful)\n",
    "\n",
    "To help you, here's the list of PoS available in SpaCy:\n",
    "\n",
    "![](media/pos_helper.png)\n",
    "\n",
    "To complete this exercise you should build a matcher to extract all adverbs that are followed by an adjective. Store this sequence in a list, and assign the result to `adv_adj_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64683287d0596aeb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Store all the adv-adj sequence in a list called adv_adj_list\n",
    "#matcher = ...\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "#\n",
    "#adv_adj_list = []\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    for ... in matches:\n",
    "#        adv_adj_list.append(...)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'ADV'},{'POS':'ADJ'}]\n",
    "matcher.add('adjs',None,pattern)\n",
    "\n",
    "adv_adj_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        span_text = span.text \n",
    "        adv_adj_list.append(span_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2d0d7a2d2ed5ed85",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "list_hash = '1cde2b9de1483573a1d70aed1fc8f90eb8c3f18a992f289543e3aa4c09a14edd'\n",
    "assert len(adv_adj_list) == 15679\n",
    "assert sha256(','.join(map(str, adv_adj_list)).encode()).hexdigest() == list_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e178d0f93c0b875c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.d) Extract entities\n",
    "\n",
    "Your intuition is that Positive Reviews are likely to describe the movie plot, citing several locations in the movie. An idea is to extract some locations from the text.\n",
    "\n",
    "Build a `Matcher` to match Location in the text and extract the top 10 most common ones. Assign them to `most_common_ents`.\n",
    "\n",
    "*hint: Use [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) to extract the most common elements (check the most_common(n) method). You will need to feed it strings (not SpaCy spans)*\n",
    "\n",
    "*note: in a real-case scenario we would perform some text preprocessing first and build a better entity recognizer, but let us not worry about that here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c82f29c4da571fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E152] The attribute LABEL is not supported for token patterns. Please use the option validate=True with Matcher, PhraseMatcher, or EntityRuler for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-5714fbdb797a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"GPE\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'countrie'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0madv_adj_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher._preprocess_pattern\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher._get_attr_values\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E152] The attribute LABEL is not supported for token patterns. Please use the option validate=True with Matcher, PhraseMatcher, or EntityRuler for more details."
     ]
    }
   ],
   "source": [
    "# Build a matcher to extract the location-type entities from the text and assign them to most_common_ents\n",
    "#\n",
    "#matcher = ...\n",
    "#\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "# \n",
    "#...\n",
    "#\n",
    "# most_common_ents = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"label\": \"GPE\"}]\n",
    "matcher.add('countrie',None,pattern)\n",
    "\n",
    "adv_adj_list = []\n",
    "most_common_ents=[]\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        span_text = span.text \n",
    "        adv_adj_list.append(span_text)\n",
    "        counter = Counter(doc)\n",
    "    most_common_ents.append((span_text))\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f7cb2acd87b1549d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-c11fec20162f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ment_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"35eb8667c8e28dacaba6290382dc6d46e4df777b27d1eef8678e099ad359de25\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_common_ents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_common_ents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ment_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ent_hash = \"35eb8667c8e28dacaba6290382dc6d46e4df777b27d1eef8678e099ad359de25\"\n",
    "assert len(most_common_ents) == 10\n",
    "assert sha256(','.join(map(str, most_common_ents)).encode()).hexdigest() == ent_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb8248e18ff20bd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have the most common locations, let's quickly check its usefulness and whether we should include it as a feature. \n",
    "\n",
    "Indeed, as can be seen below, locations are more likely to occur in Positive Reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64f5f34fb17c25bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_locations = [loc[0] for loc in most_common_ents]\n",
    "\n",
    "for word in most_common_locations:\n",
    "    print(word)\n",
    "    print(df_raw[df_raw['text'].str.contains(word)].label.value_counts())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45e34f159eafa6a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3 - Create Numerical Features\n",
    "\n",
    "You start thinking what features could actually be useful for solving your problem. One possible factor that may help is to know the number of adjectives used, the length of the review, average word length and the count of positive & negative emojis\n",
    "\n",
    "Add extra fields to the `df` dataframe with:\n",
    "- The count of the number of adjectives - consider the adjectives as those identified by SpaCy\n",
    "- The length of the document - you can simply count the number of characters. \n",
    "- The average word length - you learned how to do this in Learning Notebook - you don't need to remove stopwords as we already did it in the beginning\n",
    "- The count of positive emojis - Hint: use `count_emoji_matches` function in Q2b.\n",
    "- The count of negative emojis - Hint: use `count_emoji_matches` function in Q2b.\n",
    "\n",
    "Assign the number of adjectives to a new column called `n_adjs`, the length of the reviews to a column called `text_length`, average word length to a column called `avg_word_length`, and the count of positive and negative emojis to two columns called `positive_emojis_count` and `negative_emojis_count`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bab0e5a3f8f8cbc7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hint: you can iterate over the tokens in Spacy doc to inspect them \n",
    "# for doc in docs:\n",
    "#    print(doc.ents)\n",
    "#    for token in doc:\n",
    "#        print(token.pos_)\n",
    "\n",
    "#n_adjs = []\n",
    "#\n",
    "#for doc in docs:\n",
    "#    count_adjs = 0\n",
    "#    ...\n",
    "#    ...\n",
    "#    n_adjs.append(count_adjs)\n",
    "#\n",
    "#df['n_adjs'] = n_adjs\n",
    "#df['text_length'] = ...\n",
    "#df['avg_word_length'] = ...\n",
    "#df['positive_emojis_count'] = count_emoji_matches(...)\n",
    "#df['negative_emojis_count'] = count_emoji_matches(...)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "import statistics \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "pos_patterns = [{'TEXT':{\"REGEX\": '((?::)(?:)?(?:\\)))'}}] \n",
    "neg_patterns = [{'TEXT':{\"REGEX\": '((?::)(?:)?(?:\\())'}}]\n",
    "\n",
    "n_adjs = []\n",
    "\n",
    "#ADJS\n",
    "pattern_adjs = [{'POS':'ADJ'}]\n",
    "matcher.add(\"adjs\", None, pattern_adjs)\n",
    "    \n",
    "n_adjs=[]\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    count_adjs = len(matches)\n",
    "\n",
    "    n_adjs.append(count_adjs)\n",
    "        \n",
    "# word lenght\n",
    "def avg_mean(docs):\n",
    "    list_len=[]\n",
    "    list_mean=[]\n",
    "    for doc in docs:\n",
    "        list_len=[]\n",
    "        for word in doc:\n",
    "            list_len.append(len(word))\n",
    "        list_mean.append(statistics.mean(list_len))\n",
    "    return list_mean\n",
    "    \n",
    "list_mean = avg_mean(docs)\n",
    "\n",
    "df['n_adjs'] = (n_adjs)\n",
    "df['text_length'] = (df['text'].map(len))\n",
    "df['avg_word_length'] = (list_mean)\n",
    "df['positive_emojis_count'] = (count_emoji_matches(pos_patterns))\n",
    "df['negative_emojis_count'] = (count_emoji_matches(neg_patterns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03dbc9fa2ea23663",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert all(col in df.columns for col in ('n_adjs', 'avg_word_length', 'text_length', 'positive_emojis_count', 'negative_emojis_count'))\n",
    "assert df.n_adjs.sum() == 101733\n",
    "assert np.allclose(df.avg_word_length.sum(), 29805, 5)\n",
    "assert df.text_length.sum() == 3829351\n",
    "assert df.positive_emojis_count.sum() == 56\n",
    "assert df.negative_emojis_count.sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-923d795a48aa24c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q4 - Pipelines and Feature Unions\n",
    "It is now time for you to leverage on your newly built features and construct pipelines that can be fed to a classifier. You decide to use a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) as you hear from industry experts it tends to work well for text classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c8ff5d97624ac9b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritacarvalho/.virtualenvs/blu07/lib/python3.7/site-packages/pandas/core/generic.py:5159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data.label = train_data.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>n_adjs</th>\n",
       "      <th>text_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>positive_emojis_count</th>\n",
       "      <th>negative_emojis_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>guy idea cinema okay interestig theater shows ...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>370</td>\n",
       "      <td>4.189394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>deeply moved movie respects want clara lago pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1095</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>im intrigued strong sense favour sympathy sina...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1749</td>\n",
       "      <td>3.902054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>turkish bath sequence film noir located new yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1892</td>\n",
       "      <td>3.689974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>best movie ive seen white best romantic comedy...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>181</td>\n",
       "      <td>4.387755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4711</th>\n",
       "      <td>idea nice bringing stars movie great stories s...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>368</td>\n",
       "      <td>3.809859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>david webb peoples meets paul andersonif sound...</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>843</td>\n",
       "      <td>4.174658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>came montreal premiere zero dayand im surprise...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1183</td>\n",
       "      <td>4.068558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>communicate film essential things life like li...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>378</td>\n",
       "      <td>4.119658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>actedmanipulated documentary darkest places gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>417</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label  n_adjs  \\\n",
       "1501  guy idea cinema okay interestig theater shows ...     0      14   \n",
       "2586  deeply moved movie respects want clara lago pr...     1      29   \n",
       "2653  im intrigued strong sense favour sympathy sina...     1      36   \n",
       "1055  turkish bath sequence film noir located new yo...     1      38   \n",
       "705   best movie ive seen white best romantic comedy...     1       6   \n",
       "...                                                 ...   ...     ...   \n",
       "4711  idea nice bringing stars movie great stories s...     0      12   \n",
       "2313  david webb peoples meets paul andersonif sound...     1      27   \n",
       "3214  came montreal premiere zero dayand im surprise...     1      36   \n",
       "2732  communicate film essential things life like li...     1       7   \n",
       "1926  actedmanipulated documentary darkest places gu...     0      12   \n",
       "\n",
       "      text_length  avg_word_length  positive_emojis_count  \\\n",
       "1501          370         4.189394                      0   \n",
       "2586         1095         3.769231                      0   \n",
       "2653         1749         3.902054                      0   \n",
       "1055         1892         3.689974                      0   \n",
       "705           181         4.387755                      0   \n",
       "...           ...              ...                    ...   \n",
       "4711          368         3.809859                      0   \n",
       "2313          843         4.174658                      0   \n",
       "3214         1183         4.068558                      0   \n",
       "2732          378         4.119658                      0   \n",
       "1926          417         4.000000                      0   \n",
       "\n",
       "      negative_emojis_count  \n",
       "1501                      0  \n",
       "2586                      0  \n",
       "2653                      0  \n",
       "1055                      0  \n",
       "705                       0  \n",
       "...                     ...  \n",
       "4711                      0  \n",
       "2313                      0  \n",
       "3214                      0  \n",
       "2732                      0  \n",
       "1926                      0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7084694ab285eb01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "    \n",
    "def get_accuracy(feats, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Return the accuracy on the test_data by using a RandomForestClassifier trained on the \n",
    "    train_data with the features described by feats\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features',feats),\n",
    "        ('classifier', RandomForestClassifier(random_state = 42, n_estimators=10)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(train_data, train_data.label)\n",
    "\n",
    "    preds = pipeline.predict(test_data)\n",
    "    accuracy = np.mean(preds == test_data.label)\n",
    "    \n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ba44a8653efe95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q4.a) Build a Feature Union\n",
    "You hypothesize that combining the text and numerical features could help you build a strong classifier. \n",
    "\n",
    "Use `FeatureUnion` to join:\n",
    "- The Text features extracted from a standard sklearn `TfidfVectorizer` (with $ngram\\_range=(1,2)$)\n",
    "- The numeric feature of the length of the messages scaled to zero mean and unit variance *[hint](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)*\n",
    "- The average word length that you have created previously\n",
    "\n",
    "Assign the Feature Union to a variable named `feats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0848e748dc61721f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#text_pipe = Pipeline(...)\n",
    "#text_len_pipe =  Pipeline(...)\n",
    "#word_len_pipe =  Pipeline(...)\n",
    "#feats = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "text_pipe = Pipeline([\n",
    "                ('selector', TextSelector(\"text\")),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range = (1,2)))\n",
    "            ])\n",
    "text_len_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(\"text_length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "word_len_pipe =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "    \n",
    "feats = FeatureUnion([('text_pipe', text_pipe), \n",
    "                      ('text_len_pipe', text_len_pipe),\n",
    "                      ('word_len_pipe',word_len_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-80971dd7f21ec905",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7240\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert any(isinstance(obj, Selector) for obj in feats.transformer_list[0][1])\n",
    "assert any(isinstance(obj, TfidfVectorizer) for obj in feats.transformer_list[0][1])\n",
    "assert np.allclose(get_accuracy(feats, train_data, test_data), 0.7290, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f4ecd3a08b31cb9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q4.b) Add more features\n",
    "You decide to try adding the number of adjectives to your features to see if they can improve the performance of your classifier. \n",
    "\n",
    "On top of all features you have used for `feats`, add the number of adjectives `n_adjs` that you computed in Q3 to your features. Then assign your features to `feats_v2`. There should be 4 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ef7216ae6023fdf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#adjs_pipe = Pipeline(...)\n",
    "#...\n",
    "#feats_v2 = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "adjs_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(key='n_adjs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "feats_v2 = FeatureUnion([\n",
    "    ('features',feats),\n",
    "    ('adjs_pipe', adjs_pipe)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2726b142fbec4f14",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7070\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-9e14df51cf78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6860\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(feats_v2, train_data, test_data)\n",
    "assert np.allclose(accuracy, 0.6860, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aa91b08408579d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q4.c) Add the Emojis feature\n",
    "You try to improve your model even further by including the number of emojis `positive_emojis_count` and `negative_emojis_count` features that you created above. \n",
    "\n",
    "On top of all features in `feats_v2`, add the number of emojis to your features and assign the result to `feats_v3` (**no need to scale** the features this time). There should be 6 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5770ba6afa04742",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#...\n",
    "#feats_v3 = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HEREj\n",
    "pos_em = Pipeline([\n",
    "                ('selector', NumberSelector(key='positive_emojis_count')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "neg_em = Pipeline([\n",
    "                ('selector', NumberSelector(key='negative_emojis_count')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "\n",
    "feats_v3 = FeatureUnion([\n",
    "    ('features',feats_v2),\n",
    "    ('pos_em', pos_em),\n",
    "    ('neg_em', neg_em)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8289104bef520ea7",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7200\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-5a0a801247cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_v3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7010\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(feats_v3, train_data, test_data)\n",
    "assert np.allclose(accuracy, 0.7010, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dcbab21d6897ff51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You realize that your accuracy actually decreased, which reminds you that more features does not necessarily mean better results.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You realize you can get fairly ok accuracy on the sentiment analysis problem using a fairly simple solution. You know there are many things you could improve (e.g. Dimensionality Reduction) and many further paths you could choose in order to try to take your classifier to the next level, but you decide to leave that challenge for another day. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
