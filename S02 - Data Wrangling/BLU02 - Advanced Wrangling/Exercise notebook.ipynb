{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b88a7e663ede9739",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU02 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f0cbbbc1eadab9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-337eee9fcbab05b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single dataframe, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons after 1900**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf4694e22046237e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_programs():\n",
    "    files = os.listdir('data/programs/')\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1900 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_after_1900: List[str] = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    files_after_1900 = [f for f in files if int(f[0:4]) >=1900]\n",
    "    \n",
    "    # Create a list with the name of all .csv files.\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    seasons = [read_season(f) for f in files_after_1900 if '.csv' in f]\n",
    "\n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    programs = pd.concat(seasons, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Drop the column ProgramID.\n",
    "    # programs = ...\n",
    "    # YOUR CODE HERE\n",
    "    programs = programs.drop(columns='ProgramID')\n",
    "    \n",
    "    # Set the index to be the column GUID, and sort the dataframe by the index \n",
    "    #( use the DataFrame.sort_index() function).\n",
    "    # Feel free to use method chaining if you want.\n",
    "    # YOUR CODE HERE\n",
    "    programs=programs.set_index('GUID').sort_index()\n",
    "\n",
    "    return programs\n",
    "\n",
    "\n",
    "def read_season(file):\n",
    "    path = os.path.join('data', 'programs', file)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "programs = make_programs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1aa243b8e5df1a17",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert programs['Season'].min() == '1900-01'\n",
    "\n",
    "shape = str(programs.shape)\n",
    "expected_hash = '16278afb4c2032bcddc35b915f5439ef586333e2723c2ba6cfb9cc1b58eca0e1'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-673495903dcba4a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's preview the `programs` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc75b7c4c275a431",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orchestra</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0002718f-a7a0-4362-9366-92fabab4ff3c</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1928-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004749e-19e2-4c85-a51e-76a2b0987e4e</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1922-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008995b-f0ce-4bdb-b2f8-2fc9827430fe</th>\n",
       "      <td>New York Symphony</td>\n",
       "      <td>1925-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008fd59-7b87-4e87-8b42-ab5b0f8505cf</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1942-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000c0467-d7bf-4599-8e37-c856bc13a389</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1991-92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Orchestra   Season\n",
       "GUID                                                                \n",
       "0002718f-a7a0-4362-9366-92fabab4ff3c  New York Philharmonic  1928-29\n",
       "0004749e-19e2-4c85-a51e-76a2b0987e4e  New York Philharmonic  1922-23\n",
       "0008995b-f0ce-4bdb-b2f8-2fc9827430fe      New York Symphony  1925-26\n",
       "0008fd59-7b87-4e87-8b42-ab5b0f8505cf  New York Philharmonic  1942-43\n",
       "000c0467-d7bf-4599-8e37-c856bc13a389  New York Philharmonic  1991-92"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0317b5a17a59d85f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Read the concerts data.\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58bd5fde4e09e377",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(): \n",
    "    # Read concerts data and drop the ProgramID and ConcertID columns.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    concerts = pd.read_csv('./data/concerts.csv')\n",
    "    \n",
    "    concerts = concerts.drop(['ProgramID','ConcertID'], axis=1)\n",
    "    # Remember to_datetime? We need here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date and pd_to_datetime(..., \n",
    "    # format=%I:%M%p).dt.time for the Time.\n",
    "    # YOUR CODE HERE\n",
    "    concerts['Date'] = (pd.to_datetime(concerts['Date'].str[0:10])\n",
    "                        .dt.date)\n",
    "    concerts['Time'] = (pd.to_datetime(concerts['Time'],format='%I:%M%p')\n",
    "                        .dt.time)   \n",
    "    return concerts\n",
    "\n",
    "\n",
    "concerts = make_concerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1e95492b37c4c43f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(concerts.shape)\n",
    "expected_hash = 'c030586e7370b1f2c34307d5de9b921d96efa28c933e44111b121ed819f339da'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "sample = str(concerts.sample(random_state=0))\n",
    "expected_hash = '392a3db01753b02d85173c38cde95112fb5cdf06ca5a45d25f828238d56103be'\n",
    "assert hashlib.sha256(sample.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e22fb5671017b5bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GUID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Location</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38e072a7-8fc9-4f9a-8eac-3957905c0002</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1842-12-07</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7b2b95c-5e0b-431c-a340-5b37fc860b34</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-02-18</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894e1a52-1ae5-4fa7-aec0-b99997555a37</td>\n",
       "      <td>Special</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-04-07</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34ec2c2b-3297-4716-9831-b538310462b7</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-04-22</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610a4acc-94e4-4cd6-bdc1-8ad020edc7e9</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-11-18</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   GUID            EventType       Location  \\\n",
       "0  38e072a7-8fc9-4f9a-8eac-3957905c0002  Subscription Season  Manhattan, NY   \n",
       "1  c7b2b95c-5e0b-431c-a340-5b37fc860b34  Subscription Season  Manhattan, NY   \n",
       "2  894e1a52-1ae5-4fa7-aec0-b99997555a37              Special  Manhattan, NY   \n",
       "3  34ec2c2b-3297-4716-9831-b538310462b7  Subscription Season  Manhattan, NY   \n",
       "4  610a4acc-94e4-4cd6-bdc1-8ad020edc7e9  Subscription Season  Manhattan, NY   \n",
       "\n",
       "          Venue        Date      Time  \n",
       "0  Apollo Rooms  1842-12-07  20:00:00  \n",
       "1  Apollo Rooms  1843-02-18  20:00:00  \n",
       "2  Apollo Rooms  1843-04-07  20:00:00  \n",
       "3  Apollo Rooms  1843-04-22  20:00:00  \n",
       "4  Apollo Rooms  1843-11-18       NaT  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concerts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0653d1fbb5f9043",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both dataframes into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-110744d4ad3ef2ef",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "# nyp = ...\n",
    "# YOUR CODE HERE\n",
    "nyp = pd.merge(programs, concerts, on='GUID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b7730d3cb9832724",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp.shape)\n",
    "expected_hash = 'a75738e37ac4ccf37a893a1009ba624efce9efaa7721d4319e9e078193fe8de6'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac54633ced3ec3ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-030bd11a9b6c76c8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_works():\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    works = pd.read_csv('./data/works.csv')\n",
    "    # Remove the Intervals (attention to the values in the Interval column).\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    works = works[pd.isnull(works['Interval'])]\n",
    "\n",
    "    # Select the columns GUID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    work_related_columns = ['GUID','ComposerName','WorkTitle','Movement','ConductorName']\n",
    "    \n",
    "    works = works.filter(items=work_related_columns)\n",
    "          \n",
    "    return works\n",
    "\n",
    "\n",
    "def make_soloists():\n",
    "    # Read the soloists data and drop ProgramID, WorkID and MovementID.\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    soloists = pd.read_csv('./data/soloists.csv')\n",
    "    \n",
    "    to_drop = ['ProgramID', 'WorkID', 'MovementID']\n",
    "    \n",
    "    soloists = soloists.drop(to_drop, axis = 1)\n",
    "    \n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = make_works()\n",
    "soloists = make_soloists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2a37eb128ea7e979",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works.shape)\n",
    "expected_hash = 'cad58aa6cd33cfa24c08a0f0f846877178ab31278f212c80b16b952d9416f883'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(soloists.shape)\n",
    "expected_hash = 'a7b0d20a45ff1344e0398eebb162af9afb8805082b0dfdcb70e9a4b78f94dd13'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb54d5d81dda97e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c2e4becb723a4a3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join.\n",
    "# works_and_soloists : pd.DataFrame = ....\n",
    "# YOUR CODE HERE\n",
    "works_and_soloists = pd.merge(works, soloists, on='GUID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-13616b91f6c53cad",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works_and_soloists.shape)\n",
    "expected_hash = 'c0e73877aac4f3916267cb58f2f122ffef32c79039bde2ecb217fda123270d12'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-463cdbc4a6b9ab87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-081458c0c0a40de2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine everything into a single dataframe.\n",
    "# nyp_merged = ...\n",
    "# YOUR CODE HERE\n",
    "nyp_merged = pd.merge(nyp, works_and_soloists, on='GUID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a0ca421b4504282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp_merged.shape)\n",
    "expected_hash = '3c25d9867a3c0134a6625087698dac6314f7c225f806e78dd259788bedcfb10b'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76e3b2877f0a18aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 7 Final transformations (graded)\n",
    "\n",
    "Now, we perform the train-test split.\n",
    "\n",
    "We also perform some final transformations on both datasets:\n",
    "* Include some date features: Year, Month, Day and Weekday\n",
    "* Drop Date, Season and GUID\n",
    "* Change the column name Orchestra to OrchestraName, for consistency with other name columns\n",
    "* Filter out composers that appear in less than 100 concerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8ecf5eba7d6c38b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # You should follow these exact steps:\n",
    "    #   1 - add_date_features, ideally using df.pipe\n",
    "    #   2 - drop Date, Season and GUID\n",
    "    #   3 - rename Orchestra to OrchestraName\n",
    "    #   4 - filter out composers with less than 100 concerts (keep the ones with >= 100 rows)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    to_drop = ['Date','Season','GUID']\n",
    "      \n",
    "    df = df.drop(to_drop, axis = 1)\n",
    "    \n",
    "    \n",
    "    df['OrchestraName'] = df['Orchestra']\n",
    "    \n",
    "    \n",
    "    (df.groupby('ComposerName').filter(lambda x: x.shape[0] > 10)\n",
    "                .groupby('Concert').size())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "\n",
    "nyp_ = preprocess_data(nyp_merged)\n",
    "X_train, X_test = train_test_split(nyp_, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-77f754825659deb6",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp_merged.shape)\n",
    "expected_hash = '3c25d9867a3c0134a6625087698dac6314f7c225f806e78dd259788bedcfb10b'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(nyp_.shape)\n",
    "expected_hash = '31fa2b10222342d4743fa75b3a04c69945106f22fcf7473f5d1daeb84bca88b7'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(sorted(nyp_.columns.values))\n",
    "expected_hash = '097ce79d998a726b3ed60a6cd1d4e4652e10af8285a1e0b4526057805234ee4e'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6daa8cde1618bc1f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "And, finally, we would be ready to explore modeling.\n",
    "\n",
    "For the next part, however, we will be using the famous [Boston House Prices Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7cd808ac07305b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 8 Scaling features (graded)\n",
    "\n",
    "About the Boston dataset:\n",
    "\n",
    "> Each record in the database describes a Boston suburb or town. The data is from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.\n",
    "\n",
    "The features are all numerical (real, positive):\n",
    "* **CRIM** - per capita crime rate by town\n",
    "* **ZN** - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* **INDUS** - proportion of non-retail business acres per town\n",
    "* **CHAS** - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* **NOX** - nitric oxides concentration (parts per 10 million)\n",
    "* **RM** - average number of rooms per dwelling\n",
    "* **AGE** - proportion of owner-occupied units built prior to 1940\n",
    "* **DIS** - weighted distances to five Boston employment centres\n",
    "* **RAD** - index of accessibility to radial highways\n",
    "* **TAX** - full-value property-tax rate per \\$10,000\n",
    "* **PTRATIO** - pupil-teacher ratio by town\n",
    "* **B** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* **LSTAT** - % lower status of the population\n",
    "* **MEDV** - Median value of owner-occupied homes in \\$1000's.\n",
    "\n",
    "We want to scale all features to the same range, using `sklearn.preprocessing.MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-520f1d8e99c15a49",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Initialize the MinMaxScaler to a [0, 5] range.\n",
    "# YOUR CODE HERE\n",
    "scaler = MinMaxScaler([0,5])\n",
    "\n",
    "# Fit on the training set and transform X_train. We expect X_train_\n",
    "# to be a dataframe **just like** X_train, only scaled. \n",
    "# X_train_: pd.DataFrame = ...\n",
    "# YOUR CODE HERE\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train_ = pd.DataFrame(X_train_, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# Transform the test set.\n",
    "# X_test_: pd.DataFrame = ...\n",
    "# YOUR CODE HERE\n",
    "X_test_ = scaler.transform(X_test)\n",
    "\n",
    "X_test_ = pd.DataFrame(X_test_, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39a52095c35ce047",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(X_train_.shape)\n",
    "expected_hash = '6f696c7e30c15aae3f0fa4807b596cf15d28cadaf33602d8d20368f7ac921f26'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(sorted(X_train_.columns.values))\n",
    "expected_hash = 'e946a91d360d95e04adfdea57023d5242defc1e9cf5f41323bfd55a0022adf4c'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(X_test_.shape)\n",
    "expected_hash = 'aa2b4e3c1e358b4b9f21c2c86bbf1187020582395419f1a02a949d7a6efac9e4'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(sorted(X_test_.columns.values))\n",
    "expected_hash = 'e946a91d360d95e04adfdea57023d5242defc1e9cf5f41323bfd55a0022adf4c'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-458080a73da057e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 9 Build a ColumnSelector transformer (graded)\n",
    "\n",
    "There's a simple transformer that can be useful, from times to times, when modeling.\n",
    "\n",
    "What we want is to build a transformer that returns the columns we select beforehand. \n",
    "\n",
    "This transformer could be used to determine what features go into modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d8545164af32b84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    # Implement the __init__ method.\n",
    "    # Our ColumnSelector must be able to receive a parameter columns.\n",
    "    # The default value for columns must be set to 'all', so we can\n",
    "    # initialize it without any explicit parameters.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # There's no need for a fit method in this case, it does nothing.\n",
    "    # We should be able to call fit without any explicit parameters.\n",
    "    # Meaning: we should be able to call ColumnSelector.fit().\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Transform should return all columns if the parameter columns we\n",
    "    # passed upon initialization is equal to 'all'. If a column or a\n",
    "    # list of columns are passed, only those should be returned.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "\n",
    "cols = ['CRIM', 'DIS', 'INDUS', 'RM', 'DIS', 'TAX', 'B']\n",
    "selector = ColumnSelector(columns=cols)\n",
    "X_train__ = selector.fit_transform(X_train_)\n",
    "X_test__ = selector.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6d54a8e7ed69bd97",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(ColumnSelector())\n",
    "assert(selector.fit())\n",
    "\n",
    "shape = str(X_train__.shape)\n",
    "expected_hash = '5d4f688e84beb21ec07f136c16a6cc11318d4f5de7b81bf0232e5282d9834123'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(sorted(X_train__.columns.values))\n",
    "expected_hash = 'cdcff8ef5982ac9ff530dae2b67e0e3c068a4a50ea6c7dbf54b5d9341748af78'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(X_test__.shape)\n",
    "expected_hash = '0aba1c19151f76aa2ecb00fd75be05c6f73860573972e967f3d1fe1c44ae2629'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(sorted(X_test__.columns.values))\n",
    "expected_hash = 'cdcff8ef5982ac9ff530dae2b67e0e3c068a4a50ea6c7dbf54b5d9341748af78'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f751d80b4180bcd0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 10 Building the pipeline (graded)\n",
    "\n",
    "Finally, we want to use the two transformers together and run a linear regression on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6cf45c70a05aa5c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline including:\n",
    "#   1 - 'selector', ColumSelector(columns=cols)\n",
    "#   2 - 'min_max', MinMaxScaler() with same range as above\n",
    "#   3 - 'model', LinearRegression\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MSE: {}'.format(mse))\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b520a77a388d20b2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(pipeline) == Pipeline\n",
    "assert type(pipeline.named_steps['selector']) == ColumnSelector\n",
    "assert type(pipeline.named_steps['min_max']) == MinMaxScaler\n",
    "assert pipeline.named_steps['min_max'].get_params()['feature_range'] == (0,5)\n",
    "assert type(pipeline.named_steps['model']) == LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-877f9bbc108ee8be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
